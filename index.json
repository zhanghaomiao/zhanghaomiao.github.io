[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a PhD in biophysics from the Huazhong University of Science and Technoly. By combing the machine learning and molecuar dynamics, my subject focus on the conformation change of protein . For the sampling protein is diffcult, I develop a method which could accerlate the configuraiton sampling and apply it to study the configuration change of RNA or protein by using machine-realted method.\nIn my spare time, I learn a lot about computer, such as the algorithm, operating system and computer network, parallel computing. I\u0026rsquo;m interested in programming and had developed the software for my projects.\n","date":1567296000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1567296000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a PhD in biophysics from the Huazhong University of Science and Technoly. By combing the machine learning and molecuar dynamics, my subject focus on the conformation change of protein .","tags":null,"title":"Haomiao Zhang","type":"authors"},{"authors":null,"categories":null,"content":" General 吴恩达深度学习课程，来自 coursera\nPrepare  Math Python Keras Tensorflow  ","date":1577836800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1578528000,"objectID":"16003ed5db801fca7527d7903007220d","permalink":"/courses/deep_learning/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/courses/deep_learning/","section":"courses","summary":"Deep Learning from the coursera (2020/01 ~ 2020/03)","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" General 计算机网络笔记，来自 国立清华大学公开课\nPrepare  Book: Kurose, Ross, Computer Networking: A TOP-DOWN approach, 7th Edition time: 2019 / 03 ~ 2019 / 06  ","date":1552089600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1567987200,"objectID":"43e729fcaf99ff01f5b63ff3020f3fd9","permalink":"/courses/computer_network/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/courses/computer_network/","section":"courses","summary":"Computer network basic concepts (2019/03 ~ 2019/06)","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" General 操作系统概论笔记， 来自国立清华大学公开课\nPrepare  Program: Nachos Project Book: Silberschatz, P. Galvin, and G. Gangne, Operating System Concepts, 9th Edition time: 2019 / 06 ~ 2019 / 09  ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"ba275b4cb201c30f47d240f2a119939d","permalink":"/courses/operating_system/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/operating_system/","section":"courses","summary":"Operating system basic concepts (2018/10~2019/01)","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" General 并行计算， 来自国立清华大学公开课\nPrepare  Program: Pthread、OpenMP, MPI, CUDA, MapReduce Book: Parallel Programming in C with MPI and OpenMP, Michael J. Quinn, McGraw- Hill, 2003. time: 2019 / 06 ~ 2019 / 09  ","date":1552089600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1557360000,"objectID":"16dabac5323a62dc8c4fe733b9b8dcba","permalink":"/courses/parallel_computing/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/courses/parallel_computing/","section":"courses","summary":"Parallel Computing from the open course (2019/03~2019/05)","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" Creating a network application, run on different end systems and communicate over a network, no software written for devices in network core, network core devices do not function at application layer (e.g. Web server software communicate with browser software)\nPrinciple of network application Application architectures  Client-server  server: always-on host, permanent IP address, server forms for scaling clients: communicate with server, may be intermittently connected, may have dynamic IP address, do not communicate directly with each other  peer-to-peer  no always on server arbitrary end systems directly communicate peers are intermittently connected and change IP address, highly scalable, but difficult to manage  Hybrid of client-server and P2P  Napster, file transfer P2P, file search centralized(peer register content at central server, peers query same central server to locate content) Instant message: chatting between two users is P2P, presence detection/location centralized (User registers its IP address with central server when it comes online,User contacts central server to find IP addresses of buddies)   Processes Communicating  process: program running within a host with same host, two processes communicate using inter-process communication (defined by OS) processes in different hosts communicated by exchanging messages (Client process: process that initiates communication, Server process: process that waits to be contacted) figure  process sends/receives messages to/from its socket socket analogous to door, sending process relies on transport infrastructure on other side of door which brings message to socket at receiving process API (1) choice of transport protocol (2) ability to fix a few parameters addressing process(判断哪一个process需要沟通)  For a process to receive messages, it must have an identifier Every host has a unique 32-bit IP address Identifier includes both the IP address and port numbers(16-bit) associated with the process on the host (HTTP server: 80, Mail server: 25)   App-layer protocol defines  Types of messages exchanged (e.g. request \u0026amp; response) Syntax of message types (what fields in message \u0026amp; how field are delineated) Semantics of the field (i.e. meaning of information in fields) Rules for when and how processes send \u0026amp; respond to messages Public-domain protocols:  defined in RFCs allows for interoperability(相容)   What transport service does provide  Data loss  some apps can tolerate some loss other apps require 100% reliable data transfer  Timing  some apps require low delay to be \u0026ldquo;effective\u0026rdquo;  Bandwidth  some apps require minimum amount of bandwidth to be (games) effective(multimedia) other apps(\u0026ldquo;elastic apps\u0026rdquo;) make use of whatever bandwidth they get   TCP service: connection-oriented, reliable transport, flow control, congestion control (does not providing: timing, minimum bandwidth guarantees) UDP service: unreliable data transfer between sending and receiving process, dons not provide connection setup, reliability, flow control, congestion control, timing or bandwidth guarantee   Web and HTTP  Web  web page consists of objects An object is a file such as HTML, file ,a JPEG image A web page consists of a bast HTML-file and several referenced object The base HTML file references the other objects in the page with the object\u0026rsquo;s URLs (Uniform Resource Locators)   HTTP Overview  HTTP: hypertext transfer protocol web\u0026rsquo;s application layer protocol client/server model (HTTP1.0, HTTP1.1) procedure  client initiates TCP connection (creates socket) to server server accepts TCP connection from client HTTP message exchanged between browser and web server TCP connection closed  HTTP is stateless, server maintains no information about post client requests  HTTP connections  Non-persistent HTTP  At most one object is sent over a TCP connection HTTP/1.0 uses non-persistent HTTP  Persistent HTTP  Multiple objects can be sent over singe TCP connection between client and server A new connection need not be set up for the transfer of each Web object HTTP/1.1 use persistent connections in default mode   Response time modeling (Non-persistent HTTP)  RTT: time to send a small packet to travel from client to server and back Response Time: one RTT to initiate TCP connection, one RTT for HTTP request and first few bytes of HTTP response to return, file transmission time file transmission time = 2RTT + transmit time   Two versions of persistent connections  Persistent without pipelining  client issues new request only when previous response has been received One RTT for each referenced object  Persistent with pipelining  default in HTTP/1.1 client sends requests as soon as it encounters a referenced object as little as one RTT for all the referenced objects   HTTP request message  two types of HTTP messages: request, response HTTP request message (ASCII format)\nGET /somedir/page.html HTTP/1.1 Host: www.someschool.edu Connection: close User-agent: Mozilla/5.0 Accept-language: fr   Method Types  HTTP/1.0  GET: return the object POST: send information to be stored on the server Head: return only information about the object, such as how old it is, but not the object itself  HTTP/1.1  GET, POST, HEAD PUt: uploads a new copy of existing object in entity body to path specified in URL field DELETE: delete object specified in the URL field   HTTP response message  A status line, which indicates the success of failure of the request Header line, A description of the information in the response, this is the metadata or meta information actual information response status code  200 OK 300 Moved permanently 400 Bad request 404 Not found 505 HTTP version not supported    User-Server Interaction: Cookies  it is often desirable for a Web site to identify users Four components of cookie technology:  cookie header line in the HTTP response message cookie header line in HTTP request message cookie file kept on user\u0026rsquo;s host and managed by user\u0026rsquo;s browser back-end database at web site   advantages  authorization shopping carts recommendations user session state (Web e-mail)  Cookies and privacy:  Permit sites to learn a lot about you you may supply name and e-mail to sites search engines use redirection \u0026amp; cookies to learn yet more advertising companies obtain info across sites   Web caches (proxy server) Satisfy client request without involving origin server  User sets browser: web accesses via cache browser sends all HTTP requests to cache  object in cache: cache returns object else cache requests object from origin server, then returns object to client  cache acts as both client and server Typically cache is installed by ISP why web caching  Reduce response for client request Reduce traffic Internet dense with caches enables poor content providers to effectively deliver content  Example  Conditional GET: client-side caching Don\u0026rsquo;t send object if client has up-to-date cached version\n client: specify date of cached copy in HTTP (If-Modified-Since \u0026lt;data\u0026gt;) server: response contains no object if cached copy is up-to-date HTTP/1.1 304 Not Modified  FTP (File transfer protocol)  transfer file to/from remote host client/server model  client side: the side that initiates transfer server side: remote host  ftp : RFC 959 ftp server port 21 Procedure  FTP client contacts FTP server at port 21, specifying TCP as transport protocol Client obtains authorization over control connection Client browses remote directory by sending commands over control connection When server receives a command for a file transfer, the server opens a TCP data connection to client After transferring one file, server closes connection Servers open a second TCP data connection to transfer another file  FTP server maintains \u0026ldquo;state\u0026rdquo;: current directory  FTP Commands, responses  commands  USER, username PASS, password LIST, return list of file in current directory RETR filename \u0026ndash; retrieves (gets) file STOR filename \u0026ndash; stores file onto remote host  response  331 Username OK, password required 125 data connection already open, transfer starting 425 can\u0026rsquo;t open data connection 452 Error writing file   Electronic Mail (SMTP, POP3, IMAP) Three major components of a mail system user agents, mail servers ,simple mail transfer protocol: SMTP  User Agent:  known as \u0026ldquo;mail reader\u0026rdquo; composing, editing, reading mail messages outgoing, incoming message stored on server  Mail server:  mailbox: contains incoming messages for user message queue: of outgoing (to be sent) mail messages  protocol  SMTP protocol  client: sending mail receiver server: receiving mail server use TCP to reliably transfer email message from client to server, port25 direct transfer: sending server to receiving server three phases of transfer  handshaking transfer of messages closure  command/response interaction  command: ASCII text response: status code and phrase  SMTP use persistent connections SMTP requires message to be in 7-bit ASCII SMTP server uses CRLF.CRLT to determine end of message （CRLF 表示换行）  comparison with HTTP  HTTP pull protocol (client\u0026rsquo;s point of view), SMTP(push protocol) both have ASCII command/response interaction, status codes HTTP dons not require message to be in 7-bit ASCII HTTP: one object in one response message SMTP: multiple objects can be set in one message   Message format  From, To, Subject MIME(Multipurpose Internet Mail extensions) additional lines in message header declare MIME content type MIME types: (TEXT, Image, Audio, Video, Application, Multipart, Message)  Mail access protocols  SMTP: delivery/storage to receiver\u0026rsquo;s server Mail access protocol: retrieval from server  POP3: Post Office Protocol, version 3 (authorization (agent -\u0026gt; server) and download) IMAP: Internet Mail Access Protocol (RFC 2060) more features (more complex) manipulation of stores messages on server HTTP: Hotmail, Gmail etc    POP3 protocol  client opens a TCP connection to the mail server on port 110 authorization phase (client: declare username, password, server response: +OK) transaction phase: client  list: list message number retr: retrieve message by number dele quit  update phase: mail server deletes the message marked for deletion   download and delete download and keep: copies of messages on different clients POP3 is stateless across sessions  IMAP  keep all messages in one place: the server Allows user to organize messages in folders IMAP keeps user state across sessions (names of folders and mappings between message IDs and folder name)  DNS (domain name system) MAP between IP addresses and name\n A distributed database implemented in hierarchy of many name servers no server has all name-to-IP address mappings Ap application-layer protocol that allows host, routers, name servers to communicate to resolve names(address/name translation)  DNS provides a core Internet function, implemented as application-layer protocol DNS is an example of the Internet design philosophy of placing complexity at network\u0026rsquo;s edge  Services  Mapping Host aliasing (Canonical and alias name) Mail server aliasing Load distribution, Replicated Web servers: set of IP addresses for one canonical name (负载平衡)  Structures  Client wants IP for www.amazon.com  client queries a root server to find com DNS server client queries com DNS server to get amazon.com DNS server Client queries amazon.com DNS server to get IP address for www.amazon.com  Four types of name services  root name servers top level name servers authoritative name servers  for a host: stores that host\u0026rsquo;s IP address, name  local name servers, each ISP, company has local(default) name server, host DNS query first goes to local name server  Top-level domain(TLD) servers: responsible for com, org, net, edu, etc, and all top-level country domains uk, fr, ca, jp  The company Network solutions maintains servers for com TLD The company Educause maintains servers for edu TLD  Authoritative DNS servers: organization\u0026rsquo;s DNS servers, providing authoritative hostname to IP mappings for organization\u0026rsquo;s servers, can be maintained by organization or service provider local name server: does not strictly belong to hierarchy, each ISP has one Example (iterative query)  recursive query (puts burden of name resolution on contacted name server)  caching and updating  once name server learns mapping, it caches mapping, caches entries timeout(disappear) after some time the contents of each DNS servers were configured statically from a configuration file created by a system manager An update option has been added to the DNS protocol to allow data to be added or deleted from the database vid DNS messages  DNS records distributed database storing resource records (RR), RR format: (name, value, type, ttl)  DNS protocol, messages  DNS protocol query and reply messages, both with same message format   Peer-to-Peer Applications File distribution problem (client-server model) Bit Torrent Structure  P2P centralized directory  when peer connects, it informs central server (IP address, content) Single point of failure performance bottleneck copyright infringement   Socket Programming build client/server application communication using sockets\n client/server paradigm types of transport service via socket API (unreliable datagram, reliable, byte stream-oriented) socket: a host-local, application-created, OS-controlled interface(\u0026ldquo;door\u0026rdquo;) into which application can both send and receive message to/from another application process Socket Programming using Java, cross platform without recompiling, easy programming with high-level API  Socket-programming using TCP  Socket: a door between application process and end-to-end transport protocol (UCP or TCP) TCP service: reliable transfer of bytes from one process to another   Client must contact server  server process must first be running server must have created socket (door) that welcomes client\u0026rsquo;s contact client contacts server by  creating client-local TCP socket specifying IP address, port number of server process when client creates socket: client TCP established connection to server TCP   when contacted by client, server TCP creates new socket for server process to communicate with client  allows server to talk with multiple clients source port numbers used to distinguish clients    Stream  A stream is a sequence of characters that flow into or out of a process An input stream is attached to some input source for the process (e.g. keyboard or socket) An output stream is attached to an output source (e.g. monitor or socket)   TCP client  A \u0026ldquo;Socket\u0026rdquo; object, making connection requests Parameters  Remote ip address Remote tcp port Local ip address (optional) Local port (optional)  Example\nimport java.net.*; import java.io.*; public class Client{ private Socket socket = null; private DataOutputStream out = null; private BufferedReader input; public Client(String address, int port) { try { socket = new Socket(address, port); System.out.println(\u0026quot;Connected\u0026quot;); input = new BufferedReader(new InputStreamReader(System.in)); out = new DataOutputStream(socket.getOutputStream()); } catch (UnknownHostException u){ System.out.println(u);} catch (IOException i){ System.out.println(i);} String line = \u0026quot;\u0026quot;; while(!line.equals(\u0026quot;Over\u0026quot;)) { try{ line = input.readLine(); out.writeUTF(line); } catch(IOException i) { System.out.println(i); } } try{ input.close(); out.close(); socket.close(); } catch(IOException i) { System.out.println(i); } } public static void main(String[] args) { Client client = new Client(\u0026quot;127.0.0.1\u0026quot;, 5000); } }  import java.net.*; import java.io.*; public class Server{ private Socket socket = null; private ServerSocket server = null; private DataInputStream in = null; public Server (int port){ try{ server = new ServerSocket(port); System.out.println(\u0026quot;Server started\u0026quot;); socket = server.accept(); System.out.println(\u0026quot;client accepted\u0026quot;); in = new DataInputStream(socket.getInputStream()); String line = \u0026quot;\u0026quot;; while (!line.equals(\u0026quot;Over\u0026quot;)) { try{ line = in.readUTF(); System.out.println(line); } catch (IOException i) { System.out.println(i); } } System.out.println(\u0026quot;Closing connected\u0026quot;); socket.close(); in.close(); } catch (IOException i){ System.out.println(i); } } public static void main(String[] args){ Server server = new Server(5000); } }   Java socket Programming With thread import java.io.*; import java.text.*; import java.util.*; import java.net.*; public class Server { public static void main(String[] args) throws IOException { ServerSocket ss = new ServerSocket(5056); while (true) { Socket s = null; try{ s = ss.accept(); System.out.println(\u0026quot;A new client is connected\u0026quot;); DataInputStream dis = new DataInputStream(s.getInputStream()); DataOutputStream dos = new DataOutputStream(s.getOutputStream()); System.out.println(\u0026quot;Assign new thread for this client\u0026quot;); Thread t = new ClientHandler(s, dis, dos); t.start(); } catch (Exception e){ s.close(); e.printStackTrace(); } } } } class ClientHandler extends Thread{ private DateFormat fordate = new SimpleDateFormat(\u0026quot;yyyy/MM/dd\u0026quot;); private DateFormat fortime = new SimpleDateFormat(\u0026quot;hh:mm:ss\u0026quot;); final private DataInputStream dis; final private DataOutputStream dos; final private Socket s; ClientHandler(Socket s, DataInputStream dis, DataOutputStream dos){ this.dis = dis; this.dos = dos; this.s = s; } @Override public void run(){ String received; String toreturn; while(true){ try{ dos.writeUTF(\u0026quot;what do you want? [Date|TIme]..\\n\u0026quot; + \u0026quot;Type exit to terminate connection\u0026quot;); received = dis.readUTF(); if(received.equals(\u0026quot;Exit\u0026quot;)) { System.out.println(\u0026quot;Client\u0026quot; + this.s + \u0026quot;Sends exist\u0026quot;); System.out.println(\u0026quot;Closing this connection\u0026quot;); this.s.close(); System.out.println(\u0026quot;Connection closed\u0026quot;); break; } Date date = new Date(); switch (received) { case \u0026quot;Date\u0026quot;: toreturn = fordate.format(date); dos.writeUTF(toreturn); break; case \u0026quot;Time\u0026quot;: toreturn = fortime.format(date); dos.writeUTF(toreturn); break; default: dos.writeUTF(\u0026quot;Inavalid input\u0026quot;); break; } } catch (IOException e) { e.printStackTrace(); } } try { this.dis.close(); this.dos.close(); } catch(IOException e) { e.printStackTrace(); } } }  import java.io.*; import java.net.*; import java.util.Scanner; public class Client { public static void main(String[] args) { try { Scanner scn = new Scanner(System.in); InetAddress ip = InetAddress.getByName(\u0026quot;localhost\u0026quot;); Socket s = new Socket(ip, 5056); DataInputStream dis = new DataInputStream(s.getInputStream()); DataOutputStream dos = new DataOutputStream(s.getOutputStream()); while (true) { System.out.println(dis.readUTF()); String tosend = scn.nextLine(); dos.writeUTF(tosend); if (tosend.equals(\u0026quot;Exit\u0026quot;)) { System.out.println(\u0026quot;Closing the connection\u0026quot;); s.close(); System.out.println(\u0026quot;Connection closed\u0026quot;); break; } String received = dis.readUTF(); System.out.println(received); } scn.close(); dis.close(); dos.close(); } catch (IOException e) { e.printStackTrace(); } } }  Socket programming with UDP  no handshaking sender explicitly attaches IP address and port of destination to each packet server must extract IP address, port of sender from received packet transmitted data may be received out of water or lost  Exception Handling during initializing sockets, establishing connection, data transmission  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"26ef14007f80c5f869c4e53a650b4865","permalink":"/courses/computer_network/application_layer/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/computer_network/application_layer/","section":"courses","summary":"Creating a network application, run on different end systems and communicate over a network, no software written for devices in network core, network core devices do not function at application layer (e.","tags":null,"title":"Application Layer","type":"docs"},{"authors":null,"categories":null,"content":" OS services  OS services: Communication, Error detection, User Interface, Resource allocation, Program Execution User Interface  CLI (Command Line Interface) Shell: Command-line interpreter (CSHELL, BASH) GUI (Graphic User Interface) Icons, Mouse  Communication Models  Message passing 例如， process A 与 process B 交换信息： 把 processA 的 信息 copy 到 os 的 memory, 再从 os 把 process A 的信息 copy 到 process B。(Protection 机制, 需要使用 system call)， 缺点是较慢 Shared memory 有一块公用的 memory, 两个 process 可以直接使用，需要先通过 system call 分配好。 (Multi-thread programming)， 缺点是有可能有 dead-lock problem   System Calls \u0026amp; API Request OS services: Process control, File management, Device management, Information maintenance, Communication\n System calls (简单， bug-free)  OS interface to running program An explicit request to the kernel made via a software interrupt Available as assembly-language instructions  API: Application Program Interface (方便使用者, simplicity, portability, efficiency)  Users mostly program against API instead of system call Commonly implemented by language libraries, e.g. C library An api call could involve zero or multiple system call malloc() and free() use system call brk() abs() don\u0026rsquo;t need to involve system call  不会直接产生 Interrupt, 由 system call 产生 interrupt Three most common APIs:  Win32API for windows POSIX API for POSIX-based systems (Portable Operating System Interface for Unix) 所有的 API 都一样， 但是实作(Library)不一样 Java API for the java virtual machine  System calls: passing parameters  Pass parameters in registers Store the parameters in a table in memory, and the table address is passed as a parameter in a register Push the parameters onto the stack by the program, and pop off the stack by operating system   System Structure User goals and system goals is different User goals: easy to use and learn, as well as reliable, safe and fast System goals: easy to design, implement and maintain as well as reliable, error-free, and efficient\n Simple OS Architecture  unsafe, difficult to enhance   Layered OS Architecture  lower levels independent of upper levels Easy to debugging and maintenance less efficient, difficult to define layers   Microkernel OS (有新的系统朝着这个方向做）  Moves as much from the kernel info \u0026ldquo;user\u0026rdquo; space Communications is provided by message passing, 避免 synchronization Easier for extending and porting   Modular OS Architecture  Modern OS implement kernel modules Object-oriented Approach Each core component is separate Each to talk each other over known interfaces Each is loadable as needed within the kernel   Virtual Machine Hard: Critical Instruction，指的是一些指令在 User space 和 Kernel space 执行的结果不一样， 因此需要知道一些指令是否是 critical instruction.  虚拟化指的是如何加 virtual-machine implementation layer。 工作流程: 虚拟机的Kernel 在运行的时候是假设是在 kernel space, 因此才可以执行privileged instruction. 但从架构来说，虚拟机的 kernel 应该是存在于 user space 中。 Privileged instruction 在 虚拟机的 Kernel space 执行时，系统会产生 一个 signal (exception)， interrupt 会回到底层的OS， 因此系统会得知虚拟机想要执行privileged instruction， 底层的OS 会重复执行这个命令。 缺点: 虚拟机执行效率比较低。 现在很多 CPU 都支持虚拟机， 除了 User mode, kernel mode, 还有 VM mode。 Usage: Protection, compatibility problems (系统兼容)， research and development， honeypot (A virtual honeypot is software that emulates a vulnerable system or network to attract intruders and study their behavior), cloud computing (不需要直接用虚拟机提供， container) Full Virtualization (VMware, 需要有 hardware support, 执行效率才会快) Run in user mode as an application on top of OS Virtual machine believe they are running on bare hardware but in fact are running inside a user-level application Para-virtualization: Xen (存在一个 global zone) Presents guest with system similar but not identical to the guest\u0026rsquo;s preferred systems (Guest must be modified) Hardware rather than OS and its devices are virtualized Java Virtual Machine Code translation, compile 执行完成后, 会有自己的 binary code. 再进行一次translation, 在host os执行。 Just-In-Time(JIT) compliers, 会记录执行的命令，然后 reuse.   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"be3f882d38f59ee357a5355cb0a3f3f5","permalink":"/courses/operating_system/os_structures/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/os_structures/","section":"courses","summary":"OS services  OS services: Communication, Error detection, User Interface, Resource allocation, Program Execution User Interface  CLI (Command Line Interface) Shell: Command-line interpreter (CSHELL, BASH) GUI (Graphic User Interface) Icons, Mouse  Communication Models  Message passing 例如， process A 与 process B 交换信息： 把 processA 的 信息 copy 到 os 的 memory, 再从 os 把 process A 的信息 copy 到 process B。(Protection 机制, 需要使用 system call)， 缺点是较慢 Shared memory 有一块公用的 memory, 两个 process 可以直接使用，需要先通过 system call 分配好。 (Multi-thread programming)， 缺点是有可能有 dead-lock problem   System Calls \u0026amp; API Request OS services: Process control, File management, Device management, Information maintenance, Communication","tags":null,"title":"OS structures","type":"docs"},{"authors":null,"categories":null,"content":" Concept  Program: passive entity: binary stored in disk Process: active entity: a program in execution in memory A process includes:  Code segment Data section \u0026ndash;global variables Stack \u0026ndash; temporary local variables and functions Heap \u0026ndash; dynamic allocated variables or classes Current activity (program counter, register contents) 用来管理 process A set of associated resources (e.g. open file handlers)  Process in memory  Threads  lightweight process All threads belonging to the same processes share code section, data section and OS resources (e.g. open files and signals) Each thread has it own thread ID, program counter, register set and stack  Process States  New: process is being created Ready: process is in the memory waiting to be assigned to a processor, 放到 waiting queue 中 Running: instructions are being executed by CPU Waiting: the process is waiting for events to occur (e.g. IO) Terminated: the process has finished execution    Process switch  Process control Block  context switch Context-switch time is purely overhead Switch time depends on memory speed, number of registers, existence of special instructions (a single instruction to save/load all registers), hardware support (multiple sets of registers)   Process Scheduling  Concept  Multiprogramming: CPU runs process at all times to maximize CPU utilization Time sharing: switch CPU frequently such that users can interact with each program while it is running Processes will have to wait until the CPU is free and can be re-scheduled  Scheduling Queues  Job queue (New State) \u0026ndash; set of all processes in the system ready queue (Ready state) \u0026ndash; set of all processes residing in main memory, ready and waiting to execute device queue (Wait state) \u0026ndash; set of processes waiting for an I/O device  Diagram  Schedulers  Short- term scheduler (CPU scheduler) \u0026ndash; selects which process should be executed and allocated CPU (Ready state $\\longrightarrow$ Run state) 很频繁 Long-term scheduler (job scheduler) \u0026ndash; selects which processes should be loaded into memory and brought into ready queue (New state $\\longrightarrow$ Ready state) Middle-term scheduler \u0026ndash; selects which processes should be swapped in/out memory (Ready state $\\longrightarrow$ Wait state) 与 virtual memory 相结合   Long-Term Scheduler (现在memory 很大， 现在变为 middle-term scheduler)  control degree of multiprogramming (Degree 很少时，cpu 会 idle， Degree 很多时， 会竞争CPU 资源) Execute less frequently (e.g. invoked only when a process leaves the system or once several minutes) Select a good mix of CPU-bound \u0026amp; I/O- bound processes to increase system overall performance  Short-Term Scheduler  Execute quite frequently (e.g. once per 100ms) Must be efficient (averaging wait time)   Medium-Term Scheduler  Swap out: removing processes from memory to reduce the degree of multiprogramming Swap in: reintroducing swap-out processes into memory Purpose: improve process mix, free up memory Most modern OS doesn\u0026rsquo;t have medium-term scheduler because having sufficient physical memory or using virtual memory   Operations on Processes Tree of process Each process is identified by a unique processor identifier (pid)  ps-ael will list complete info of all active processes in unix \nProcess Creation  Resource sharing  Parent and child processes share all resources Child process shares subset of parent\u0026rsquo;s resources parent and child share no resources  Two possibilities of execution  Parent and children execute concurrently parent waits until children terminate  Two possibilities of address space  Child duplicate of parent (sharing variables) Child has a program loaded into it (message passing)   UNIX/LINUX Process Creation  fork system call  Create a new child process The new process duplicates the address space of its parent Child \u0026amp; parent execute concurrently after fork Child: return value of fork is 0 Parent: return value of fork is PID of the child process  execlp system call Load a new binary file into memory, destroying the old code (memory content reset) wait system call The parent waits for one of its child processes to complete Memory space of fork(), A 调用 fork() 时  old implementation: A\u0026rsquo;s child is an extra copy of parent current implementation: use copy-on-write technique to store differences in A\u0026rsquo;s child address space    Process Termination  Terminate when the last statement is executed or exit() is called Parent may terminate execution of children processes by specifying its PID (abort) Cascading termination killing (exiting) parent $\\longrightarrow​$ killing all its children  Control-C , OS在启动时，会产生 console process, 在执行程序时，是 console 再去产生其他程序，按 Control-C 时是用 console 来终止程序\n kill, 通过 OS 来终止程序, 需要权限\n Interprocess Communication (IPC)  IPC: a set of methods for the exchange of data among multiple threads in one or more processes Independent process: cannot affect or be affected by other process Cooperating process: otherwise Purposes: information sharing, computation speedup, convenience, modularity  Communication Methods  shared memory:  Require more careful user synchronization implemented by memory access: faster speed use memory address to access data  Message passing:  No conflict : more efficient Use send/recv message Implemented by system call : slower speed  Sockets:  A network connection identified by IP \u0026amp; port (port number is process) Exchange unstructured stream of bytes  Remote Procedure Calls:  Cause a procedure to execute in another space Parameters and return values are passed by message    Shared Memory  Establishing a region of shared memory  Typically, a shared-memory region resides in the address space of the process creating the shared-memory segment Participating processes must agree to remove memory access constraint from OS  Determining the form of the data and the location Ensuring data are not written simultaneously by processes Consumer and Producer problem (系统里面都有很多这样的问题, Compile(Producer), Link(Consumer))\n Producer process produces information that is consumed by a Consumer process Buffer as a circular array with size B next free: in first available : out empty: in = out full (in + 1) % B = out The solution allows at most (B-1) item in the buffer, otherwise, cannot tell the buffer is fall or empty while(1){ while (((in+1) % BUFFER_SIZE) == out); // wait if buffer is full buffer[in] = nextProduced; in = (in + 1) % BUFFER_SIZE; } while(1){ while(in == out); // wait if buffre is empty nextConsumed = buffer[out]; out = (out+1) % BUFFER_SIZE }    Message-Passing System  Mechanism for processes to communicate and synchronize their actions IPC facility provides two operations:  Send (Message) \u0026ndash; message size fixed or variable Receive(Message)  Message system \u0026ndash; process communicate without resorting to shared variables To communicate, processes need to  Establish a communication link Physical (HW bus, network) Logical (logical properties) 1. Direct or indirect communication 2. Blocking and non-blocking Exchange a message via send/receive  Direct Communication  Process must name each other explicitly Links are established automatically One-to-One relationship between links and processes The link may be unidirectional, but is usually bi-directional  Limited modularity, if the name of a process is changed, all old names should be found\n  Indirect communication (E-mail)  Messages are directed and received from mailboxes Each mailbox has a unique ID Processes can communicate if the share a mailbox Send(A, message) , Receive(A, message), communicate through mailbox A Many-to-Many relationship between links and processes Link established only if processes share a common mailbox Mailbox can be owned either by OS or processes MailBox 怎么解决一对一的问题？ Solutions: Allow a link to be associated with at most two processes Allow only one process at a time to execute a receive operation Allow the system to select arbitrarily a singe receiver. Sender is notified who the receiver was  Synchronization  Message passing may be either blocking (synchronous) or non-blocking (asynchronous) Blocking send: send is blocked until the message is received by receiver or by the mailbox Nonblocking send: sender sends the message and resumes operation Blocking receive: receiver is blocked until the message is available Nonblocking receive: receiver receives a valid message or a null (存在一个 token, 来判断是否收到信息)  Buffer implementation (中间存在一个 buffer 来进行消息的储存)  Zero capacity: blocking send/receive Bounded capacity: if full, sender will be blocked Unbounded capacity: sender never blocks  Sockets  Remote Procedure Calls: RPC Stubs, client-side proxy for the actual procedure on the server   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"914e48f3e3ba98aa77f952fc8f3da09f","permalink":"/courses/operating_system/process/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/process/","section":"courses","summary":"Concept  Program: passive entity: binary stored in disk Process: active entity: a program in execution in memory A process includes:  Code segment Data section \u0026ndash;global variables Stack \u0026ndash; temporary local variables and functions Heap \u0026ndash; dynamic allocated variables or classes Current activity (program counter, register contents) 用来管理 process A set of associated resources (e.","tags":null,"title":"Process","type":"docs"},{"authors":null,"categories":null,"content":" Background  Main memory and registers are the only storage CPU can access directly Collection of processes are waiting on disk to be brought into memory and be executed Multiple programs are brought into memory to improve resource utilization and response time to users A process may be moved between disk and memory during its execution Multistep processing of a program   Address Binding  Address Binding - Compile Time  Program is written as symbolic code Compiler translates symbolic code into absolute code If starting location changes (recompile)  Address Binding - Load Time  Complier translates symbolic code into relocatable code Relocatable code: machine language that can be run from any memory location If starting location changes (reload the code)  Address Binding - Execution Time  Compiler translates symbolic code into logical-address (virtual-address) code Special hardware (MMU memory management unit) is needed for this scheme Most general-purpose OS use this method  MMU(Memory-management unit)  Hardware device that maps virtual to physical address The value in the relocation register is added to every address generated by a user process at the time it is sent to memory   Logical VS. Physical Address  Logical Address — generated by CPU (a.k.a virtual address) Physical address — seen by the memory module Compile-time \u0026amp; load time address binding (logical address = physical address) Execution-time address binding (logical address $\\neq$ physical address) The user program deals with logical addresses; it never sees the real physical addresses   Dynamic loading  The entire program doesn\u0026rsquo;t need all memory for it to execute, it\u0026rsquo;s a routine is loaded into memory when it is called Better memory-space utilization, unused routine is never loaded, particularly useful when large amounts of code are infrequently used (e.g., error handling code) No special support from OS is required implemented through program (library, API calls) Dynamic Loading Example in C\n dlopen(): opens a library and prepares it for use desym(): looks up the value of a symbol in a given opened library dlclose(): close a DL library\n#include \u0026lt;dlfcn.h\u0026gt; int main() { double (*cosine)(double); void* handle = dlopen(\u0026quot;/lib/libm.so.6\u0026quot;, RTLD_LAZY); cosine = dlsym(handle, \u0026quot;cos\u0026quot;); printf(\u0026quot;%f\\n\u0026quot;, (*cosine)(2.0)); // load into memory dlclose(handle); }    Static/Dynamic Linking  Static Linking: libraries are combined by the loaded into the program in-memory image  Waste memory: duplicated code Faster during execution time   Dynamic Linking: Linking postponed until execution time  Only one code copy in memory and shared by everyone A stub is included in the program in-memory image for each lib reference Stub call $\\rightarrow$ check if the referred lib is in memory $\\rightarrow$ if not, load the lib $\\rightarrow$ execute the lib DLL (Dynamic link library) on Windows    Swapping  A process can be swapped out of memory to a backing store, and later brought back into memory for continuous execution, also used by midterm scheduling, different from context switch Backing store: a chunk of disk, separated from file system, to provide direct access to these memory images Free up memory, roll out, roll in, swap lower-priority process with a higher one Swap back memory location  if binding is done at compile / load time, swap back memory address must be same if binding is done at execution time, swap back memory address can be different  A process to be swapped == must be idle (不能做I/O)  Imagine a process that is waiting for I/O is swapped? 1. Never swap a process with pending I/O 2. I/O operations are done through OS buffers (i.e. a memory space not belongs to any user processes)  Major part os swap time is transfer time; total transfer time is directly proportional to the amount of memory swapped  Memory allocation  Fixed-partition allocation (规划停车场) Each process loads into one partition of fixed-size, degree of multi-programming is bounded by the number of partitions Variable-size partition Hole: block of contiguous free memory, holes of various size are scattered in memory  Multiple Partition (Variable-size) method  First-fit allocate the 1st hole that fits Best-fit allocate the smallest hole that fits (must search through the whole list) Worst-fit allocate the largest hole (must also search through the whole list) First-fit and best-fit better than worst-fit in terms of speed and storage utilization  Fragmentation (存在零碎的空间)  external fragmentation Total free memory space is big enough to satisfy a request, but is not contiguous, occur in variable-size allocation Internal fragmentation  Memory that is internal to a partition but is not being used, occur in fixed-partition allocation  Solution: compaction  Shuffle the memory contents to place all free memory together in one large block at execution time Only if binding is done at execution time    Non-contiguous memory Allocation - Paging  Divide physical memory into fixed-sized blocks called frames Divide logical address space into blocks of the same size called pages To run a program of n pages, need to find n free frames and load the program keep track of free frames Set up a page table to translate logical to physical addresses Benefit  Allow the physical-address space of a process to be noncontiguous Avoid external fragmentation Limited internal fragmentation Provide shared memory/pages   Page table  Each entry maps to the base address of a page in physical memory A structure maintained by OS for each process  page table includes only pages owned by a process A process cannot access memory outside its space    Address Translation Scheme  Logical address is divided into two parts  Page number (p) Use as an index into a page table which contains base address of each page in physical memory, N bits means a process can allocate at most $2^{N}$ pages Page offset(d) Combines with base address to define the physical memory address that is sent to the memory unit, N bits means the page size is $2^{N}$  Physical address = page base address + page offset example: If page size is 1KB(2^10) and page 2 maps to frame 5. Given 13 bits logical address: (p=2, d=20), what is physical address? $$5*(1KB) + 20 = 1,010,000,000,000 + 0,000,010,100 = 1,010,000,010,100$$ Figure  total number of pages dose not need to be the same as the total number of frames Given 32 bits logical address, 36 bits physical address and 4KB page size, what does it mean?  Page table size: $2^{32} / 2^{12} = 2^{20}$ entries Max program memory: $2^{32} = 4GB$ Total physical memory size: $2^{36} = 64GB$ Number of bits for page number: $2^{20}$ pages $\\rightarrow 20$ bits Number of bits for frame number: $2^{24} \\text{frames} \\rightarrow 2^24$ bits\n number of bits for page offset: 4KB page size = $2^{12}$ bytes $\\rightarrow 12$  Page / Frame Size  Typically power of 2 Ranging from 512 bytes to 16 MB/ page, 4KB/8KB page is commonly used Larger page size $\\rightarrow$ More space waste page sizes have grown over time , memory, process, data sets have become larger   Paging Summary  Paging helps separate user\u0026rsquo;s view of memory and the actual physical memory User view\u0026rsquo;s memory: one single contiguous space OS maintains a copy of the page table for each process OS maintains a frame table for managing physical memory  One entry for each physical frame Indicate whether a frame is free or allocated If allocated, to which page of which process or processes   Implementation of Page Table  Page table is kept in memory Page table base register (PTBR) (需要load到MMU的register)  The physical memory address of the page table The PTBR value is stored in PCB (Process Control Block) Changing the value of PTBR during Context-switch  With PTBR, each memory reference results in 2 memory reads, one for the page table and one for the real address The 2-access problem can be solved by, Translation Look-aside Buffers(TLB) which is implemented by Associative memory Associative Memory All memory entries can be accessed at the same time, lookup time is O(1), each entry corresponds to an associative register, the number of entries are limited (64 ~ 1024)  TLB is a cache for page table shared by all processes  TLB must be flushed after a context switch, otherwise, TLB entry must has a PID field (address-space identifiers (ASIDs)), the flush method is preferred. Effective Memory-Access Time (EMAT)  $20 ns$ for TLB search $100 ns$ for memory access $70%$ TLB hit-ratio  $70$ TLB hit-ratio $\\text{EMAT} = 0.7\\times (20+100) + (1-0.7) \\times (20+100+100) = 150ns$ $98%$ TLB hit-ration $\\text{EMAT} = 0.98\\times 120+0.02 \\times 220=122ns$    Memory Protection  Each page is associated with a set of protection bit in the page table (A bit to define read/write/execution permission)\n Common use : valid-invalid bit  Valid: the page/frame is in the process\u0026rsquo; logical address space, and is thus a legal page Invalid: the page/frame is not in the process\u0026rsquo; logical address space potential issues: Un-used page entry cause memory-waste, use page table length register(PTLR) Process memory may NOT be on the boundary of a page, memory limit register is still needed   Shared Pages Paging allows processes share common code, which must be reentrant  Reentrant code (pure code): it never change during execution, text editors, compilers, web servers, etc Only one copy of the shared code needs to be kept in physical memory Two (several) virtual addresses are mapped to one physical address Process keeps a copy of its own private data and code Shared code must appear in the same location in the logical address space of all processes    Page Table Memory Structure  Page table could be huge and difficult to be loaded  4GB ($2^{32}$) logical address space with 4KB ($2^{12}$) page, needs 1 million $2^{20}$ page table entry Assume each entry need 4 bytes (32 bits), total size = 4MB (MMU 读的时候是需要连续的4MB memory) Need to break it into several smaller page tables, better within a single page size (i.e. 4KB) Or reduces the total size of page table   Hierarchical Paging  Break up the logical address space into multiple page tables  12-bit offset (d) $\\rightarrow$ 4KB($2^{12}$) page size 10-bit outer page number $\\rightarrow$ 1K $2^{10}$ page table entries 10-bit inner page number $\\rightarrow$ 1K $2^{10}$ page table entries   Two-level paging example(32-bit address with 4KB ($2^{12}$) page size)  example:  64-bit Address?  2 level: 42(p1) + 10 (p2) + 12 (offset), outer table requires $2^{42} \\times 4B = 16 {TB}$ contiguous memory 6 level: $12(p1) + 10(p2) + 10(p3) + 10(p4) + 10 (p5) + 12 (offset)$, needs 6 memory accesses  SPAPC(32-bit) and linux use 3-level paging, Motorola 68030 (32-bit) use 4-level paging    Hashed Page Table  Commonly-used for address \u0026gt; 32 bits Virtual page number is hashed into a hash table The size of the hash table varies, larger hash table $\\rightarrow$ smaller chains in each entry Each entry in the hashed table contains  Virtual Page Number, Frame Number, Next Pointer Pointers waster memory Traverse linked list waste time \u0026amp; cause additional memory references  将 entries group 到一起， 存入连续的空间，MMU一次性读进去，可以减少 LinkedList traverse 的时间\n   Inverted Page Table (很少见）  Maintains NO page table for each process ( 节省 memory 空间） Maintains a frame table for the whole memory, one entry for each real frame of memory Each entry in the frame table has (PID Page number) Eliminate the memory needed for page tables but increase memory access time, each access needs to search the whole frame table (use hashing for the frame table) Hard to support shared/page memory   Segmentation  Memory-management scheme that supports user view of memory A program is a collection of segments. A segment is a logical unit includes following:   Segmentation Table  Logical address: (segmentation #, offset), offset has the same length as physical address Maps two-dimensional physical addresses, each table entry has:  Base (4 bytes): the start physical address Limit (4 bytes): the length of the segment  Segment-table base register(STBR), the physical address of the segmentation table Segment-table length register (STLR), the number of segments example   Segmentation Hardware  Limit register is used to check offset length MMU allocate memory by assigning an appropriate base address for each segment (physical address cannot overlap between segments)  Sharing and Protection  Protection bits associated with segments  Read-only segment (code) Read-write segments (data, heap, stack) Code sharing occurs at segment level (memory communication, shared library) Share segment by having same base in two segment tables    Segmentation \u0026amp; paging  Apply segmentation in logical address space Apply Paging in physical address space   Address Translation  CPU generates logical address  Given to segmentation unit $\\rightarrow$ produces liner address Linear address given to paging unit $\\rightarrow$ generates physical address in main memory  Segmentation and paging units form equivalent of MMU   Example  Let the physical memory size is 521B, the page size is 32B and the logical address of a program can have 8 segments. Given a 12 bits hexadecimal logical address \u0026ldquo;448\u0026rdquo;, translate the address with below page and segment tables   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1085b2813425cd3a80d37cbe197b3cab","permalink":"/courses/operating_system/memory_manage/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/memory_manage/","section":"courses","summary":"Background  Main memory and registers are the only storage CPU can access directly Collection of processes are waiting on disk to be brought into memory and be executed Multiple programs are brought into memory to improve resource utilization and response time to users A process may be moved between disk and memory during its execution Multistep processing of a program   Address Binding  Address Binding - Compile Time  Program is written as symbolic code Compiler translates symbolic code into absolute code If starting location changes (recompile)  Address Binding - Load Time  Complier translates symbolic code into relocatable code Relocatable code: machine language that can be run from any memory location If starting location changes (reload the code)  Address Binding - Execution Time  Compiler translates symbolic code into logical-address (virtual-address) code Special hardware (MMU memory management unit) is needed for this scheme Most general-purpose OS use this method  MMU(Memory-management unit)  Hardware device that maps virtual to physical address The value in the relocation register is added to every address generated by a user process at the time it is sent to memory   Logical VS.","tags":null,"title":"Memory Management","type":"docs"},{"authors":null,"categories":null,"content":" Transport services and protocols  provide logical communication between app processes running on different hosts transport protocols run in end systems  send side: breaks app message into segments, passes to network layer receive side: reassembles segments into message passes to app layer  more than one transport protocol available to apps (Internet: TCP and UDP)  network layer vs. transport layer  network layer: logical communication between hosts transport layer: logical communication between processes  reliable, in-order delivery(TCP), congestion control, flow control, connection setup unreliable, unordered delivery(UDP): no-frills extension of \u0026ldquo;best-effort\u0026rdquo; IP services not available: delay guarantees, bandwidth guarantees  Multiplexing / demultiplexing  demultiplexing: delivering received segments to correct socket multiplexing: gathering data from multiple sockets, enveloping data with header(later used for demultiplexing)  Demultiplexing  Host receive IP datagrams:  each datagram has source IP address, destination IP address each datagram carries 1 transport-layer segment each segment has source, destination port number  host uses IP addresses \u0026amp; port numbers to direct segment to appropriate socket   Connectionless demultiplexing  create sockets with port number UDP socket identified by two-tuple(destination IP address and destination port number) when host receives UDP segment: checks destination port number in segment, directs UDP segment to socket with that port number IP datagrams with different source IP addresses and/or source port numbers directed to same socket  Connection-oriented demultiplexing  TCP socket identified by 4-tuple: source/destination IP address, source/destination port number receiving host uses all four segment to appropriate socket Server host may support many simultaneous TCP sockets, each socket identified by its own 4-tuple Web servers have different sockets for each connection client, non-persistent HTTP will have different socket for each request   UDP: User Datagram Protocol (RFC 768)  \u0026ldquo;no frills\u0026rdquo;, \u0026ldquo;bare bones\u0026rdquo; Internet transport protocol \u0026ldquo;best effort\u0026rdquo; service connectionless, no handshaking between UDP sender, receiver, each UDP segment handled independently of others why using UDP:  no connection establishment simple: no connection state at sender, receiver small segment header no connection control: UDP can blast away as fast as desired  often used for streaming multimedia apps other UDP uses (DNS, SNMP(Simple network manager protocol) 网络管理资料收集) reliable transfer over UDP: add reliability at application layer(application-specific error recovery)   UDP checksum  Goal: detect \u0026ldquo;errors\u0026rdquo; in transmitted segment sender:  treat segment contents as sequence of 16-bit integers checksum: addition (1\u0026rsquo;s complement sum) of segment contents sender puts checksum value into UDP checksum fields  Receiver:  compute checksum of received segment check if computed checksum equals checksum field value (No, error-detected, Yes, no error-detected)   Principles of Reliable data transfer  top-10 list of important networking topics  characteristics of unreliable channel will determine complexity of reliable data transfer protocol rdt_send(): called from above, passed data to deliver to receiver upper layer udt_send(): called by rdt, to transfer packet over unreliable channel to receiver rdt_rcv(): called when packet arrives on rcv-side of channel deliver_data() called by rdt to deliver data to upper  Reliable data transfer  incrementally develop sender, receiver sides of reliable data transfer protocol(rdt) consider only unidirectional data transfer(but control info will flow on both directions) use finite state machines(FSM) to specify sender, receiver  Rdt1.0: reliable transfer over a reliable channel  underlying channel perfectly reliable  no bit errors no loss of packets  separate FSMs for sender, receiver  sender sends data into underlying channel receiver read data from underlying channel    Rdt2.0: channel with bit errors  underlying channel may flip bits in pocket: checksum to detect bit errors how to recover from errors:  acknowledgements(ACKs) receiver explicitly tells sender that packet received OK negative acknowledgements(NAKs) receiver explicitly tells sender that packet had errors sender retransmits packet on receipt of NAK  new mechanisms in rdt2.0  error detection receiver feedback: control msgs(ACK, NAK)   rdt2.0 has a fatal flow(if ACK/NAK corrupted?)  sender doesn\u0026rsquo;t know what happened at receiver can\u0026rsquo;t just retransmit: possible duplicate  Handling duplicates:  sender retransmits current packet if ACK/NAK corrupted sender adds sequence number to each packet receiver discards (doesn\u0026rsquo;t deliver up) duplicate packet  Stop and wait (sender sends one packet, then waits for receiver response)  Rdt2.1: sender, handles garbled ACK/NAKs  Sender  Receiver   Rdt2.2: a NAK-free protocol  same functionality as rdt2.1, using ACKS only instead of NAK, receiver sends ACK for last packet received OK, receiver must explicitly include sequence number of packet being ACKed duplicate ACK at sender results in same action as NAK: retransmit current packet  Rdt3.0: channels with errors and loss  New assumption:  underlying channel can also pockets(data or ACKs) checksum, sequence number, ACKs, retransmission will be of help, but not enough  Approach: sender waits \u0026ldquo;reasonable\u0026rdquo; amount of time for ACK retransmits if no ACK received in this time if packet (or ACK) just delayed (not lost):  retransmission will be duplicate, but use of sequence number receiver must specify sequence number of packet being ACKed  requires countdown timer  performance of rdt3.0 1Gbps link, 15 ms prop. delay, 8000 bit packet:  sender time $$ d = L/R = \\frac{8000 bits}{10^9bps} = 8 ms$$ utilization - fraction of time sender busy sending $$ \\frac{L/R}{RTT + L/R} = 0.008/30.0008 = 0.00027$$  stop and wait operation   Pipelined protocols  sender allows multiple, \u0026ldquo;in-flight\u0026rdquo; yet-to-be-acknowledged` packets  range of sequence numbers must be increased buffering at sender and/or receiver   Two generic forms of pipelined protocols: go-Back-N, selective repeat  Go-back-N: big picture  sender can have up to N unacked packets in pipeline Receiver only sends cumulative ACKs, doesn\u0026rsquo;t ACK packet if there\u0026rsquo;s gap Sender has timer for oldest unacked packet, if timer expires, retransmit all unlocked packets  ACK(n): ACKs all packet up to including sequence number n \u0026ndash; \u0026ldquo;cumulative ACK\u0026rdquo; (may receive duplicate ACKs) timer for each in-flight packet timeout(n): retransmit packet n and all higher sequence number packets in window  sender  receiver  example   Selective Repeat: big picture  Sender can have up to N unacked packets in pipeline Receiver ACKs individual packets Sender maintains timer for each unacked packet, when timer expires, retransmit only unacked packet  Sender  if next available sequence number in window, send packet timeout(n): resend packet n, restart timer ACK(n) in (sendbase, sendbase + N): mark packet n as reader, if n smallest unACked packet, advance window base to next unACKed sequence number  receiver  send ACK(N) out-of-order : buffer in-order: deliver, advance window to next not-yet-received packet if packet n in [revbase-revbase+N], ACK(n), otherwise ignore   problem window size must be less than or equal t0 half the size of the sequence number space  Connection-Oriented Transport: TCP Overview  Point-to-Point reliable: in-order byte stream: no \u0026ldquo;message boundaries\u0026rdquo; pipelined: TCP congestion and flow control set window size send \u0026amp; receive buffer flow control: sender will not overwhelm receiver full duplex data: bi-directional data flow in some connection, maximum segment size(MSS， 控制流量，控制传输速度) connection-oriented: handshaking (Exchange of control messages) TCP segment structure  Sequence number: byte stream \u0026ldquo;number\u0026rdquo; of first byte in segment\u0026rsquo;s data ACKs: sequence number of next byte expected from other side, cumulative ACK   Round-Trip Time Estimation and Timeout  how receiver handles out-of-order segments: TCP spec doesn\u0026rsquo;t say, up to implementor how to set TCP timeout value?  longer than RTT(Round Trip Time) too short: premature timeout (unnecessary retransmission) too long: slow reaction to segment loss  how to estimate RTT?  SampleRTT: measured time from segment transmission until ACT receipt (ignore retransmissions) SampleRTT will vary, want estimated RTT \u0026ldquo;smoother\u0026rdquo;, average several recent measurement, not just current SampleRTT  EstimatedRTT = $(1-\\alpha)\\cdot \\text{EstimatedRTT} + \\alpha \\cdot \\text{SampleRTT} \\text{ typically}, \\alpha$ is 0.125 estimate of how much SampleRTT deviates from EstimatedRTT $DevRTT = (1-\\beta) \\cdot \\text{DevRTT} \\cdot \\beta \\cdot |\\text{SampleRTT}-\\text{Estimated RTT}|$, typically, $\\beta = 0.25$ set timeout interval TimeoutInterval = EstimatedRTT + 4 * DevRTT  TCP sender events  data received from application:  create segment with sequence number sequence number is a byte-stream number of first data byte in segment start timer if not already running (think of timer as for oldest unacked segment) expiration interval: timeout Interval  timeout: retransmit segment that caused timeout, restart timer ACK received: if acknowledges previously unacked segments, update what is known to be acked, start timer if there are outstanding segments figure:  TCP retransmission Scenarios   TCP ARK Generation Fast retransmit  Time-out period often relatively long (long delay before resending lost packet) detect lost segment via duplicate ACKs (If segment is lost, there will likely be many duplicate ACKs) if Sender receives ACKs for the same data, it supposes that segment after ACKed data was lost fast retransmit: resend segment before timer expires  Algorithm   Flow Control  Sender won\u0026rsquo;t overflow receiver\u0026rsquo;s buffer by transmitting too much or too fast Receive side of TCP connection has a receive buffer  Suppose TCP receiver discards out-of-order segments Receiver advertises spare room by including value of RcvWindow in segments Sender limits unACKed data to RcvWindow rwnd(receive window) = RcvBuffer - [LastByteRcvd - LastByteRead]  TCP Connection Management  Three way handshake  Client host sends TCP SYN segment to server, specifies initial sequence number (no data) Server host receivers SYN, replies with SYNACK segment, server allocates buffers, specifies server initial sequence number client receives SYNACK, replies with ACK segment, which may contain data   Closing a connection  client end system send TCP FIN control segment to server server receives FIN, replies with ACK, closes connection, sends FIN client receives FIN, replies with ACK, enters \u0026ldquo;timed wait\u0026rdquo; will respond with ACK to received FINs server, receives ACK, connection closed   Figure   TCP congestion control Principles of Congestion Control  too many sources sending too much data too fast for network to handle manifestations: lost packets, long delays a top-10 problem Cause/costs of congestion: scenario 1, two connections sharing a single hop with infinite buffers, maximum achievable throughput, large delays when congested  scenario 2, one router, finite buffers, and retransmission  always: $\\lambda{in} = \\lambda{out}$ \u0026ldquo;perfect\u0026rdquo; retransmission only when loss: $\\lambda{in}\u0026rsquo; = \\lambda{out}$ retransmission of delayed (not lost) packet makes $\\lambda{in}\u0026lsquo;$ larger (than perfect case) for same $\\lambda{out}$  Scenario 3, four senders, multi-hop paths, timeout/retransmit  even more crowded when packet dropped, any upstream transmission capacity used for that packet was wasted (之前走过的路径全部浪费掉)   Approaches towards congestion control  End-end congestion control  no explicit feedback from network congestion inferred from end-system observed loss, delay approach taken by TCP  Network-assisted congestion control:  routers provide feedback to end system single bit indicating congestion (SNA, TCP/IP ECN, ATM) explicit rate sender should send  CASE: ATM (ABR congestion control)  ABR: available bit rate, elastic service if sender\u0026rsquo;s path \u0026ldquo;underloaded\u0026rdquo;, sender should use available bandwidth if sender\u0026rsquo;s path congested: sender throttled to minimum guaranteed rate RM (resource management) cells: sent by sender, interspersed with data cells bit in RM cell set by switches: NI bit: no increase in rate; CI bit: congestion indication RM cells returned to sender by receiver, with bits intact  EFCI bit in data cells: set to 1 in congested switch, if data cell preceding RM cell has EFCI set, receiver sets CI bit in returned RM cell   TCP congestion control  Congestion window(cwnd): imposes a constraint on the rate at which a TCP sender can send traffic into the network, LastByteSent – LastByteAcked \u0026lt;= cwnd MSS: maximum segment size: the maximum amount of data that can be grabbed and placed in a segment roughly rate = CongWin / RTT (Bytes/sec) How does sender perceive congestion?  loss event = timeout or 3 duplicate ACKs TCP sender reduces rate (cwnd) after loss event  three mechanisms:  AIMD slow start conservative after timeout events   AIMD  Approach: increase transmission rate (window size), probing for usable bandwidth, until loss occurs Additive increase: increase CongWin by 1 MSS every RTT until loss detected multiplicative decrease: cut CongWin in half after loss   Slow start  when connections begins, CongWIn = 1MSS available bandwidth may be \u0026gt;\u0026gt; MSS/RTT when connection begins, increase rate exponentially fast until first loss event  double congwin every RTT done by incrementing congwin for every ACK received   inferring loss  After 3 duplicate ACKs, cwnd is cut in half, window the grows linearly after timeout event, cwnd set to 1 MSS, window then grows exponentially to a threshold, then grows linearly    TCP Summary  TCP throughput: Let W be the window size when loss occurs, when window is W, throughput is W/RTT, just after loss, windows drops to W/2, throughput is W/(2RTT), average throughput 0.75W/RTT TCP future: TCP over \u0026ldquo;long, fat pipes\u0026rdquo; 1500 byte segments, 100ms RTT, want 10Gbps throughputs, requires window size W = 83.333 in flight ($W \\cdot 1500 \\cdot 8 / 100ms = 10Gbps$), throughput in terms of loss rate: $1.22\\cdot MSS/(RTT\\sqrt{L})$ , $L = 2\\cdot 10^{-10}$ TCP Fairness, if K TCP sessions share same bottleneck link of bandwidth R, each should have average rate of R/K, the TCP is fair   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"001142840c0377c3efc57ca5483dfda3","permalink":"/courses/computer_network/transport_layer/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/computer_network/transport_layer/","section":"courses","summary":"Transport services and protocols  provide logical communication between app processes running on different hosts transport protocols run in end systems  send side: breaks app message into segments, passes to network layer receive side: reassembles segments into message passes to app layer  more than one transport protocol available to apps (Internet: TCP and UDP)  network layer vs.","tags":null,"title":"Transport Layer","type":"docs"},{"authors":null,"categories":null,"content":" Introduction  transport segment from sending to receiving host on sending side encapsulates segments into datagrams on receiving side, delivers segments to transport layer network layer protocols in every host, router router examines header fields in all IP datagrams passing through it  Two keys  Forwarding: move packets from router\u0026rsquo;s input to appropriate router output  Routing: determine route taken by packets from source to destination, routing algorithm  Connection setup  3rd important function in some network architectures: ATM, frame relay, X.25 before datagrams flow, two end hosts and intervening routers establish virtual connection (routers get involved) network vs transport layer connection service:  network: between two hosts(may also involve intervening routers in case of VCs) transport: between two processes   Network Service model  example services for individual datagrams: guaranteed delivery, guaranteed delivery with less than 40 milliseconds delay example services for a flow of datagrams: in order datagram delivery, guaranteed minimum bandwidth to flow, restrictions on change in inter-packet spacing (封包和封包的间隔的变化(时间差), 变异度) 建连线有上面的这些机制  Virtual circuit and datagram networks connection and connection-less service  datagram network provides network-layer connectionless service VC network provides network-layer connection service analogous to transport-layer services, service: host-to-host; no choice: network providers one or the other(只能选一种); implementation: in network core  Virtual circuits  source-to-dest path behaves like telephone circuit (performance-wise, network actions along source-to-dest path) call setup, teardown for each call before data can flow each packet carries VC identifier (not destination host address) every router on source-dest path maintains \u0026ldquo;state\u0026rdquo; for each passing connection link, router resources(bandwidth, buffers) may be allocated to VC (dedicated resources = predictable service)   VC implementation  VC consists of  path from source to destination VC numbers, one number for each link along path entries in forwarding tables in routers along path  packet belonging to VC carries VC number(rather than destination address) VC number can be changed on each link (New VC number comes from forwarding table)   Datagram Networks  no call setup at network layer routers: no state about end-to-end connections, no network-level concept of \u0026ldquo;connection\u0026rdquo; packets forwarded using destination host address, packets between some source-destination pair may take different paths  forwarding table  longest prefix matching    Datagram or VC network: Why  Internet (datagram)  data exchange among computers, elastic service, no strict timing request smart end systems (computer), can adapt, perform control error recovery, simple inside network, complexity at \u0026ldquo;edge\u0026rdquo; many link types, different characteristics, uniform service difficult  ATM (VC)  evolved from telephony human conversation, strict timing, reliability requirements, need for guaranteed service \u0026ldquo;dumb\u0026rdquo; end systems (telephones, complexity inside network)   What\u0026rsquo;s inside a router - routing algorithms / protocol (RIP, OSPF, BGP) - forwarding algorithms from incoming to outgoing link\nInput Port Functions  given datagram destination, lookup output port using forwarding table in input port memory goal: complete input port processing at line speed queuing: if datagrams arrive faster than forwarding rate into switch fabric  Switching fabrics  switching via a bus:  datagram from input port memory to output port memory via a shared bus bus connection: switching speed limited by bus bandwidth 32 Gbps bus, Cisco 5600: sufficient speed for access and enterprise routers  switching through a crossbar switching via An interconnection network  overcome bus bandwidth limitations Banyan networks, other interconnection nets initially developed to connect processors in multiprocessor advanced design: fragmenting datagram into fixed length cell   Output ports  buffering: required when datagrams arrive from fabric faster than the transmission rate scheduling discipline: chooses among queued datagrams for transmission  queueing (delay) and loss due to output port buffer overflow How much buffering  RFC 3439 rule of thumb: average buffering equal to \u0026ldquo;typical\u0026rdquo; RTT (250 millisecond) times link capacity C (C = 10 Gpbs link, 2.5 Gbit buffer) Recent recommendation: with N flows, buffering equal to $RTT*C/(\\sqrt(N))$  Input port Queuing problem  HOL (head-of-the-line) blocking   IP: Internet Protocol IP datagram format  16-bit identifier, flags, 13-bit fragmentation offset: fragmentation/reassembly time to live: max number remaining hops (decremented at each router) upper layer: upper layer protocol to deliver header checksum  IP Datagram Fragmentation IPv4 addressing  IP address: 32-bit identifier for host, router interface interface: connection between host/router and physical link  router\u0026rsquo;s typically have multiple interfaces host typically has one interface ip address associated with each interface   Subnets  subnet part (high order bits) host part (low order bits) device interfaces with same subnet part of IP address can physically reach other without intervening router  to determine the subnets, detach each interface from its host or router, creating islands of isolated networks, each isolated network is called a subnet six subnets  CIDR: Classless Inter Domain Routing  subnet portion of address of arbitrary length address format: a.b.c.d/x where x is number bits in subnet portion of address  Get IP  hard-coded by system admin in a file, unix: /etc/rc.config DHCP: Dynamic Host Configuration Protocol: dynamically get address from as server, \u0026ldquo;plug-and-play\u0026rdquo;  Get subnet part of IP address: Gets allocated portion of its provider ISP\u0026rsquo;s address space ISP get block of address? ICANN (Internet Corporation for Assigned Names and Numbers)  DHCP: Dynamic Host Configuration Protocol  Goal: allow host to dynamically obtain its IP address from network server when it joins network, can renew its lease on address in use, allows reuse of address (only hold address while connected an \u0026ldquo;on\u0026rdquo;), support for mobile users who want to join network (more shortly) DHCP overview:  host broadcasts \u0026ldquo;DHCP discover\u0026rdquo; message DHCP server respond with \u0026ldquo;DHCP offer\u0026rdquo; message host requests IP address \u0026ldquo;DHCP request\u0026rdquo; message DHCP server sends address \u0026ldquo;DHCP ACK\u0026rdquo; message    Hierarchical addressing more specific route Network Address Translation (NAT)  Motivation: local network use just one IP address as far as outside world is concerned:  range of addresses not needed from ISP: just one IP address for all devices can change addresses of devices in local network without notifying outside world can change ISP without changing addresses of devices in local network devices inside local net not explicitly addressable, visible by outside world (a security plus)  Implementation: NAT router must:  outgoing datagrams: replace (source IP address, port number) of every outgoing datagram to (NAT IP address, new port number), remote clients/ servers will respond using (NAT IP address, )    16 bit port-number field (65536) NAT is controversial  routers should only process up to layer 3 (but involved layer 4) violates end-to-end argument, NAT possibility must be taken into account by app designers (eg. P2P applications) address shortage should instead be solved by IPv6  NAT traversal problem  client wants to connect to server with address 10.0.0.1, server address 10.0.0.1 local to LAN (client can\u0026rsquo;t use it as destination address), only one externally visible NAT address solution 1: statically configure NAT to forward incoming connection requests at given port to server (e.g. 123.76.29.7 port 2500 always forwarded to 10.0.0.1 port 2500) solution2: Universal Plug and Play (UPnP), Internet Gateway Device(IGD) protocol. Allows NATed host to: learn public IP address add /remove port mappings (with lease times) automate static NAT port map configuration solution3: relaying (used in Skype) NATed client establishes connection to relay external client connects to relay relay bridges packets between connections   ICMP (Internet Control Message Protocol)  used by host \u0026amp; routers to communicate network-level information  error reporting unreachable host, network, port, protocol echo request/reply (used by ping)  network-layer above \u0026ldquo;IP\u0026rdquo;, ICMP messages carried in IP datagrams ICMP message: type, code plus first 8 bytes of IP datagram causing error  Traceroute and ICMP  source sends series of UDP segment to destination  first has TTL=1 second has TTL=2, etc unlikely port number  when nth datagram arrives to nth router  router discards datagram and sends to source an ICMP message (type 11, code 0) Message includes name of routers \u0026amp; IP address  when ICMP message arrives, source calculates RTT traceroute does this 3 times Stoping criterion  UDP segment eventually arrives at destination host destination returns ICMP \u0026ldquo;port unreachable\u0026rdquo; packet (type 3, code 3) when source gets this ICMP, stops   IPv6  Initial motivation: 32-bit address space soon to be completely allocated additional motivation:  header format helps speed processing/forwarding header changes to facilitate QoS IPv6 datagram format: fixed-length 40 byte header, no fragmentation allowed   priority: identify priority among datagrams in flow flow label: identify datagrams in same \u0026ldquo;flow\u0026rdquo; next header: identify upper layer protocol for data checksum: removed entirely to reduce processing time at each step options: allowed, but outside of header, indicated by \u0026ldquo;Next Header\u0026rdquo; field ICMPv6: new version of ICMP, additional message types (\u0026ldquo;packet too big\u0026rdquo;), multicast group management functions  Transition From IPv4 to IPv6  not all routers can be upgraded simultaneous, no \u0026ldquo;flag days\u0026rdquo; operate mixed IPv4 and IPv6 routers: Tunneling, IPv6 carried as payload in IPv4 datagram among IPv4 routers   Routing Algorithm  Graph abstraction  Routing Algorithm: algorithm that finds least-cost path Global: all routers have complete topology, link cost info (link-state algorithm) Decentralized  router knows physically-connected neighbors, link costs to neighbors iterative process of computation, exchange info with neighbors \u0026ldquo;distance vector\u0026rdquo; algorithms  static or dynamic?  static: routes change slowly over time dynamic: routes change more quickly (periodic update, in response to link cost changes)   Link-State Routing Algorithm Dijkstra\u0026rsquo;s algorithm (See Algorithm)\n Oscillations with congestion-sensitive routing (link cost = amount of carried traffic)   The Distance-Vector (DV) Routing Algorithm Distance Vector Algorithm (see Algorithm)\n Node x maintains distance vector Node x also maintains its neighbors\u0026rsquo; distance vectors Bellman-Ford equation (dynamic programming) $d_x(y) = min_v (c(x,v) + d_v(y))$ where min is taken over all neighbors v of x  good news travels fast, bad news travels slow  Figure b needs 44 iterations before algorithm stabilizes  Poisoned reverse  If Z routes through Y to get to X: Z tells Y its (Z\u0026rsquo;s) distance to X is infinite (So y won\u0026rsquo;t route to X via Z) loops involving three or more nodes (rather than simply two immediately neighboring nodes) will not be detected by the poisoned reverse technique   Comparison of LS and DV  Message complexity  LS: with n nodes, E links O(nE) message send DV: exchange between neighbors only, convergence time varies  Speed of Convergence  LS: $O(n^2)$ algorithm requires $O(nE)$ messages, may have oscillations DV: convergence time varies, may be routing loops, count-to-infinity  Robustness: what happens if router malfunctions?  LS: node can advertise incorrect link cost, each node computes only its own table DV node can advertise incorrect path cost, each node\u0026rsquo;s table used by others, error propagate through network   Hierarchical routing  scale: with 200 million destinations:  can\u0026rsquo;t store all destination\u0026rsquo;s in routing tables routing table exchange would swamp links  administrative autonomy  internet = network of networks each network admin may want to control routing in its own network  aggregate routers into regions \u0026ldquo;autonomous systems (AS)\u0026rdquo; routers in same AS run same routing protocol  \u0026ldquo;intra-AS\u0026rdquo; routing protocol routers in different AS can run different intra-AS running protocol  Gateway router : direct link to router in another AS  Inter-AS tasks, suppose router in AS1 receives datagram destined outside of AS1: router should forward packet to gateway router, but which one?  AS1 must learn which destination are reachable through AS2, which through AS3 propagate this reachability info to all routers in AS1   Hot-potato routing Suppose AS1 learns from inter-AS protocol that subnet X is reachable from AS3 and from AS2, to configure forwarding table, router 1d must determine towards which gateway it should forward packets for destination x.(hot-potato routing- send packet towards closest of two routers) Routing in the Internet Interior Gateway Protocols (IGP)\nRIP (Routing Information Protocol)  distance vector algorithm included in BSD-UNIX distribution in 1982 distance metric: number of hops (max = 15hops)  exchanged among neighbors every 30 seconds via Response Message (also called advertisement) each advertisement: list of up to 25 destination subnets within AS Link Failure and Recovery: if no advertisement heard after 180 seconds $\\rightarrow$ neighbor/link declared dead  routes via neighbor invalidated new advertisements send to neighbors neighbors in turn send out new advertisements link failure info quickly (?) propagates to entire network poison reverse: used to prevent ping-pong loops (infinite distance = 16 loops)  RIP table processing  RIP routing tables managed by application-level process called route-d (daemon) (Send distance vector) advertisements send in UDP packets, periodically repeated    OSPF (Open Shortest Path First)  open: publicly available uses Link State algorithm:  LS packet dissemination topology map at each node route computation using Dijkstra\u0026rsquo;s algorithm  OSPF advertisement carries one entry per neighbor router advertisements disseminated to entire AS (via flooding), carried in OSPF messages directly over IP (rather than TCP or UDP) advanced features  security: all OSPF messages authenticated(to prevent malicious intrusion) multiple same-cost paths allowed (only one path in RIP) For each link, multiple cost metrics for different TOS (type of service) (e.g. satellite link cost set \u0026ldquo;low: for best effort: high for real time) integrated uni- and multicast support: Multicast OSPF (MOSPF) uses same topology data base as OSPF hierarchical OSPF in large domains   BGP (Border Gateway Protocol) Inter-AS Routing  the de facto standard BGP provides each AS a means to:  Obtain subnet reachability information from neighboring ASs Propagate reachability information to all AS-internal routers Determine \u0026ldquo;good\u0026rdquo; routers to subnets based on reachability information and policy  allows subnet to advertise its existence to rest of Internet Pairs of routers (BGP peers) exchange routing info over semi-permanent TCP connections: BGP sessions (BGP sessions need not correspond to physical links)  When As2 advertise to a prefix to AS1:  AS2 promises it will forward datagrams towards that prefix AS2 can aggregate prefixes in its advertisement   Distributing reachability information:\n Using eBGP(external) session between 3a and 1c, AS3 sends prefix reachability info to AS1  1c can then use iBGP do distribute new prefix info to all routers in AS1 1b can then re-advertise new reachability info to AS2 over 1b-to-2a eBGP session  when router learns of new prefix, it creates entry for prefix in its forwarding table advertised prefix includes BGP attributes: prefix + \u0026ldquo;attributes\u0026rdquo; = \u0026ldquo;route\u0026rdquo; two important attributes  AS-PATH: contains ASs through which prefix advertisement has passed NEXT-HOP: indicates specific internal-AS router to next-hop AS(may be multiple links from current AS to next-hop-AS)  when gateway router receives route advertisement, use import policy to accept/decline  BGP route selection  router may learn about more than 1 route to some prefix, router must select route elimination rules:  local preference value attribute: policy decision shortest AS-PATH closest NEXT-HOP router: hot potato routing additional criteria   BGP messages exchanged using TCP  OPEN: open TCP connections to peer and authenticates sender UPDATE: advertises new path KEEPALIVE: keeps connection alive in absence of UPDATES NOTIFICATION: reports error in previous message also used to close connection  BGP routing policy  A, B, C are provider networks X, W, Y are customer (of provider networks) X is dual-homed: attached to two networks  X dose not want to route from B via X to C, so X will not advertise to B a route to C  A advertises path AW to B B advertises path BAW to X should B advertise path BAW to C?  No, B gets no \u0026ldquo;revenue\u0026rdquo; for routing CBAW since neither W nor C are B\u0026rsquo;s customers B wants to force C route to w via A B wants to route only to/from its customers   Different Intra- and Inter-AS routing  Policy  Inter-AS: admin wants control over how its traffic routed, who routes through its net Intra-AS: single admin, no policy  Scale: hierarchical routing saves table size, reduced update traffic Performance:  Intra-AS: can focus on performance Inter-AS: policy may dominate over performance   Broadcast and multicast routing Broadcast Routing  deliver packets from source to all other nodes source duplication is inefficient  In-network duplication:  flooding: when node receives broadcast packet, sends copy to all neighbors (problem: cycles \u0026amp; broadcast storm) controlled flooding: node only broadcast packet if it hasn\u0026rsquo;t broadcast same packet before, Node keeps track of packet ids already broadcasted; Reverse Path Forwarding(RPF): only forward packet if it arrived on shortest path between node and source spanning tree: no redundant packet received by any node   Spanning Tree - First construct a spanning tree - Nodes forward copies only along spanning tree - Creation: - center node - each node send unicast join message to center node - message forwarded until it arrives at a node already belonging to spanning tree Multicast Routing: Problem statement  find a tree (or trees) connecting routers having local multicast group members  tree: not all paths between routers used source-based: different tree from each sender to receivers shared-tree: same tree used by all group members  sourced-based tree: one tree per source, shorted path trees, reverse path forwarding group-shared tree: group uses one tree (minimal spanning tree, center-based trees)  Shortest Path tree Dijkstra\u0026rsquo;s algorithm\nReverse path forwarding Pruning: forwarding tree contains subtrees with no multicast group members: no need to forward datagrams down subtree, \u0026ldquo;prune\u0026rdquo; message send upstream by router with no downstream group members.\nShared-Tree: Steiner Tree  minimum cost tree connecting all routers with attached group members problem is NP-complete excellent heuristics exists (近似解) not used in practice  computational complexity information about entire network needed monolithic(庞大): rerun whenever a router needs to join/leave   Center-based trees  single delivery tree shared by all one router identified as \u0026ldquo;center\u0026rdquo; of tree to join:  edge router sends unicast join-message addressed to center router join-message \u0026ldquo;processed\u0026rdquo; by intermediate routers and forwarded towards center join-message either hits existing tree branch for this center, or arrives at center path taken by join-message becomes new branch of tree for this router   Internet Multicasting Routing: DVMRP  DVMRP: distance vector multicast routing protocol, RFC1075 flood and prune: reverse path forwarding, source-based tree soft state: DVMRP router periodically (1min) \u0026ldquo;forgets\u0026rdquo; branches are pruned  multicast data again flows down unpruned branch downstream router: re-prune or else continue to receive data  routers can quickly regraft to tree, following IGMP (internet Group Management protocol) join at leaf  Protocol Independent Multicast: RIP  not dependent on any specific underlying unicast routing algorithm (works with all) Dense: group members densely packed, in \u0026ldquo;close\u0026rdquo; proximity, bandwidth more plentiful  group membership by routers assumed until routers explicitly prune data-driven construction on multicast tree (e.g. RPF1) bandwidth and non-group-router processing profligate (浪费)  Sparse: number of networks with group members small w.r.t. number of interconnected networks, group members \u0026ldquo;widely dispersed\u0026rdquo;, bandwidth not plentiful  no membership until routers explicitly join receiver-driven construction of multicast tree (e.g. center-based) bandwidth and non-group-router processing conservative   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"9692cd21063b5cd8e082d6c3f50ff309","permalink":"/courses/computer_network/network_layer/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/computer_network/network_layer/","section":"courses","summary":"Introduction  transport segment from sending to receiving host on sending side encapsulates segments into datagrams on receiving side, delivers segments to transport layer network layer protocols in every host, router router examines header fields in all IP datagrams passing through it  Two keys  Forwarding: move packets from router\u0026rsquo;s input to appropriate router output  Routing: determine route taken by packets from source to destination, routing algorithm  Connection setup  3rd important function in some network architectures: ATM, frame relay, X.","tags":null,"title":"Network Layer","type":"docs"},{"authors":null,"categories":null,"content":" Background  We don\u0026rsquo;t want to run a program that is entirely in memory  Many code for handling unusual errors or conditions Certain program routines or features are rarely used The same library code used by many programs Arrays, lists and tables allocated but not used  Virtual memory — separation of user logical memory from physical memory  To run a extremely large process, logical address space can be much larger than the physical address space To increase CPU/resources utilization (higher degree of multiprogramming degree) To simplify programming tasks (Free programmer from memory limitation) To run programs faster (less I/O would be needed to load or swap)  Virtual memory can be implemented by  Demand paging Demand segmentation: more complicated due to variable size    Demand Paging  A program rather than the whole process is brought into memory only when it is needed  Less I/O needed $\\rightarrow$ Faster response Less memory needed $\\rightarrow$ More users  Page is needed when there is a reference to the page  Invalid reference $\\rightarrow$ abort Not-in-memory $\\rightarrow$ bring to memory via paging  Pure demand paging  Start a process with no page Never bring a page into memory until it is required  A swapper (midterm scheduler) manipulates the entire process, whereas a pager is concerned with individual page of a process Hardware support  Page table : a valid-invalid bit (1 $\\rightarrow$ page in memory, $0 \\rightarrow$ page no in the memory) Secondary memory (swap space, backing store), usually, a high-speed disk (swap device) is use    Page Fault First reference to a page will trap to OS (page fault trap)\n OS looks at the internal table (PCB) to decide  Invalid reference $\\rightarrow$ abort Just not in memory $\\rightarrow$ continue  Get an empty frame Swap the page from disk (swap) space into the frame Reset page table, invalid-valid bit = 1 Restart instruction   Page replacement If there is no free frame when a page fault occurs\n swap a frame to backing store swap a page from backing store into the frame different page replacement algorithms pick different frames for replacement  Demand Paging Performance  Effective Access Time (EAT): $(1-p) \\times ma + p \\times PFT$  P: page fault rate, ma: memory, access time, PFT: page fault time  Example ma=200ns, PFT=8ms  EAT = 200ns + 7999800 ns $\\times$ p  Access time is proportional to the page fault rate  If one access out of 1000 causes a page fault, then EAT = 8.2 ms $\\rightarrow$ slowdown by a factor of 40  Programs tend to have locality of reference Locality means program often accesses memory addresses that are close together  A single page fault can bring in 4KB memory content Greatly reduce the occurrence of page fault  Major components of page fault time (about 8ms)  serve the page-fault interrupt read in the page from disk (most expensive) Restart the process   Process Creation and Virtual memory  Demand paging: only bring in the page containing the first instruction Copy-on-Write: the parent and the child process share the same frames initially, and frame-copy when a page is written Memory-Mapped File: map a file into the virtual address space to bypass file system calls (e.g. read(), write())  Copy-on-Write  if either process modifies a frame, only then a frame is copied COW allows efficient process creation Free frames are allocated from a pool of zeroed-out frames, the content of a frame is erased to 0 Figure: a child process is forked  After parent modifies page C   Memory-Mapped Files  Approach:  MMF allows file I/O to be treated as routine memory access by mapping a disk block to memory frame A file is initially read using demand paging. Subsequent read/writes to/from the file are treated as ordinary memory accesses  Benefit:  Faster file access by using memory access rather than read() and write() system calls Allows several process to map the SAME file allowing the pages in memory to be SHARED  Concerns:  Security(access control), data lost, more programming efforts    Page replacement  when a page fault occurs with on free frame  swap out a process, freeing all its frames page replacement, find one not currently used add free it. Use dirty bit to reduce overhead of page transfers \u0026ndash; only modified pages are written to disk  Solve two major problems for demand paging  Frame-allocation algorithm, determine how many frames to be allocated to a process Page-replacement algorithm, select which frame to be replaced   Page Replacement Algorithms  Goal: lowest page-fault rate Evaluation: running against a string of memory references (reference string) and computing the number of page faults Reference String: 1,2,3,4,1,2,5,1,2,3,4,5  FIFO algorithm  The oldest page in a FIFO queue is replaced 3 frames (available memory frames = 3) 9 page faults  FIFO illustrating Belady\u0026rsquo;s Anomaly More allocated frames doesn\u0026rsquo;t guaranteed less page fault  Figure illustration   Optimal (Belady) Algorithm  Replace the page that will not be used for the longest period of time, need future knowledge 4 frames (6 page faults) In practice, we don\u0026rsquo;t have future knowledge, only used for reference and comparison   LRU Algorithm (Least Recently Used)  An approximation of optimal algorithm, looking backward, rather than forward It replaces the page that has not been used for the longest period of time It is often used, and is considered as quite good Counter implementation  page referenced: time stamp is copied into the counter replacement: remove the one with oldest counter, but linear search is required  Stack implementation  page referenced: move to top of the double-linked list replacement: remove the page at the bottom    Stack Algorithm  A property of algorithms Stack algorithm: the set of pagers in memory for n frames is always a subset of the set of pages that would be in memory with n+1 frames Stack algorithms do not suffers from Belady\u0026rsquo;s anomaly Both optimal algorithm and LRU algorithm stack algorithm  LRU approximation algorithms Few systems provide sufficient hardware support for the LRU page-replacement\n additional-reference-bits algorithm second-chance algorithm enhanced second-chance algorithm  Counting Algorithms  LFU algorithms (least frequently used)  keep a counter for each page idea: an actively used page should have a large reference count  MFU algorithms (most frequently used)  idea: the page with the smallest count was probably just brought in and has yet to be used   Both counting algorithms are not common\n implementation is expensive (overflow) do not approximate OPT algorithm very well  Allocation of frames  each process needs minimum number of frames Fixed allocation  Equal allocation \u0026ndash; 100 frames, 5 processes $\\rightarrow$ 20 frames/process Proportional allocation \u0026ndash; Allocate according to the size of the process  Priority allocation Using proportional allocation based on priority, instead of size. Local allocation: each process select from its own set of allocated frames Global allocation : process selects a replacement frame from the set of all frames  One process can take away a frame of another process e.g. Allow a high-priority process to take frames from a low-priority process Good system performance and thus is common used A minimum number of frames must be maintained for each process to prevent trashing   Trashing  If a process dost not have \u0026ldquo;enough\u0026rdquo; frames to support pages in active page $\\rightarrow$ very high paging activity A process is trashing if it spending more time paging than executing  Performance problem caused by trashing Processes queued for I/O to swap (page fault) $\\rightarrow$ low CPU utilization $\\rightarrow$ OS increased the degree of multiprogramming $\\rightarrow$ new processes take frames from old processes $\\rightarrow$ CPU utilization drops even further To prevent trashing, must provide enough frames for each process (Working-set model, page-fault frequency)  Working-Set Model (实现起来比较繁琐， 比较少用)  Locality: a set of pages that are actively used together Locality model: as a process executes, it moves from locality to locality (program structure, data structure) Working-set model  working-set window: a parameter $\\Delta$ working set: set of pages in most recent $\\Delta$ page reference (an approximation locality)   Explanation  $WSS_i:$ Working-set size for process i $D = \\sum WSS_i:$ Total demand frames If $D \u0026gt; m$ (available frames) $\\rightarrow$ trashing The OS monitors the $WSS_i$ of each process and allocates to the process enough frames: if $D \u0026lt;\u0026lt; m$ , increase degree of MP, if $D \u0026gt; m$, suspend a process.  Advantages: prevent thrashing while keeping the degree of multiprogramming as high as possible , optimize CPU utilization  Page Fault frequency scheme  page fault frequency directly measures and controls the page-fault rate to prevent trashing Establish upper and lower bounds on the desired page-fault rate of a process If page fault rate exceed the upper limit, allocate another frame to the process if page fault rate fails below the lower limit, remove a frame from the process   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"395bf14557c5cb3c46371bf6f570e3cd","permalink":"/courses/operating_system/virtual_memory/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/virtual_memory/","section":"courses","summary":"Background  We don\u0026rsquo;t want to run a program that is entirely in memory  Many code for handling unusual errors or conditions Certain program routines or features are rarely used The same library code used by many programs Arrays, lists and tables allocated but not used  Virtual memory — separation of user logical memory from physical memory  To run a extremely large process, logical address space can be much larger than the physical address space To increase CPU/resources utilization (higher degree of multiprogramming degree) To simplify programming tasks (Free programmer from memory limitation) To run programs faster (less I/O would be needed to load or swap)  Virtual memory can be implemented by  Demand paging Demand segmentation: more complicated due to variable size    Demand Paging  A program rather than the whole process is brought into memory only when it is needed  Less I/O needed $\\rightarrow$ Faster response Less memory needed $\\rightarrow$ More users  Page is needed when there is a reference to the page  Invalid reference $\\rightarrow$ abort Not-in-memory $\\rightarrow$ bring to memory via paging  Pure demand paging  Start a process with no page Never bring a page into memory until it is required  A swapper (midterm scheduler) manipulates the entire process, whereas a pager is concerned with individual page of a process Hardware support  Page table : a valid-invalid bit (1 $\\rightarrow$ page in memory, $0 \\rightarrow$ page no in the memory) Secondary memory (swap space, backing store), usually, a high-speed disk (swap device) is use    Page Fault First reference to a page will trap to OS (page fault trap)","tags":null,"title":"Virtual Memory","type":"docs"},{"authors":null,"categories":null,"content":"  understand principles behind data link layer services:  error detection, correction sharing a broadcast channel, multiple access (无线网络) link layer addressing (48 bit) reliable data transfer, flow control  instantiation and implementation of various link layer technologies  Introduction  hosts and routers are nodes communication channels that connect adjacent nodes along communication path are links (wired links, wireless links, LANs) layer-2 packet is a frame, encapsulates datagram data-link layer has a responsibility of transferring datagram from one node to adjacent node over a link datagram transferred by different link protocols over different links each link protocol provides different services (may or may not provide rdt(reliable data service) over link)  Services  Framing, link access  encapsulate datagram into frame, adding header, trailer channel access if shared mediums \u0026ldquo;MAC\u0026rdquo; (media access control protocol) addresses used in frame headers to identify source, destination  reliable delivery between adjacent nodes  TCP seldom used on low bit-error link(fiber, some twisted pair) wireless links high error rates  flow control: pacing between adjacent sending and receiving nodes error detections:  error caused by noise receiver detects presence of errors  error correction: receiver identifies and corrects bit error(s) without resorting to retransmission half-duplex and full-duplex with half duplex, nodes at both ends of link can transmit, but not at same time  Where is link layer implemented  in each and every host link layer implemented in \u0026ldquo;adaptor\u0026rdquo; (network interface card)  ethernet card, PCMCI card, 802.11 card implement link, physical layer  attaches into host\u0026rsquo;s system buses combination of hardware, software, firmware sending side : encapsulates datagram in frame, address error checking bits, rdt, flow control, etc receiving side: look for errors, rdt, flow control etc, extracts datagram, passes to upper layer at receiving side  Error detection and error correction  EDC: Error detection and correction bits (redundancy) D: data protected by error checking, may include header files Error detection not 100% reliable  protocol may miss some errors, but rarely larger EDC field yields better detection and correction   Parity Checking  Single Bit Parity (Detect single bit errors)  Two dimensional Bit Parity (Detect and correct single bit errors)   Internet Checksum  detect \u0026ldquo;errors\u0026rdquo; (e.g. flipped bits) in transmitted packet (used at transport layer only) Sender:  treat segment contents as sequence of 16-bit integers checksum: addition (1\u0026rsquo;s complement sum) of segment contents sender puts checksum value into UDP checksum fields  Receiver:  compute checksum of received checksum check if computed checksum equals checksum field value   Cyclic Redundancy Check  view data bits D as a binary number choose r+1 bit pattern (generator) G goal: choose r CRC bits, R, such that   exactly divisible by G receiver knows G, divides  by G can detect all burst errors less than r+1 bits  widely used in practice (Ethernet, 802.11 WIFI, ATM)   Multiple Access Links and Protocols  Two types of \u0026ldquo;links\u0026rdquo;  Point-to-point: PPP for dial-up access, point-to-point link between Ethernet switch and host  broadcast (shared wire or medium)  old-fashioned Ethernet upstream HFC (hybrid fiber coax) 802.11 wireless LAN   single shared broadcast channel two or more simultaneous transmissions by nodes: collision if node receives two or more signals at the same time multiple access protocol:  distributed algorithm that determines how nodes share channel, i.e. determine when node can transmit communication about channel sharing must use channel itself, no out-of-band channel for coordination   Ideal Multiple Access Protocol Broadcast channel of rate R bps\n when one node wants to transmit, it can send at rate R when M nodes want to transmit, each can send at average rate R?M fully decentralized  no special node to coordination transmissions no synchronization of clocks, slots  simple  MAC protocols: a taxonomy Channel Partitioning: TDMA/ FDMA  TDMA: time division multiple access  access to channel in \u0026ldquo;rounds\u0026rdquo; each station gets fixed length slot(length= packet transmission time) in each round unused slots go idle  FDMA: frequency division multiple access  channel spectrum divided into frequency bands each station assigned fixed frequency band unused transmission time in frequency bands go idle    Random access channel not divided, allow collisions, \u0026ldquo;recover\u0026rdquo; from collisions\n when node has packet to send, transmit at full channel data rate R two or more transmitting nodes $\\rightarrow$ collision random access MAC protocol specifies  how to detect collisions how to recover from collisions (e.g. via delayed retransmission)  examples of random access MAC protocols: slotted ALOHA, CSMA, CSMA/CD (ethernet), CSMA/CA  Slotted ALOHA - all frames same size - time divided into equal size slots (time to )\n Operation: when node obtains fresh frame, transmits in next slot  if no collision:\n if collision: node retransmits frame in each subsequent slot with pro  Cons:  collisions, wasting slots idle slots nodes may be able to detect collision in less than time to transmit pocket clock synchronization  Pros  single highly decentralized simple  Efficiency: long-run fraction of successful slots (many nodes, all with many frames to send)  suppose: N nodes with many frames to send, each transmits in slot with probability p prob that given node has success in a slot = $p(1-p)^{N-1}$ prob than any node has a success = $Np(1-p)^{N-1}$ for many nodes, max efficiency $p = 1/e = 0.37$   Pure(unslotted) ALOHA  unslotted Aloha: simpler, no synchronization when frame first arrives: transmit immediately collision probability increases, frame set at $t_0$ collides with other frames send in $[t_0-1, t_0+1]$  prob $p*(1-p)^{2(N-1)}$ for many nodes, max efficiency $p = 1/(2e) = 0.18$  CSMA (Carrier Sense Multiple Access) CSMA: listen before transmit, if channel sensed idle transmit entire frame, if channel sensed busy, defer transmission\n collision: entire packet transmission time wasted role of distance \u0026amp; propagation delay in determining collision probability  CSMA/CD (collision detection)  collision detection:  easy in wried LANS: measure signal strengths, compare transmitted, received signals difficult in wireless LANs; received signal strength overwhelmed by local transmission strength CSMA/CD used in Ethernet CSMA/CA (collision avoidance) used in 802.11   \u0026ldquo;Taking Turns\u0026rdquo; MAC protocols  channel partitioning MAC protocols:  share channel efficiently and fairly at high load inefficient at low load: delay in channel access, 1/N bandwidth allocated even if only 1 active node  Random access MAC protocols:  efficient at low load: single node can fully utilized channel high load: collision overhead  taking turns protocols: look for best of both worlds Polling  master node \u0026ldquo;invites\u0026rdquo; slave nodes to transmit in turn typically used with \u0026ldquo;dumb\u0026rdquo;  Token passing (Taking Turns):  control token passed from one node to next sequentially token message concerns: token overhead, latency, singe point of failure  example: Bluetooth, FDDI, IBM Token Ring  Link-Layer Addressing  32-bit IP address:  network-layer address used to get datagram to destination IP subnet  MAC (or LAN or physical or Ethernet) address:  function: get frame from one interface to another physically-connected interface (same network) 48 bit MAC address (for most LANS) burned in NIC ROM, also sometimes software settable  Each adapter on LAN has unique LAN address (FF-FF-FF-FF-FF-FF: broadcast address) mac address allocation administered by IEEE manufacturer buys portion of MAC address space (to assure uniqueness) MAC flat address $\\rightarrow$ portability, can move LAN card from one LAN to another IP hierarchical address not portable  ARP: address Resolution Protocol  how to determine MAC address of B knowing B\u0026rsquo;s IP address? Each IP node (host, router) on LAN has ARP table ARP table: IP/MAC address mappings for some LAN nodes, TTL(time to live): time after which address mapping will be forgotten (typically 20 minutes)  procedure  A wants to send datagram to B, and B\u0026rsquo;s MAC address not in A\u0026rsquo;s ARP table A broadcast ARP query packet, containing B\u0026rsquo;s IP address, all machines on LAN receive ARP query B receivers ARP packet, replies to A with its (B\u0026rsquo;s) MAC address (frames send to A\u0026rsquo;s MAC address(unicast)) A caches (saves) IP-to-MAC address pair in its ARP table until information becomes old (times out) ARP is \u0026ldquo;plug-and-play\u0026rdquo;, node creates their ARP tables without intervention form net administrator  example:   Ethernet  features:  cheap first widely used LAN technology simpler, cheaper than token LANs and ATM kept up with speed race: 10Mbps - 10 Gbps  bus topology popular through mid 90-s (all nodes in same collision domain, can collide with each other) today: star topology prevails  active, switch in center each \u0026ldquo;spoke\u0026rdquo; run a (separate) Ethernet protocol (nodes do not collide with each other)  Ethernet Frame structure  Preamble: 7 bytes with pattern 10101010 followed by one byte with pattern 10101011 used to synchronize receiver, sender clock rates Addresses: 6 bytes, if adapter receives frame with matching destination address, or with broadcast address (e.g. ARP packet), it passes data in frame to network layer protocol, otherwise, adapter discards frame Type: indicates higher layer protocol (mostly IP but others possible, e.g. Novell IPX, Apple Talk) CRC: checked at receiver, if error is detected, frame is dropped  Unreliable, connectionless  connectionless: No handshaking between sending and receiving NICs unreliable： receiving NIC(network interface card) doesn\u0026rsquo;t send ACKs or NAKs to sending NIC stream of datagrams passed to network layer can have gaps(missing datagrams) gaps will be filled if app is using TCP, otherwise app will see gaps Ethernet\u0026rsquo;s MAC protocol: unslotted CSMA/CD   Ethernet CSMA/CD algorithm  NIC receives datagram from network layer, creates frame IF NIC senses channel idle, starts frame transmission, IF NIC senses channel busy, waits until channel idle, then transmits If NIC transmits entire frame without detecting another transmission, NIC is done with frame If NIC detects another transmission while transmitting, aborts and sends jam signal After aborting, NIC enters exponential backoff: after mth collision, NIC chooses K at random from $(0, 1,2, \u0026hellip;, 2^m-1)$ NIC waits K*512 bit times, returns to Step2   Jam signal: make sure all other transmitters are aware of collision: 48 bits Bit time: 1 microsecond for 10 Mbps Ethernet, for K=1023,wait time is about 50 msec Exponential Backoff (最多连续16次)  Goal: adapt retransmission attempts to estimated current load (heavy load: random wait will be longer) first collision: choose K from (0,1), delay is K*512 bit transmission times after second collision: choose K from {0,1,2,3} after ten collisions: choose K from {0,1,2,3,4,\u0026hellip;,1023}  Efficiency (70%)  $T_{prop}$ = max prop delay between 2 nodes in LAN $T{trans}$ = time to transmit max-size frame $$\\text{efficiency} = \\frac{1}{1+5t{prop}/t_{trans}}$$ Efficiency goes to 1 as $t{prop}$ goes to 0, as t{trans}$ goes to infinity better performance than ALOHA: and simple, cheap, decentralized   802.3 Ethernet Standards: Link \u0026amp; Physical Layers  many different Ethernet standards  common MAC protocol and frame format different speeds: 2Mbps, 10Mbps, 100Mbps, 1Gbps, 10Gbps different physical layer media: fiber, cable   Manchester encoding  used in 10Base T each bit has a transition allows clocks in sending and receiving nodes to synchronize to each other, no need for a centralized, global clock among nodes   Link-Layer switches  Hubs  physical-layer (\u0026ldquo;dumb\u0026rdquo; repeaters, 只处理信号) bits coming in one link go out all other links at same rate all nodes connected to hub can collide with one another no frame buffering no CSMA/CD at hub: host NIC detect collisions  Switch: allow multiple simultaneous transmissions  link-layer device: smarter than hubs, take active role store, forward Ethernet frames examine incoming frame\u0026rsquo;s MAC address, selectively forward frame to one-or-more outgoing links when frame is to be forwarded on segment, uses CSMA/CD to access segment transparent, hosts are unaware of presence of switches plug-and-play, self-learning, switch do not need to be configured switch table: Each switch has a switch table, each entry (MAC address of host, interface to reach host, time stamp   Self-Learning  Switch table is initially empty For each incoming frame received on an interface, the switches stores in its table  The MAC address in the frame\u0026rsquo;s source address field the interface from which the frame arrived the current time  the switch deletes an address in the table if no frames are received with that address as the source address after some period of time  Filtering/forwarding  Filtering: whether a frame should be forwarded to some interface or should just be dropped Forwarding: determine the interfaces to which a frame should be directed, and then moves the frame to those interfaces suppose a frame with destination address DD-DD-DD-DD-DD-DD arrives at the switch on interface x  there is no entry in the table for DD-DD-DD-DD-DD-DD, the switch forwards copies of the frame to the output buffers preceding all interfaces (broadcast the frame) there is an entry in the table, associating DD-DD-DD-DD-DD-DD with interface x: there is no need to forward the frame to any of the other interfaces, switch performs filtering function by discarding the frame. (送和收在同一区域） there is an entry in table, associating DD-DD-DD-DD-DD-DD with interface $y \\neq x$, the frame needs to be forwarded to the LAN segment attached to interface y. Switch performs its forwarding function by putting the frame in an output buffer that precedes interface y.   Switches vs. Routers  both store-and-forward devices: routers are network layer devices, switches are link layer address Routers maintain routing tables, implement routing algorithms switches maintain switch tables, implement filtering, learning algorithms  PPP [RFC 1557] Point to Point Data Link Control\n one sender, one receiver, one link: easier than broadcast link no media Access control no need for explicit MAC addressing (e.g. dialup link) popular point-to-point DLC protocols: PPP, HDLC  PPP Frame  packet framing: encapsulation of network-layer datagram in data link frame  carry network layer data of any network layer protocol (not just IP) at same time ability to demultiplex upwards  bit transparency: must carry any bit pattern in the data field error detection (no correction) connection liveness: detect, signal link failure to network layer network layer address negotiation: endpoint can learn/configure each other\u0026rsquo;s network address data frame  Flag: delimiter (framing) Address: does nothing control: does nothing: protocol: upper layer protocol to which frame delivered (e.g. PPP-LCP, IP, IPCP etc) info: information check: cyclic redundancy check for error-detection  PPP non-requirements  no error correction no flow control out of order delivery no need to support multipoint links error recovery, flow control, data re-ordering all relegated to higher layers  Byte Stuffing  \u0026ldquo;data transparency\u0026rdquo; requirement: data field must be allowed to include flag pattern  Sender: adds extra  byte after each  data byte Receiver: two 01111110 bytes in a row: discard first byte, continue data reception; singe 01111110:flag byte  PPP data Control Protocol  before exchanging network-layer data, data link peers must configure PPP link (max frame length, authentication) learn / configure network layer information   ATM  Virtualization of networks  Layering of abstractions: don\u0026rsquo;t sweat the details of the lower layer, only deal with lower layers abstractly two layers of addressing: internetwork and local network new layer (IP) makes everything homogeneous at internetwork layer  Asynchronous Transfer Mode: ATM  Goal: integrated, end-end transport of carry voice, video, data meeting timing / QoS requirements of voice, video (versus Internet best-effort model) \u0026ldquo;next generation\u0026rdquo; telephony (下一代网络电话） packet-switching (fixed length packets, 53 bytes, called \u0026ldquo;cells\u0026rdquo;) using virtual circuits  For more information, see Computer Networking Website  Multi-protocol label switching (MPLS)  Goal: speed up IP forwarding by using fixed length label (instead of IP address) to do forwarding (20bits)  Borrowing ideas from Virtual Circuit (VC) approach but IP datagram still keeps IP address   MPLS-enhanced forwarding   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"5ff4a819c8f52a6f491cda30db245d14","permalink":"/courses/computer_network/link_layer/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/computer_network/link_layer/","section":"courses","summary":"understand principles behind data link layer services:  error detection, correction sharing a broadcast channel, multiple access (无线网络) link layer addressing (48 bit) reliable data transfer, flow control  instantiation and implementation of various link layer technologies  Introduction  hosts and routers are nodes communication channels that connect adjacent nodes along communication path are links (wired links, wireless links, LANs) layer-2 packet is a frame, encapsulates datagram data-link layer has a responsibility of transferring datagram from one node to adjacent node over a link datagram transferred by different link protocols over different links each link protocol provides different services (may or may not provide rdt(reliable data service) over link)  Services  Framing, link access  encapsulate datagram into frame, adding header, trailer channel access if shared mediums \u0026ldquo;MAC\u0026rdquo; (media access control protocol) addresses used in frame headers to identify source, destination  reliable delivery between adjacent nodes  TCP seldom used on low bit-error link(fiber, some twisted pair) wireless links high error rates  flow control: pacing between adjacent sending and receiving nodes error detections:  error caused by noise receiver detects presence of errors  error correction: receiver identifies and corrects bit error(s) without resorting to retransmission half-duplex and full-duplex with half duplex, nodes at both ends of link can transmit, but not at same time  Where is link layer implemented  in each and every host link layer implemented in \u0026ldquo;adaptor\u0026rdquo; (network interface card)  ethernet card, PCMCI card, 802.","tags":null,"title":"Link Layer","type":"docs"},{"authors":null,"categories":null,"content":" Threads  Threads is a light process, basic unit of CPU utilization All threads belonging to the same process share code section, data section, and OS resources (e.g. open files and signals) Each thread has its own (thread control block) thread ID, program counter, register set, and a stack  Example  web browser, once thread displays contents while the other thread receives data from network web server, one request(thread), better performance as code and resource sharing RPC server   Benefits of Multithreading  Responsiveness: allow a program to continue running even if part of it is blocked or is performing a lengthy operation Resource sharing: several different threads of activity all within the same address space Utilization of MP arch: Several thread may be running in parallel on different processors Economy: Allocating memory and resources for process creation is costly.   Multicore Programming  multithread programming provides a mechanism for more efficient use of multiple cores and improved concurrency (threads can run in parallel) Multicore systems putting pressure on system designers and application program (scheduling algorithms use cores to allow the parallel execution challenges in Multicore Programming  Dividing activities: divide program into concurrent tasks Data splitting: divide data accessed and manipulated by the tasks Data dependency: synchronize data access Balance: evenly distribute tasks to cores Testing and debugging   User vs. Kernel Threads  User thread \u0026ndash; thread management done by user level threads library (Pthreads, Java threads, Win32 threads) Kernel threads \u0026ndash; supported by the kernel(OS) directly (Windows 2000, Linux) User threads  thread library provides support for thread creation, scheduling and deletion Generally fast to create and manage If the kernel is single-threaded, a user-thread blocks $\\rightarrow$ entire process blocks even if other threads are ready to run  Kernel threads  The kernel performs thread creation, scheduling, etc. Generally slower to create and manage If a thread is blocked, the kernel can schedule another thread for execution  Models (user threads to kernel threads) Many-to-one, one-to-one(kernel threads 有限制，大部分系统）, Many-to-Many  Shared-Memory Programming  Definition: processes communicate or work together with each other through a shared memory space which can be accessed by all processes (Faster \u0026amp; more efficient than message passing) Many issues as well, Synchronization, Deadlock, Cache coherence Programming techniques (Parallelizing compiler, Unix processes, Threads(Pthread, Java))  Pthread  Pthread is the implementation of POSIX standard for thread pthread_create(thread, attr, routine, arg)  thread: An unique identifier (token) for the new thread attr: it is used to set thread attributes. NULL for the default values routine: The routine that the thread will execute once it is created arg: A single argument that may be passed to routine   pthread_join(threadId, status)  Blocks until the specified threadId thread terminates One way to accomplish synchronization between threads  pthread_detach(threadId)  Once a thread is detached, it can never be joined Detach a thread could free some system resources   Example\n#include \u0026lt;pthread.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #define NUM_THREADS 5 void *PrintHello(void *threadId) { long* data = static_cast \u0026lt;long*\u0026gt; threadId; printf(\u0026quot;Hello World! It's me, thread #%ld!\\n\u0026quot;, *data); pthread_exit(NULL); } int main (int argc, char *argv[]) { pthread_t threads[NUM_THREADS]; for(long tid=0; tid\u0026lt;NUM_THREADS; tid++){ pthread_create(\u0026amp;threads[tid], NULL, PrintHello, (void *)\u0026amp;tid); } /* Last thing that main() should do */ thread_exit(NULL); }  Java Threads  Thread is created by Extending Thread class, Implementing the Runnable interface Java threads are implemented using a thread library on the host System Thread mapping depends on the implementation of JVM  Linux threads  Linux does not support multithreading Various Pthreads implementation are available for user-level The fork system call \u0026ndash; create a new process and a copy of the associated data of the parent process The clone system call \u0026ndash; create a new process and a link that points to the associated data of the parent process A set of flags is used in the clone call for indication of the level of the sharing  None of the flag is set $\\rightarrow$ clone = fork All flags are set $\\rightarrow$ parent and child share everything    Threading Issues  Does fork() duplicate only the calling thread or all threads? Some UNIX system support two versions of fork()  execlp() works the same, replace the entire process, if exec() is called immediately after forking, then duplicating all threads is unnecessary   Thread Cancellation  Asynchronous cancellation (one thread terminates the target thread immediately) 等 main thread 有空 Deferred cancellation (default option) The target thread periodically checks whether it should be terminated, allowing it an opportunity to terminate itself in an orderly fashion (canceled safety)  Signal Handling  Signals (synchronous or asynchronous) are used in UNIX systems to notify a process that an event has occurred A signal handler is used to process signals Signal is generated by particular event Signal is delivered to a process Signal is handled  Thread Pools  Create a number of threads in a pool where they await work Advantages Usually slightly faster to service a request with an existing thread than create a new thread Allows the number of threads in the application(s) to be bound the size of the pool # Of threads: # of CPUs, expected # of requests, amount of physical memory   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"b5462720240c3a6f0cdbb53dd706387e","permalink":"/courses/operating_system/multithread_programming/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/multithread_programming/","section":"courses","summary":"Threads  Threads is a light process, basic unit of CPU utilization All threads belonging to the same process share code section, data section, and OS resources (e.g. open files and signals) Each thread has its own (thread control block) thread ID, program counter, register set, and a stack  Example  web browser, once thread displays contents while the other thread receives data from network web server, one request(thread), better performance as code and resource sharing RPC server   Benefits of Multithreading  Responsiveness: allow a program to continue running even if part of it is blocked or is performing a lengthy operation Resource sharing: several different threads of activity all within the same address space Utilization of MP arch: Several thread may be running in parallel on different processors Economy: Allocating memory and resources for process creation is costly.","tags":null,"title":"Multithread Programming","type":"docs"},{"authors":null,"categories":null,"content":" Introduction CPU-I/O burst cycle: Process execution consists of a cycle of CPU execution and I/O wait\n Generally, there is a large number of short CPU bursts, and a small number of long CPU bursts A I/O bound program would typically has many very short CPU bursts A CPU-bound program might have a few long CPU bursts CPU scheduler: Select from ready queue to execute (i.e. allocates a CPU for the selected process)  Preemptive vs. Non-preemptive  CPU scheduling decisions may take palce when a process:  Switches from running to waitting state (IO) Switches from running to ready state (Time-sharing) Swtiches from waiting to ready state Terminates  Non-preemptive scheduling (不会打断)  Scheduling under 1 and 4 (no choice in terms of scheduling) The process keeps the CPU until it is terminated or switched to the waitting state  Preemptive scheduling, Scheduling under all cases 使用率很高 Preemptive Issues  Inconsistent state of shared data, require process synchronization, incurs a cost assocated with access to shared data Affect the design of OS kernel Unix solution: waiting either for a system call to complete or for an I/O block to take palce before doing a context switchd (disable interrupt)   Dispatcher Dispatcher module gives control of the CPU to the process selected by scheduler\n switching context jumping to the proper location in the selected program Dispatch latency \u0026ndash; time it takes for the dispatcher to stop one process and start another running  Scheduling Algorithms Scheduling Criteria  CPU utilization theoretically: $0\\% ~ 100\\%$, real systems : $40\\% ~ 90\\%$ Throughput: number of completed processes per time unit Turnaround time (submission ~ completion) Waiting time (total waiting time in the ready queue) Response time (submission ~ the first response is produced (第一个CPU burst(执行)的时间))  Algorithms First-Come, First-served (FCFS) scheduling  Process (Burst Time) in arriving order: P1(24), P2(3), P3(3) Gantt chart  Waiting time P1 = 0, P2 =24, P3 =27 Average waiting time (AWT) (0+24+27)/3 = 17 Convoy effect: short process behind a long process  Shortest-Job-First (SJF) scheduling  Associate with each process the length of its next CPU burst A process with shortest burst length gets the CPU first SJF provides the minimum average waiting time (optimal) Two schemes  Non-preemptive \u0026ndash; once CPU given to a process, it cannot be preempted until its completion Preemptive \u0026ndash; if a new process arrives with shorter burst length, preemption happens  Non-preemptive SJF example\n     Process Arrival Time Burst Time     P1 0 7   P2 2 4   P3 4 1   p4 5 4    - AWT = [(7-0-7) + (12 - 2 -3) + (8 -4-1) + (16-5-4)] /4 = 4 - Response Time: p1 =0, p2 = 6, p3=3, p4 = 7 - Preemptive SJF example - AWT = 3 - Response time P1 = 0, P2 = 0, P3 = 0, P4 = 2 SJF difficulty: no way to know length of the next CPU burst\n - Approximate SJF: the next burst can be predicted as an exponentail average of the measured length of previous CPU bursts (set $\\alpha = 1\u0026frasl;2$) $$\\tau_{n+1} = \\alpha t_n + (1-\\alpha) \\tau_n = \\frac12 tn + \\frac14 t{n1} + \\frac18 t{n-2}​$$ Priority Scheduling  A priority number is associated with each process The CPU is allocated to the highest priority process SJF is a priority scheduling where priority is the predicted next CPU burst time Problem: Starvation (low priority process never execute) Solution: aging(as time progresses increase the priority of process)  Round-Robin(RR) Scheduling  Each process gets a small unit of CPU time(time quantum) After TQ elapsed, process is preempted and added to the end of the ready queue TQ large $\\rightarrow$ FIFO TQ small $\\rightarrow$ (context switch) overhead increases  Multilevel Queue Scheduling  Ready queue is partitioned ino separate queues Each queue has its own scheduling algortihm Scheduling must be done between queues  Fixed priority scheduling: prossibility of starvation    Mutillevel Feedback Queue Schedule  A process can move between the various queues; aging can be implemented Idea: separate processes according to the characteristic of their CPU burst  I/O-bound and interactive processes in higher priority queue $\\rightarrow$ short CPU burst CPU-bound processes in lower priority queue long CPU burst   multilevel feedback queue scheduler is defined by the following parameters:  Number of queues Scheduling algorithm for each queue Method used to determin when to upgrade/demote a process   Evaluation methods  Deterministic Modeling — takes a particular predetermined workload and defines the performance of each algorithm for the workload Queueing Model — mathematical analysis Simulation — random-number generator or trace tapes for workload generation Implementation — the only completely accurate way for algorithm evaluation  Mutli-Processor Scheduling \u0026amp; Multi-Core Processor Scheduling \u0026amp; Real-Time Scheduling  Asymmetric multiprocessing  All system activities are handled by a processor (alleviationg the need for data sharing) the other only execute user code (allocated by the master) far simple than SMP  Symmetric multiprocesiing (SMP):  each processor is self-scheduling all processor in common ready queue, or each has its own private queue of ready processes need synchronization mechanism  Processor affinity A process has an affinity for the processor on which it is currently running  A process populates its recent used data in cache memory of its running processor cache invalidation and repopulation has high cost Soft affinity: possible to migrate between processors hard affinity: not to migrate to other processor  NUMA (non-uniform memory access):  Occurs in systems containing combines CPU and memory boards CPU scheduler and memory-replacement works together A process (assigned affinity to a CPU) can be allocated memory on the board where that CPU resides   Load-balancing  Keep the workload evently distributed across all processors, only necessary on systems where each processor has its own private queue of eligible processes to execute Push migration : move(push) processes from overloaded to idle or less-busy processor Pull migration: idle processors pulls a waiting task from a busy processor Often implemented in parallel   Multi-core Processor Scheduling  Faster and consumer less power memory stall: When access memory, it spends a significant amount of time waiting for the data become available. (e.g. cache miss) Multi-threaded multi-core systems  Two (or more) hardware threads are assigned to each core (Intel Hyper-threading) Takes advantage of memory stall to make progress on another thread while memory retrieve happens   Two ways to multithread a processor:  coarse-grained: switch to another thread when a memory stall occurs. The cost is high as the instruction pipeline must be flushed fine-grained(interleaved): switch between threads at the boundary of an instruction cycle. The architecture design includes logic for thread switching \u0026ndash; cost is low  Scheduling for Multi-threaded multi-core systems  1st level: Choose which software thread to run on each hardware thread(logical processor) 2nd level: How each core decides which hardware thread to run   Real-Time CPU Scheduling Real-time does not mean speed, but keeping deadlines\n Soft real-time requirements, missing the deadline is unwanted , but is not immediately critical (multimedia streaming) Hard real-time requirements, Missing the deadline results in a fundamental failure (nuclear power plant controller)  Read-Time Scheduling Algorithms Evaluation: Ready, Execution, Deadline\nRate-Monotoinc (RM) algorithm  short period, higher priority (fixed-priority RTS scheduling algorithm) Ex: T1 = (4,1) (red), T2 = (5,2) (orange), T3 = (20,5)(green) (Period, Execution) priority: T_1 \u0026gt; T_2 \u0026gt; T_3   Earliest-Deadline-First(EDF) algorithm  Earlier deadline, higher priority (dynamic priority algorithm) Ex: T1 = (2, 0.9), T2 = (5, 2.3)   Operating System Examples  Solaris Scheduler Windows XP Scheduler Linux Scheduler  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"e8ee4ac4e5f1480ec84a895ee2223555","permalink":"/courses/operating_system/process_schedule/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/process_schedule/","section":"courses","summary":"Introduction CPU-I/O burst cycle: Process execution consists of a cycle of CPU execution and I/O wait\n Generally, there is a large number of short CPU bursts, and a small number of long CPU bursts A I/O bound program would typically has many very short CPU bursts A CPU-bound program might have a few long CPU bursts CPU scheduler: Select from ready queue to execute (i.","tags":null,"title":"Process Scheduling","type":"docs"},{"authors":null,"categories":null,"content":" Background  Concurrent access to shared data may result in data inconsistency Maintaining data consistency requires mechanism to ensure the orderly execution of cooperating processes Consumer \u0026amp; Producer Problem Race condition: the situation where several processes access and manipulate shared data concurrenlty. The final value of the shared data depends upon which process finishes last, commonly described as critical section problem To prevent race condition, concurrent processes must be synchronized, on a single-process machine, we could disable interrupt or use non-preemptive CPU scheduling  Crtical Section  Purpose: a protocal for processes to cooperate Probelm description:  N process are competing use some shared data Each process has a code segment, called critical selection, in which the shared data is accessed Ensure that when one process is executing in its critical section, no other process is allowed to execute in its critical selection $\\rightarrow$ mutually exclusive  Critical Section Requirements  Mutual Exclusion: if process P in executing in its CS, no other processes can be executing in their CS Progress: if no process is executing in its CS and there exist some processes that wish to enter their CS, these processes cannot be postponed indefinitely Bounded Waiting: A bound must exist on the number of times that other processes are allowed to enter their CS after a process has made a request to enter its CS   Solution Algorithm for two Processes  only 2 processes, $P_0$ and $P_1$ Shared variables  int turn; // initially turn = 0 turn = i $\\rightarrow$ $P_i$ can enter its critical section  Mutual exclustion (yes); Progress (No); Bounded-Wait(Yes)   Peterson\u0026rsquo;s solution for Two processes  shared variables  int turn, initially turn =0 turn = i $\\rightarrow$ $P_i$ can enter its critical section boolean flag[2] // initially flag[0] = flag[1] = false flag[I] = true $\\rightarrow$ $P_i$ ready to enter its critical section Mutual Selection, progress, bounded waiting proof  Example\ndo { flag[i] = TRUE; // Pi 是否想要进去 turn = j; // 先把key交给对方 while (flag[j] \u0026amp;\u0026amp; turn == j); // critical section flag[i] = FALSE; remainder section } while(1);   Bakery Algorithm (n processes)  Before enter its CS, each process receives a number Holder of the smallest number enters CS The numbering scheme always generates number in non-decreasing order; i.e. 1,2,3,3,4,5,5,5 if processes $P_i$ and $P_j$ receive the same numbe, if $i\u0026lt;j$ then $P_i$ is served first Bounded-waiting because processes enter CS on a First-come, First Served basis\n// process i: do { choosing[i] = TRUE; num[i] = max(num[0], num[1], ..., num[n-1]) + 1; choosing[i] = FALSE; for(j =0; j\u0026lt;n; j++){ while(choosing[j]); // cannot compare when num is being modified while((num[j]!=0) \u0026amp;\u0026amp; ((num[j], j)) \u0026lt; (num[i],i))); // FCFS } // critical section num[i] = 0; // reminder section }while(1);   Pthread Lock/Mutex Routines  To use mutex, it must be declared as type pthread_mutex_t and initialized with pthread()_mutex_init() A mutex is destoryed with pthread_mutex_destory() A critical selection can then be protected using pthread_mutex_lock() and pthread_mutex_unlock()  Condition Variables (CV)  CV represent some condition that a thread can:  Wait on, until the condition occurs; or Notify other waiting threads that the condition has occured  Three operations on condition variables:  wait() — Block until another thread calls signal() or broadcast() on the CV signal() — Wake up one thread waiting on the CV broadcast() — Wake up all threads waiting on the CV  All condition variable operation must be performed while a mutex is locked In Pthread, CV type is pthread_cond_t  Use pthread_cond_init() to initalize pthread_cond_wait(\u0026amp;theCV, \u0026amp;somelock) pthread_cond_signal(\u0026amp;theCV) pthread_cond_broadcast(\u0026amp;theCV)  Example:  A thread is designed to take action when x = 0 Another thread is responsible for decrementing the counter All condition variable operation must be performed while a mutex is locked   Procedure  left thread Lock mutex Wait()  Put the thread into sleep and releases the lock Waked up, but the thread is locked Re-acquire lock and resume execution  Release the lock right thread  Lock mutex Signal() Release the lock()    ThreadPool Implementation struct threadpool_t { pthread_mutex_t lock; pthread_cond_t notify; pthread_t *treahd; threadpool_task_t *queue; int thread_count; int queue_size; int head; int tail; int count; int shutdown; int started; }; typedef struct { void (*function) (void*); void *argument; } threadpool_task_t; // allocate thread and task queue pool-\u0026gt;threads = (pthread_t *) malloc(sizeof(pthread_t) * thread_count); pool-\u0026gt;queue = (threadpool_task_t *) malloc(sizeof(threadpool_task_t) * queue_size); // threadpool implementation static void *threadpool_thread(void *threadpool) { threadpool_t *pool = (threadpool_t *) threadpool; threadpool_task_t task; for(;;){ // lock must be taken to wait on conditional varaibl pthread_mutex_lock(\u0026amp;(pool-\u0026gt;lock)); while((pool-\u0026gt;count=0) \u0026amp;\u0026amp; (!pool-\u0026gt;shutdown)) { pthread_cond_wait(\u0026amp;(pool-\u0026gt;notify), \u0026amp;(pool-\u0026gt;lock)); task.function = pool-\u0026gt;queue[pool-\u0026gt;head].function; task.argument = poll-\u0026gt;queue[pool-\u0026gt;head].argument; pool-\u0026gt;head +=1; pool-\u0026gt;head = (pool-\u0026gt;head == poll-\u0026gt;queue_size) ? 0 : pool-\u0026gt;head; pool-\u0026gt;count -=1; pthread_mutex_unlock(\u0026amp;(pool-\u0026gt;lock)); (*(task.function))(task.argument); } } }  Hardware Support  The CS problem occurs because the modification of a shared variable may be interrupted If disable interrupts when in CS, not feasible in multiprocessor machine, clock interrupts cannot fire in any machine HW support solution: atomic instruction  atomic: as one uninterruptible unit Examples: TestAndSet(var), Swap(a, b)   Atomic TestAndSet() booelean TestAndSet(bool \u0026amp;lock) { bool value = lock; lock = TRUE; // return the value of \u0026quot;lock\u0026quot; and set \u0026quot;lock\u0026quot; to ture return value; }  Mutual Exclusion (Yes), Progress(Yes), Bounded-Wait(No)\nAtomic Swap()  Idea: enter CS if lock=false Shared data: boolean lock; //initially lock=FALSE   Semaphores  A tool to generalize the synchronization problem (easier to solve, but no guarantee for correctness) A record of how many units of a particular resource are available  if #record = 1 $\\rightarrow$ binary semaphore, mutex lock if #record \u0026gt; 1 $\\rightarrow$ counting semaphore  Accessed only through 2 atomic ops: wait \u0026amp; signal Spinlock implementation (浪费CPU资源)\nwait(S) { while (S\u0026lt;=0); S--; } signal(S){ S++; }  POSIX Semaphore (OS support) Semaphore is part of POSIX standard but it is not belong to Pthread, it can be used with or without thread\n POSIX Semaphore routines\n sem_init(sem_t *sem, int pshared, unsigned int value) sem_wait(sem_t *sem) sem_post(sem_t *sem) sem_getvalue(sem_t *sem, int *valptr) sem_destory(sem_t *sem)  Example\n#include\u0026lt;semaphore.h\u0026gt; sem_t sem; sem_init(\u0026amp;sem); sem_wait(\u0026amp;sem); // critical section sem_post(\u0026amp;sem); sem_destory(\u0026amp;sem);   n-Process Critical Section Problem  shared data: semaphore mutex; // initially mutex = 1 Process Pi\ndo { wait(mutex); // pthread_mutex_lock(\u0026amp;mutex) critical section signal(mutex); // pthread_mutex_unlock(\u0026amp;mutex) remainder section } while(1); Progress? Yes Bounded waiting? Depends on the implementation of `wait()`   Non-busy waiting Implementation  Semaphore is data struct with a queue, may be any queuing strategy\ntypedef struct { int value; // init to 0 struct process *L // queue } semaphore  wait() and signal()\n use system calls: block() and wakeup() must be executed atomically Ensure atomic wait \u0026amp; signal ops? Single-process: disable interrupts Multi-processor: 1. HW support 2. SW solution(Peterson\u0026rsquo;s solution, Bakery algorithm)\nvoid wait(semaphore S) { S.value--; if(S.value \u0026lt; 0){ add this process to S.L sleep(); } } void signal(semaphore S) { S.value++; if(S.value\u0026lt;=0){ remove a process P from S.L; wakeup(P); } }    Semaphore with Critical Section Classical Synchronization Problems  Purpose: Used for testing newly proposed synchronization scheme Bounded-Buffer (Producer-Consumer) Reader-Writer Problem Dining-Philosopher Problem  Bounded-Buffer Problem A poof of n buffers, each capable of holding one item\n Producer:  Grad an empty buffer place an item into the buffer waits if no empty buffer is available  Consumer  grab a buffer and retracts the item place the buffer back to the free poll waits if all buffers are empty   Readers-Writers Problem  A set of shared data objects A group of processes (reader processes, writer processes, a writer process has exclusive access to a shared object) Different variations involving priority  first RW problem: no reader will be kept waiting unless a writer is updating a shared object (writer会starvation) second RW problem: once a writer is ready, it performs the updates as soon as the shared object is released (writer has higher priority than reader; once a writer is ready, no new reader may start reading)  First Reader-Writer Algorithm\n// mutual exclustion for write semaphore wrt = 1; semaphore mutex = 1; int readcount = 0; Writer() { while(TRUE) { wait(wrt); // Writer Code signal(wrt); } } Reader() { while(TRUE) { wait(mutex); readcount++; if(readcount==1) // 之后的 Reader 不需要拿lock wait(wrt); signal(mutex); // Reader Code wait(mutex); readcount--; if(readcount == 0) signal(wrt); signal(mutex); } }   Dining-Philosopher Problem  5 person sitting on 5 chairs with 5 chopsticks A person is either thinking or eating  thinking: no interaction with the rest 4 persons eating: need 2 chopsticks at hand a person picks up 1 chopsticks at a time done eating: put down both chopsticks  deadlock problem  one chopstick as one semaphore  starvation problem  Monitors (A high-level language construct) Motivation Although semaphores provide a convenient and effective synchronization mechanism, its correctness is depending on the programmer\n All processes access a shared data object must execute wait() and signal() in the right order and right place This may not be true because honest programming error or uncooperative programmer The representation of a monitor type consists of declarations of variables whose values define the state of an instance of the type and the functions(procedures) that implement operations on the type The monitor type is similar to a class in O.O. language  A procedure within a monitor can access only local variables and the formal parameters The local variables of a monitor can be used only by the local procedures  The monitor ensures that only one process at a time can be active within the monitor  Similar idea is incorporated to many programming language (concurrent pascal, C#, and Java)   Monitor introduction High-level synchronization construct that allows the safe sharing of an abstract data type among concurrent processes  Monitor Condition Variables  To allow a process to wait within the monitor, a condition variable must be declared as condtion x, y; Condition variable can only be used with the operations wait() and signal() wait() means that the process invoking this operation is suspended until another process invokes signal() resumes exactly one suspended process. If no process is suspended, the signal operation has no effect (in contrast, signal always change the state of semaphore)    Dining-Philosophers Example monitor dp { enum {thinking, hungry, eating} state[5]; //current state condition self[5]; // delay eating if can't obtain chopsticks void pickup(int i) // pickup chopsticks void putdown (int i) // putdown chopsticks void test (int i) // try to eat void init(){ for (int i =0; i\u0026lt;5; i++) state[i] = thinking; } } void pickup(int i) { state[i] = hungry; test(i); if (state[i] != eating) self[i].wait(); // wait to eat } void putdown(int i) { state[i] = thinking; // check if neighbors are waiting to eat test((i+4) % 5); test((i+1) % 5); } void test(int i) { if ((state[(i+4) % 5] != eating) \u0026amp;\u0026amp; (state[(i+1) % 5] != eating) \u0026amp;\u0026amp; (state[i] == hungry)) { state[i] = eating; self[i].signal(); // if Pi is suspended, resume it, if Pi is not suspended, no effect } }  Synchronized Tools in JAVA  Synchronized Methods(Monitor)\n Synchronized method uses the method receiver as a lock Two invocations of synchronized methods cannot interleave on the same object When one thread is executing a synchronized method for a object, all other threads that invoke synchronized methods the same object block until the first thread exist the object\npublic class SynchronizedCounter { private int c= 0; public synchronized void increment() {c++;} public synchronized void decrement() {c--;} public synchronized int value() {return c;} }   Synchronized Statement (Mutex Lock)\n Synchronized blocks uses the expression as a lock A synchronized statement can be only be executed once the thread has obtained a lock for the object or the class that has been referred to in the statement useful for improving concurrency with fine-grained\npublic void run(){ synchronized(p1) { int i = 10; // statement without locking requirement p1.display(s1); } }    Atomic Transactions  Transactions: a collection of instructions (or instructions) that performs a single logic function Atomic Transactions: Operations happen as a single logical unit of work, in its entirely, or not at all Atomic translation is particular a concern for database system (Strong interest to use DB techniques in OS)  File I/O example  Transaction is a series of read and write operations Terminated by commit (transaction successful) or abort (transaction failed) operation Aborted transaction must be rolled back to undo any changes it performed (it is part of )  Log-Based Recovery  Record to stable storage information about all modifications by a transaction Write-ahead logging: Each log record describes single transaction write operation  Transaction time Data item name Old \u0026amp; new values Special Events: \u0026lt;$T_i$, start\u0026gt;, \u0026lt;$T_i$, commmits\u0026gt;  Log is used to reconstruct the state of the data items modified by the transactions Checkpoints  when faiulre occurs, must consult the log to determine which transactions must be re-done, searching process is time consuming and redone may not be necessary for all transactions use checkpoints to reduce the above overhead, output all log records, modified data, log record  to stable storage   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"d59e68c245240c3f235ae079ef5326ed","permalink":"/courses/operating_system/process_synchronization/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/process_synchronization/","section":"courses","summary":"Background  Concurrent access to shared data may result in data inconsistency Maintaining data consistency requires mechanism to ensure the orderly execution of cooperating processes Consumer \u0026amp; Producer Problem Race condition: the situation where several processes access and manipulate shared data concurrenlty.","tags":null,"title":"Process Synchronization","type":"docs"},{"authors":null,"categories":null,"content":" Introduction  A set of blocked process each holding some resources and waiting to acquire a resource held by another process in the set Ex1: 2 processes and 2 tape dirves, each process holds a tape drive, each process requests another tape drive Ex2: 2 processes, and semaphores A \u0026amp; B, P1(hold B, wait A): wait(A), signal(B), P2(hold A, wait B): wait(B), signal(A)  Necessary Conditions  Mutual exclustion (only 1 process at a time can use a resource) Hold \u0026amp; Wait: a process holding some resources and is waiting for another resource No preemption: a resource can be only released by a process voluntarily Circular wait: there exist a set {$P_0, P_1, \u0026hellip;, P_n$} of waiting process such that $P_0 \\rightarrow P_1 \\rightarrow P_2, \u0026hellip; , P_0$ All four conditions must hold for possible deadlock   System Model  Resources type $R_1, R_2, \u0026hellip; , R_m$ E.g. CPU, memory pages, I/O devices Each resource type $R_i$ has $W_i$ instances, E.g. a computer has 2 CPUs Each process utilizes a resouce as follows: Request $\\rightarrow$ use $\\rightarrow$ release  Resource-Alocation Graph  3 processes, P1-P3 4 resources, R1-R4 (the black dot represent the number of instance) Request edges: P1$\\rightarrow$ R1: P1 requests R1 Assignment edges: R2$\\rightarrow$ P1: one instance of R2 is allocated to P1 P1 is hold on an instance of R2 and waiting for an instance of R1  if the graph consists a cycle, a deadlock may exist deadlock  no deadlock  If graph contains no cycle $\\rightarrow$ no deadlock If graph contains a cype:  if one instance per resource type $\\rightarrow$ deadlock if multiple instances per resource type $\\rightarrow$ possibility of deadlock   Handing Deadlocks  Ensure the system will never a deadlock state  deadlock preventation: ensure that at least one of the 4 necessary conditions cannot hold deadlock avoidance: dynamically examines the resource-allocation state before allocation  Allow to enter a deadlock state and then recover  deadlock detection dedlock recovery  Ignore the problem and pretend that deadlocks never occur in the system (used by most operating systems, including UNIX)  Deadlock Prevention  Mutual Exclustion(ME): do not require ME on sharable resources  e.g. there is no needto ensure ME on read-only files Some resources are not shareable, however (e.g. printer)  Hold \u0026amp; Wait:  When a process requests a resource, it does not hold any resource Pre-allocate all resources before executing (resource utilization is low, starvation is possible)  No preemption  When a process is waiting on a resource, all its holding resources are preempted (e.g. P1 request R1, which is allocated P2, which in turn is waiting on R2, R1 can be preempted and reallocated to P1) Applied to resources whose states can be easily saved and restored later (e.g. CPU registers \u0026amp; memory) It cannot easily be applied to other resources(e.g. printers \u0026amp; tape drives)  Circular wait  impose a total ordering of all resources types A process requests resources in an increasing order Let $R = {R_0, R_1, \u0026hellip; , R_N}$ be the set of resource types When request $R_k$, should release all $R_i$, $i \\geq k$   Avoidance Algortihm  safe state: a system is in a safe state if there exists a sequence of allocations to satisfy requests by all processes (this sequence of allocations is called safe sequence)   Single instance of resource type (resource-allocation graph (RAG) algorithm based on circle detection) Multiple instance of resource type banker\u0026rsquo;s algorithm based on safe sequence detection  Resource-Allocation Graph (RAG) Algorithm  Request edge Assignment edge Claim edge: $P_i \\rightarrow R_j$, process $P_i$ may request $R_j$ in the future Clain edge converts to request edge (when a resource is requested by process) Assignment edge converts back to claim edge (when a resource is released by a process) Resources must be claimed a priori in the system Grant a request only if no cycle created Check for safety using a cycle-detection algorithm, $O(n^2)$  R2 cannot be allocated to P2  Banker\u0026rsquo;s algorithm  Safe State with Safe Sequnce use for multiple instance of each resource type use a general safety algorithm to pre-determine if any safe sequence exists after allocation only proceed the allocation if safe sequence exists Procedures:  Assume processes need maximum resources Find a process that can be satisfied by free resources Free the resource usage of the process repeat to step 2 until all processes are satisfied  Example Safe sequence: $P_1, P_3, P_4, P_2, P_0$  if Request (P1) = (1,0,2): P1 allocation $\\rightarrow$ 3, 0, 2 (Safe sequence: $P_1, P_3, P_4, P_0, P_2$) if request (P4) = (3,3,0): P4 allocation $\\rightarrow$ 3,3,2 (no safe sequence can be found)   Deadlock Detection  Single instance of each resource type  convert request/assignment edges into wait-for graph deadlock exists if there is a cycle in the wait-for graph   Multiple-Instance for each Resource type Total instances: A(7), B(2), C(6) (Request 表示已经发生)  The system is in a safe state $\\rightarrow$ (no deadlock) If P2 request = $\\rightarrow$ no safe sequence can be found $\\rightarrow$ the system is deadlocked   Deadlock Recovery  process termination  abort all deadlocked processes abort 1 process at a time until the deadlock cyle is eliminated (which process should we abort first)  Resource preemption  select a victim: which one to preempt rollback: partial rollback or total rollback? starvation: can the same process be preempted always?   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"a59146e4b935a81b2d64b2e9336521ce","permalink":"/courses/operating_system/deadlocks/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/deadlocks/","section":"courses","summary":"Introduction  A set of blocked process each holding some resources and waiting to acquire a resource held by another process in the set Ex1: 2 processes and 2 tape dirves, each process holds a tape drive, each process requests another tape drive Ex2: 2 processes, and semaphores A \u0026amp; B, P1(hold B, wait A): wait(A), signal(B), P2(hold A, wait B): wait(B), signal(A)  Necessary Conditions  Mutual exclustion (only 1 process at a time can use a resource) Hold \u0026amp; Wait: a process holding some resources and is waiting for another resource No preemption: a resource can be only released by a process voluntarily Circular wait: there exist a set {$P_0, P_1, \u0026hellip;, P_n$} of waiting process such that $P_0 \\rightarrow P_1 \\rightarrow P_2, \u0026hellip; , P_0$ All four conditions must hold for possible deadlock   System Model  Resources type $R_1, R_2, \u0026hellip; , R_m$ E.","tags":null,"title":"DeadLocks","type":"docs"},{"authors":null,"categories":null,"content":" File Concept  A logical storage unit created by OS (v.s. physical storage unit in disk (sector, track))  file attributes (identifier, Name, type, Location, Size, protection, Last-access time, last-updated time) file operations (creating, reading, writing, repositioning, deleting, truncating) file types: .exe .com .obj .cc .mov (Hint for OS to operate file in a resonable way) Process: open-file table OS: system-wide table (process shared)  Open-File Tables \u0026amp; System-wide table  Per-process table  Tracking all files opened by this process Current file pointer for each opened file Access rights and accounting information  System-wide table  Each entry in the per-process table points to this table Process-independent information such as disk location, access dates, file size    Access Methods  Sequential access  Read/write next (block) Reset: repositoning the file pointer to the beginning Skip/rewind n records  Direct(relative) access  Access an element at an arbitrary positin in a sequence File operation include the block # as parameter Often use random access to refer the access pattern from direct access   Index Access Methods  Index: contains pointers to blocks of a file To find a record in a file (search the index file $\\rightarrow$ find the pointer, use the pointer to directly access the record) with a large file $\\rightarrow$ index could become too large    Directory Structure  Partition (formatted or raw)  raw partiton (no file system): UNIX swap space, database Formatted partition with file system is called volume a partition can be a portion of a disk or group of multiple disks (distributed system) some storage devices (e.g.: floopy disk) dose not and cannot have partition  Directories are used by file system to store the information about the files in the partition  Directory Operations (search, create, delete, list, rename, traverse)  Single-Level Directory All files in one directory, filename has to be unique, poor efficiency in locating a file as number of file increases Two-Level Dicectory  a separate dir for each user path = user name + file name single-level dir problems still exist per user   Tree-Structured Directory  Absolute path: starting from the root Relative path: starting from a directory  Acyclic-Graph Directory  use links to share files or directories (symbolic link ln) a file can have multiple absolute paths when dose a file actually get deleted?  deleting the link but not the file deleting the file but leaves the link $\\rightarrow$ dangling pointer    General-Graph Directory  May contain cycles  Reference count dose not work any more (e.g. self-reference file) How can we deal with cycles? (Garbage collection) First pass traverses the entire graph and marks accessible files or directories second pass collect and free everything that is un-marked poor performance on millions of files  Use cycle-detection algorithm when a link is created  File-System Mounting \u0026amp; File sharing File-System Mounting  A file system must be mounted before it can be accessed Mount point: the root path that a FS will be mounted to Mount timing: boot time, automatically at run-time, manually at run time  mount -t type deivce dir (mount -t ext2 /dev/sda0 /mnt)   File sharing On Multiple Users  Each user: (userID, groupID)  ID is associated with every ops/process/thread/ the user issues  each file has 3 sets of attributes (owner, group, others) Owner attributes describe the privileges for the owner of the file  same for group/others attributes group/others attributes are set by owner or root   Access-Control List  We can create an access-control list (ACL) for each user  check requested file access against ACL problem: unlimited # of users  3 classes of users $\\rightarrow$ 3 ACL (RWX) for each file  owner (e.g 7 = RWX = 111) group (e.g. 6 = RWX = 110) others (e.g. 4 = RWX = 100)   File Protection  File owner/creator should be able to control (what can be done, by whom, access control list(ACL)) file should be kept from (  physical damage (reliability): RAID improper access (protection): i.e. password   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"0f99ca4aed86f46c0fccdba0fe2f437a","permalink":"/courses/operating_system/file_system_interface/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/file_system_interface/","section":"courses","summary":"File Concept  A logical storage unit created by OS (v.s. physical storage unit in disk (sector, track))  file attributes (identifier, Name, type, Location, Size, protection, Last-access time, last-updated time) file operations (creating, reading, writing, repositioning, deleting, truncating) file types: .","tags":null,"title":"File System Interface","type":"docs"},{"authors":null,"categories":null,"content":" File-System Structure  I/O transfer between memory and disk are performed in units of blocks  one block is one or more sectors one sector is usually 512 bytes  One OS can support more than 1 FS types (NTFS, FAT32) Two design problems in FS  interface to user programs interface to physical storage (disk)  Layered File System   Data Structure On-Disk Structure  Boot control block (per partition): information needed to boot an OS from that partition  typical the first block of the partition (empty means no OS) UFS (Unix File Sys): boot block, NTFS: partition boot sector  Partition Control Block (per partion): partion details  details: # of blocks, block size, free-block-size, free FCB pointers USF: superblock, NTFS: Master File Table  File control block (per file): details regrading a file  details: permission, size, location of data blocks UFS: inode, NTFS: stored in MFT(relational database)  Directory structure (pef file system): organize files   In-Memory Structure  in-memory partition table: information about each mounted parition in-memory directory structure: information of recently accessed directories system-wide open-file table: contain a copy of each opened file\u0026rsquo;s FCB (file control block) per-process open-file table: pointer (file handler / descriptor) to the corresponding entry in the above table  Example  create  OS allocates a new FCB update directory structure  OS reads in the corresponding directory Updates the dir structure with the new file name and the FCB (After file being closed), OS writes back the directory structure back to disk  the file appears in user\u0026rsquo;s dir command   Virtual File System  VFS provides an object-oriented way of implementing file systems VFS allows the same system call interface to be used for different types of FS VFS calls the appropriate FS routines based on the partition info  Four main object types defined by Linux VFS:  inode (an individual file, file control block) file object (an open file) superblock object (an entire file system) dentry object (an individual directory entry)  VSF defines a set of operations that must be implemented (e.g. for file object)  int open(\u0026hellip;) (open a file) ssize_t read() (read from a file)  Directory implementation  Linear Lists (list of names with pointers to data blocks, easy to program but poor performance) Hash table \u0026ndash; linear list w/hash data structure, constant time for searching, linked list for collosions on a hash entry, hash table usually has fixed number of entires   Allocation Methods  An allocation method refers to how disk blocks are allocated for files Allocation strategy: Contiguous allocation, Linked allocation, Indexed allocation  Contiguous Allocation - Each file occupies a set of contiguous blocks - num of disk seeks is minimized - The dir entry for each file = (starting num, size) - Both sequential \u0026amp; random access can be implemented efficiently - Problems - External fragmentation $\\rightarrow$ compaction - file cannot grow $\\rightarrow$ extend-based FS\nExtent-Based File System  Many newer file system use a modified contiguous allocation scheme Extent-based file system allocate disk blocks in extents An extent is a contiguous blocks of disks  A file contains one or more extents An extent: (starting block num, length, pointer to next extent)  Random access become more costly Both internal \u0026amp; external fragmentation are possible  Linked Allocation  each file is a linked list of blocks  each block contains a pointer to the next block data portion: block size \u0026ndash; pointer size  file read: following through the list  Problems  only good for sequential-access files random access requires traversing through the link list each access to link listis a disk I/O (link pointer is stored inside the data block) space required for pointer (4\u0026frasl;512 = 0.78%) (solution: cluster of blocks) Reliability (one missing link breaks the whole file)   FAT (File Allocation Table) file system - FAT32 - store all links in a table - 32 bits per table entry - located in a section of disk at the beginning of each partition - FAT(table) is often cached in memory - Random access is improved - Disk head find the location of any block by reading FAT\nIndex Allocation Example  The directory contains the address of the file index block each file has its own index block index block stores block # for file data  Bring all the pointers together into one location: the index block (one for each file) Implement direct and random access efficiently No external fragmentation Easy to create file (no allocation problem) Disadvantages  space for index blocks how large the index block should be:  linked scheme multilevel index combined scheme (inode in BSD UNIX)    Linked Indexed Scheme Multilevel Scheme (two-level) combined scheme: unix inode  File pointer: 4B (32 bits) Let each data/index block be 4KB   Free space  Free-space list: records all free disk blocks Scheme  Bit vector Linked List (same as linked allocation) Grouping (same as linked index allocation) Counting (same as contiguous allocation)  file system usually manage free space in the same way as a file  Bit vector  Bit vector(bitmap): one bit foreach block simplicity, efficient (HW support bit-manipulation instruction) bitmap must be cached for gooe performance  A 1-TB(4KB block) disk needs 32MB bitmap    ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"adb66d458430c7d04542a7c899d35566","permalink":"/courses/operating_system/file_system_implementatin/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/file_system_implementatin/","section":"courses","summary":"File-System Structure  I/O transfer between memory and disk are performed in units of blocks  one block is one or more sectors one sector is usually 512 bytes  One OS can support more than 1 FS types (NTFS, FAT32) Two design problems in FS  interface to user programs interface to physical storage (disk)  Layered File System   Data Structure On-Disk Structure  Boot control block (per partition): information needed to boot an OS from that partition  typical the first block of the partition (empty means no OS) UFS (Unix File Sys): boot block, NTFS: partition boot sector  Partition Control Block (per partion): partion details  details: # of blocks, block size, free-block-size, free FCB pointers USF: superblock, NTFS: Master File Table  File control block (per file): details regrading a file  details: permission, size, location of data blocks UFS: inode, NTFS: stored in MFT(relational database)  Directory structure (pef file system): organize files   In-Memory Structure  in-memory partition table: information about each mounted parition in-memory directory structure: information of recently accessed directories system-wide open-file table: contain a copy of each opened file\u0026rsquo;s FCB (file control block) per-process open-file table: pointer (file handler / descriptor) to the corresponding entry in the above table  Example  create  OS allocates a new FCB update directory structure  OS reads in the corresponding directory Updates the dir structure with the new file name and the FCB (After file being closed), OS writes back the directory structure back to disk  the file appears in user\u0026rsquo;s dir command   Virtual File System  VFS provides an object-oriented way of implementing file systems VFS allows the same system call interface to be used for different types of FS VFS calls the appropriate FS routines based on the partition info  Four main object types defined by Linux VFS:  inode (an individual file, file control block) file object (an open file) superblock object (an entire file system) dentry object (an individual directory entry)  VSF defines a set of operations that must be implemented (e.","tags":null,"title":"File System Implementation","type":"docs"},{"authors":null,"categories":null,"content":"","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"85f179d6cd0695a77a5da0548a6c04e7","permalink":"/courses/parallel_computing/introduction/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/parallel_computing/introduction/","section":"courses","summary":"","tags":null,"title":"Introduction","type":"docs"},{"authors":null,"categories":null,"content":" Disk Structure  Disk drives are addressed as large 1-dim array of logical blocks (logical block: smallest unit of transfer(sector)) Logical blocks are mapped onto disk sequentially  Sector 0: 1st sector of 1st track on the outermost cyl go from outermost cylinder to innermost one  Disk drive attached to a computer by an I/O bus  Sectors Per Track  Constant linear velocity (CLV)  density of bits per track is uniform more sectors on a track in outer cylinders keeping same data rate increase rotation speed in inner cylinders applications: CD-ROM and DVD-ROM  Constant angular velocity (CAV)  keep same rotation speed larger bit density on inner tracks keep same data rate applications: hard disks   Disk Scheduling  Disk-access time has 3 major components  Seek time: move disk arm to the desired cylinder rotational latency: rotate disk head to the desired sector read time: constant transfer time  Disk bandwidth: number of bytes transferred / (complete of last req - start of first req)  Minimize seek time illustrate with a request queue(0-199) (98, 183, 37, 122, 14, 124, 65, 67)  Algorithm  FCFS (First come first served) SSTF(Shortest-seek-time-first)  SSTF scheduling is a form of SJF scheduling; may cause starvation of some requests total head movement: 236 cylinders common and has a natural appeal, but not optimal  SCAN scheduling  disk head move from one end to the other end A.k.a elvator algorithm total head movement: 236 cylinders perform better for disks with heavy load No staravation problem  C-SCAN scheduling  Disk head move in one direction only A variant of SCAN to provide more uniform wait time More uniform wait time  C-LOOK scheduling  version of C-SCAN Disk head moves only to the last request location  Performance is also influenced by the file-allocation method  Contiguous: less head movement Indexed \u0026amp; linked: greater head movement   Disk Management Disk Formatting  Low-level formatting (or physical formatting): dividing a disk into sectors that disk controller can read and write each sector = header + data area + trailer  header \u0026amp; trailer : sector number and ECC (error-correction code) ECC is calculated based on all bytes in data area data area size: 512B, 1KB, 4KB  OS does the next 2 steps to use the disk  partition the disk into one or more groups of cylinders logical formatting (i.e. creation of a file system)   Boot Block  Bootstrap program  Initialize CPU, registers, device, controllers, memory, and then starts OS First boostrap code stored in ROM complete bootstrap in the boot block of the boot disk (aka system disk)   Windows 2000  Run bootstrap code in ROM Read boot code in MBR (Master Boot Record) Find boot partition from partition table read boot sector/block and continue booting   Bad Blocks  Simple disks like IDE disks Sophisticated disks like SCSI disks Sector sparing (forwarding): remap bad block to a spare one  Could affect disk-scheduling performance A few spare sectors in each cylinder during formatting  Sector slipping: ships sectors all down one spot  Swap-Space Management  swap-space: Virtual memory use disk space (swap-space) as an extension of main memory UNIX: allows use of multiple swap spaces Location  part of a normal file system (e.g. NT) less efficient separate disk partition (raw partition) size is fixed allows access to both types (e.g. LINUX)   Swap Space Allocation  1st version: copy entire process between contiguous disk regions and memory 2nd version: copy pages to swap space  Solaris 1: text segments read from file system, thrown away when pageout, only anonymous memory (stack, heap, etc) store in swap space Solaris 2: swap-space allocation only when pageout rather than vitrual memory creation time  Data structures for Swapping   RAID Structure  RAID = Redundant Arrays of Inexpensive Disks  Provide reliability via redundacy improve performance via parallelism  RAID is arranged into different levels (Striping, mirror, Error-correcting code (ECC) \u0026amp; Parity bit)  RAID 0 \u0026amp; RAID 1  RAID 0: non-redundant striping  improve performance via parallelism I/O bandwidth is proportional to the striping count (Both read and write BW increase by N times)  RAID 1: Mirrored disks  Provide reliability via redundancy Read BW increases by N times Write BW remains the same    RAID 2: Hamming code  E.g.: Hamming code (7,4)  4 data bits (on 4 disks) + 3 parity bits (on 3 disks) each parity bit is linear code of 3 data bits  Recover from any single disk failure  can detect up to two disk (i.e. bits) error but can only \u0026ldquo;correct\u0026rdquo; one bit error  better space efficient than RAID1 (75% overhead)   RAID 3 \u0026amp; 4: Parity Bit  Disk controller can detect whether a sector has been read correctly a single parity bit is enough to correct error from a single disk failure RAID 3: bit-level striping; RAID 4: Block-level striping Even though space efficiency Cost to compute \u0026amp; store parity bit RAID4 has higher I/O throughput, because controller does not need to reconstruct block from multiple disks   RAID 5: Distributed Parity  Spread data \u0026amp; parity across all disks Prevent over use of a single disk  Read BW increases by N times, because all four disks can serve a read request write BW:  Method 1: (1) read out all unmodified (N-2) data bits (2) re-compute parity bit (3) write both modified bit and parity bit to disks (BW = N/(N-2+2) = 1) remains the same Method 2: (1) only read the parity bit and modified bit. (2) re-compute parity bit by the difference. (3) write both modified bit and parity bit (BW = N/(2+2) = N/4 times faster)    RAID 6: P + Q Dual Parity Redundancy  Like RAID 5, but stores extra redundant information to guard against multiple disk failure Use Ecc code (i.e. Error correction code) instead of single parity bit Parity bits are also striped across disks   Hybird RAID  RAID 0+1: Stripe then replicate RAID 1+0: Replicate then stripe  First level control by a controller. Therefore, RAID 10 has better fault tolerance than RAID01 when multiple disk failes  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"586aec3089a261a12b3f07c2fbb9794c","permalink":"/courses/operating_system/mass_storage_system/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/mass_storage_system/","section":"courses","summary":"Disk Structure  Disk drives are addressed as large 1-dim array of logical blocks (logical block: smallest unit of transfer(sector)) Logical blocks are mapped onto disk sequentially  Sector 0: 1st sector of 1st track on the outermost cyl go from outermost cylinder to innermost one  Disk drive attached to a computer by an I/O bus  Sectors Per Track  Constant linear velocity (CLV)  density of bits per track is uniform more sectors on a track in outer cylinders keeping same data rate increase rotation speed in inner cylinders applications: CD-ROM and DVD-ROM  Constant angular velocity (CAV)  keep same rotation speed larger bit density on inner tracks keep same data rate applications: hard disks   Disk Scheduling  Disk-access time has 3 major components  Seek time: move disk arm to the desired cylinder rotational latency: rotate disk head to the desired sector read time: constant transfer time  Disk bandwidth: number of bytes transferred / (complete of last req - start of first req)  Minimize seek time illustrate with a request queue(0-199) (98, 183, 37, 122, 14, 124, 65, 67)  Algorithm  FCFS (First come first served) SSTF(Shortest-seek-time-first)  SSTF scheduling is a form of SJF scheduling; may cause starvation of some requests total head movement: 236 cylinders common and has a natural appeal, but not optimal  SCAN scheduling  disk head move from one end to the other end A.","tags":null,"title":"Mass Storage System","type":"docs"},{"authors":null,"categories":null,"content":" Overview  The two main jobs of a computer I/O and computation I/O devices: tape, HD, mouse, joystick, screen I/O subsytems: the methods to control all I/O devices Two conflicting trends  Standardization of HW/SW interfaces Board variety of I/O devices  Device drivers: a uniform device-access interface to the I/O subsystem(Simliar to system calls between apps and OS) I/O Hardware  Port: A connection point between I/O diveces and the host(E.g. USB ports) Bus: A set of wires and a well-defined protocal that specifies message sent over the wires (E.g. PCI bus) Controller: A collecton of electronics that can operate a port, a bus, or a device (A controller could have its own processor, memory, etc(E.g. SCSI controller))   Basic I/O method (Port-mapped I/O)  Each I/O port (device) is identified by a unique port address Each I/O port consists of four registers (1~4 Bytes)  Data-in regsiter: read by the host to get input Data-out register: written by the host to send output status register: read by the host to check I/O status Control register: written by the host to control the device  Program interact with an I/O port through special I/O instructions (differnet from memory access)  I/O methods Categorization  Depends on how to address a deive  port-mapped I/O: use different address space from memory, access by special I/O instruction(e.g. IN, OUT) Memory-mapped I/O: Reserve specific memory space for device, Access by standard data-transfer instruction (e.g. MOV) (more efficient for large memory I/O, vulnerable to accidental modification)  Depends on how to interact with a deivce:  Poll(busy-waiting): processor periodically check status register of a device Interrupt: device notify processor of its completion  Depending on who to control the transer:  Programmed I/O: transfer controlled by CPU Direct memory access(DMA) I/O: controlled by DMA controller(A special purpose controller, design for large data transfer)    Kernel I/O Subsystem  I/O scheduling \u0026ndash; improve system performance by ordering the jobs in I/O queue (e.g. disk I/O order scheduling) Buffering \u0026ndash; store data in memory while transferring between I/O deivces  Speed mismatch between devices Devices with different data-transfer sizes Support copy semantics  Caching \u0026ndash; fast momory that holds copies of data (Key to performance) Spooling \u0026ndash; holds output for a device(e.g. printing, cannot accept interleaved files) Error handlig \u0026ndash; when I/O error happens(e.g. SCSI device returns error information) I/O protection(privileged instructions)  Blocking and Nonblocking I/O  Blocking \u0026ndash; process suspended until I/O completed  Easy to use and understand Insufficient for some needs Use for synchronous communication \u0026amp; I/O  Nonblocking  Implemented via multi-threading Returns quickly with count of bytes read or wriiten Use for asynchronous communication \u0026amp; I/O   Transforming I/O requests to Hardware operations Performance and Improving  I/O is a major factor in system performance  It placs heavy demancs on the CPU to execute device driver code The resulting context switches stress the CPU and its hareware caches I/O loads dwon the memory bus during data copy bewteen controllers and physcial memory Interrupt handling is a relatively expensive task Busy-waiting could be more efficient than interrupt-driven if I/O time is small  Improving Performance  Reduce number of context switches Reduce data copying Reduce interrupts by using large transers, smart controllers, polling Use DMA Balance CPU, memory, bus and I/O performance for highest throughput   Application I/O interface  Device drivers: a uniform device-access interface to the I/O subsystem; hide the differences among device controllers from the I/O sub-system of OS  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"687abf32fd582b5ca914e102b97ccb51","permalink":"/courses/operating_system/io_system/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/io_system/","section":"courses","summary":"Overview  The two main jobs of a computer I/O and computation I/O devices: tape, HD, mouse, joystick, screen I/O subsytems: the methods to control all I/O devices Two conflicting trends  Standardization of HW/SW interfaces Board variety of I/O devices  Device drivers: a uniform device-access interface to the I/O subsystem(Simliar to system calls between apps and OS) I/O Hardware  Port: A connection point between I/O diveces and the host(E.","tags":null,"title":"I/O system","type":"docs"},{"authors":null,"categories":null,"content":" Overview 一般计算机视觉问题包含以下几类:\n 图像分类(Image classification) 目标检测(Object Detection) 风格转换(Neural Style Transfer)  面临计算机视觉问题时，如何处理图像数据变得很重要，对于一般图片 $1000 \\times 1000 \\times 3$, 神经网络的输入层将有 $300,000,000$ 数据, 那么网络权重 $W$ 则很大，这会导致两个后果\n 数据量很少，但是神经网络很复杂，容易出现过拟合 所需要的内存和计算量很大  卷积运算 ","date":1578092400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578092400,"objectID":"b04210900de4364a90299ede207da0d2","permalink":"/courses/deep_learning/cnn/","publishdate":"2020-01-04T00:00:00+01:00","relpermalink":"/courses/deep_learning/cnn/","section":"courses","summary":" Overview 一般计算机视觉问题包含以下几类:\n 图像分类(Image classification) 目标检测(Object Detection) 风格转换(Neural Style Transfer)  面临计算机视觉问题时，如何处理图像数据变得很重要，对于一般图片 $1000 \\times 1000 \\times 3$, 神经网络的输入层将有 $300,000,000$ 数据, 那么网络权重 $W$ 则很大，这会导致两个后果\n 数据量很少，但是神经网络很复杂，容易出现过拟合 所需要的内存和计算量很大  卷积运算 ","tags":null,"title":"Foundations of Convolutional Neural Networks","type":"docs"},{"authors":null,"categories":["Learn"],"content":" Prepare  Perplexity Measurement of how well a probability model predicts a sample.  Definition $$H(P) = -\\sum_x p(x) \\log_2 p(x)$$ $$\\text{Perplexity} = 2^{H(p)}$$ Understanding $H(p)$ be the average of bits to decode the information $\\text{Perplexity}$ is the total amount of all possible information Usage In the NLP, the best language model is one that predicts an unseen test set gives the highest P $$ PP(w) = p (w_1w_2\\dots w_N )^{-1/N}$$ Minmizing perplexity is the same as maximizing probability  Gaussian Distriubtion and T Distribution  T distribution is used when the samples is small T distribution is heavy tail    T-SNE method SNE method (Stochastic Neighbor Embedding)  High Dimension Space $$p_{j | i}=\\frac{\\exp \\left(-\\left\\|x_{i}-x_{j}\\right\\|^{2} / 2 \\sigma_{i}^{2}\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|x_{i}-x_{k}\\right\\|^{2} / 2 \\sigma_{i}^{2}\\right)}$$  Low dimension space\n$$q_{j | i}=\\frac{\\exp \\left(-\\left\\|y_{i}-y_{j}\\right\\|^{2}\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|y_{i}-y_{k}\\right\\|^{2}\\right)}$$  Cost function $$C=\\sum_{i} K L\\left(P_{i} \\| Q_{i}\\right)=\\sum_{i} \\sum_{j} p_{j | i} \\log \\frac{p_{j | i}}{q_{j | i}}$$  How to choose $\\sigma$ for different points\nDifferent region have different density, so the $\\sigma$ determines how many effective neighbors needs to be considered. As the $\\sigma$ increase, the entropy of the distribution is increased. And the entroy has an expontential form with perplexity. Using the perplexity to determine the $\\sigma$ with every point has a fixed perplexity.\n   Problem  The cost function is not symmetric, it focus on retaining the local structure of the data Dealing with outlier Crowding problem : The area of the two-dimensional map that is available to accommodate distant datapoints will not be large enough compared with the area available to accommodate nearby datapoints    T-SNE advantages  Symmetric SNE\n Pairwise similarities in the low dimensional map $q_{ij}$ $$q_{i j}=\\frac{\\exp \\left(-\\left\\|y_{i}-y_{j}\\right\\|^{2}\\right)}{\\sum_{k \\neq l} \\exp \\left(-\\left\\|y_{k}-y_{l}\\right\\|^{2}\\right)}$$  High dimensional $p_{ij}$ $$P_{ij} = \\frac {p_{j|i} + p_{i|j}}{2n}$$   Crowding problem The gradient could be negative, so the low dimensional space could be expanded, and the heavy tail T-Distribution is preferred.\n Easy to compute the gradient\n$$\\frac{\\delta C}{\\delta y_{i}}=4 \\sum_{j}\\left(p_{i j}-q_{i j}\\right)\\left(y_{i}-y_{j}\\right)\\left(1+\\left\\|y_{i}-y_{j}\\right\\|^{2}\\right)^{-1}$$   Algorithm  Data : data Set $\\mathcal{X} = {x_1, x_2, \\dots, x_n}$ cost function parameters: perplexity $Perp$ optimzation paraemters: number of iterations $T$, learning rate $\\eta$, momentum $\\alpha(t)$ Output: low-dimensional data representation $\\mathcal{Y}^{T} = {y_1, y_2, \\dots, y_n}$ Steps  compute pairwise affinities $p_{j|i}$ with perplexity $Perp$ Set $p_{ij} = (p_{j|i} + p_{i|j}) / 2n$, sample initial solution $\\mathcal{Y}^{(0)} = {y_1, y_2, \\dots, y_n}$ from $\\mathcal{N}(0, 10^{-4}I)$ for $t = 1$ to $T$ do  compute low-dimensional affinities $q_ij$ compute gradient $\\partial C / \\partial \\mathcal{Y}$ set $$\\mathcal{Y}^{(t)}=\\mathcal{Y}^{(t-1)}+\\eta \\frac{\\delta C}{\\delta \\gamma}+\\alpha(t)\\left(\\mathcal{Y}^{(t-1)}-\\mathcal{Y}^{(t-2)}\\right)$$    Code with python import numpy as np import pylab def Hbeta(D=np.array([]), beta=1.0): \u0026quot;\u0026quot;\u0026quot; Compute the perplexity and the P-row for a specific value of the precision of a Gaussian distribution. \u0026quot;\u0026quot;\u0026quot; # Compute P-row and corresponding perplexity P = np.exp(-D.copy() * beta) sumP = sum(P) H = np.log(sumP) + beta * np.sum(D * P) / sumP P = P / sumP return H, P def x2p(X=np.array([]), tol=1e-5, perplexity=30.0): \u0026quot;\u0026quot;\u0026quot; Performs a binary search to get P-values in such a way that each conditional Gaussian has the same perplexity. \u0026quot;\u0026quot;\u0026quot; # Initialize some variables print(\u0026quot;Computing pairwise distances...\u0026quot;) (n, d) = X.shape sum_X = np.sum(np.square(X), 1) D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X) P = np.zeros((n, n)) beta = np.ones((n, 1)) logU = np.log(perplexity) # Loop over all datapoints for i in range(n): # Print progress if i % 500 == 0: print(\u0026quot;Computing P-values for point %d of %d...\u0026quot; % (i, n)) # Compute the Gaussian kernel and entropy for the current precision betamin = -np.inf betamax = np.inf Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] (H, thisP) = Hbeta(Di, beta[i]) # Evaluate whether the perplexity is within tolerance Hdiff = H - logU tries = 0 while np.abs(Hdiff) \u0026gt; tol and tries \u0026lt; 50: # If not, increase or decrease precision if Hdiff \u0026gt; 0: betamin = beta[i].copy() if betamax == np.inf or betamax == -np.inf: beta[i] = beta[i] * 2. else: beta[i] = (beta[i] + betamax) / 2. else: betamax = beta[i].copy() if betamin == np.inf or betamin == -np.inf: beta[i] = beta[i] / 2. else: beta[i] = (beta[i] + betamin) / 2. # Recompute the values (H, thisP) = Hbeta(Di, beta[i]) Hdiff = H - logU tries += 1 # Set the final row of P P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP # Return final P-matrix print(\u0026quot;Mean value of sigma: %f\u0026quot; % np.mean(np.sqrt(1 / beta))) return P def pca(X=np.array([]), no_dims=50): \u0026quot;\u0026quot;\u0026quot; Runs PCA on the NxD array X in order to reduce its dimensionality to no_dims dimensions. \u0026quot;\u0026quot;\u0026quot; print(\u0026quot;Preprocessing the data using PCA...\u0026quot;) (n, d) = X.shape X = X - np.tile(np.mean(X, 0), (n, 1)) (l, M) = np.linalg.eig(np.dot(X.T, X)) Y = np.dot(X, M[:, 0:no_dims]) return Y def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0): \u0026quot;\u0026quot;\u0026quot; Runs t-SNE on the dataset in the NxD array X to reduce its dimensionality to no_dims dimensions. The syntaxis of the function is `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array. \u0026quot;\u0026quot;\u0026quot; # Check inputs if isinstance(no_dims, float): print(\u0026quot;Error: array X should have type float.\u0026quot;) return -1 if round(no_dims) != no_dims: print(\u0026quot;Error: number of dimensions should be an integer.\u0026quot;) return -1 # Initialize variables X = pca(X, initial_dims).real (n, d) = X.shape max_iter = 1000 initial_momentum = 0.5 final_momentum = 0.8 eta = 500 min_gain = 0.01 Y = np.random.randn(n, no_dims) dY = np.zeros((n, no_dims)) iY = np.zeros((n, no_dims)) gains = np.ones((n, no_dims)) # Compute P-values P = x2p(X, 1e-5, perplexity) P = P + np.transpose(P) P = P / np.sum(P) P = P * 4.\t# early exaggeration P = np.maximum(P, 1e-12) # Run iterations for iter in range(max_iter): # Compute pairwise affinities sum_Y = np.sum(np.square(Y), 1) num = -2. * np.dot(Y, Y.T) num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y)) num[range(n), range(n)] = 0. Q = num / np.sum(num) Q = np.maximum(Q, 1e-12) # Compute gradient PQ = P - Q for i in range(n): dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0) # Perform the update if iter \u0026lt; 20: momentum = initial_momentum else: momentum = final_momentum gains = (gains + 0.2) * ((dY \u0026gt; 0.) != (iY \u0026gt; 0.)) + \\ (gains * 0.8) * ((dY \u0026gt; 0.) == (iY \u0026gt; 0.)) gains[gains \u0026lt; min_gain] = min_gain iY = momentum * iY - eta * (gains * dY) Y = Y + iY Y = Y - np.tile(np.mean(Y, 0), (n, 1)) # Compute current value of cost function if (iter + 1) % 10 == 0: C = np.sum(P * np.log(P / Q)) print(\u0026quot;Iteration %d: error is %f\u0026quot; % (iter + 1, C)) # Stop lying about P-values if iter == 100: P = P / 4. # Return solution return Y if __name__ == \u0026quot;__main__\u0026quot;: print(\u0026quot;Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\u0026quot;) print(\u0026quot;Running example on 2,500 MNIST digits...\u0026quot;) X = np.loadtxt(\u0026quot;mnist2500_X.txt\u0026quot;) print(X.shape) labels = np.loadtxt(\u0026quot;mnist2500_labels.txt\u0026quot;) Y = tsne(X, 2, 50, 20.0) pylab.scatter(Y[:, 0], Y[:, 1], 20, labels) pylab.show()  Outcome for mnist dataset Reference  Perplexity Intuition (and its derivation) - Towards Data Science GitHub - lvdmaaten/bhtsne: Barnes-Hut t-SNE t-SNE – Laurens van der Maaten  ","date":1570147200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570147200,"objectID":"eaef9af9347ce38745ced87304fde4e9","permalink":"/post/ml/t-sne/","publishdate":"2019-10-04T00:00:00Z","relpermalink":"/post/ml/t-sne/","section":"post","summary":"一种降维方法，将高维数据映射到低维空间，用于可视化。","tags":["machine learning"],"title":"T-SNE","type":"post"},{"authors":null,"categories":["Learn"],"content":" 讲一下随机森林，GBDT，XGBoost  随机森林：\n 集成算法， 有放回的随机选择特征， 构建决策树 不同基学习器是独立的 特征随机和和样本随机 bagging 方法， 最后做投票  优点：\n 集成学习，方差偏差都比较低， 泛化性能比较好 高维数据集处理能力很好，并确定最重要的变量 可以处理确缺失数据 可以并行 不需要归一化  缺点：\n 回归问题表现不好，不能连续输出 无法了解模型内部 忽略属性之间的相关性  参数调配\n 网格搜索， GridSearchCV 贪心算法， 固定其他参数， 把这个参数选好 随机搜索  GBDT (Gradient Boosting Decsion Tree)\n 每一次计算减少上次出现的残差， 为了消除残差，训练一个新的模型， 训练好的弱分类器累加到现有模型中 回归树，不是分类树。  XGBoost\n GBDT 是机器学习算法， XGBoost 是该算法的实现 GBDT 算法基于经验损失函数的负梯度构造新的决策树， 在构建完成之后进行剪枝， 而 XGBoost 在决策树构造阶段加入了正则项 GBDT 在模型只使用了一阶导数信息， XGBoost 对损失函数进行二阶泰勒展开。 根据百分位列举几个可能成为分隔点的候选者， 从中找最佳的分割点 传统 GBDT 采用 CART 作为基分类器， XGBoost 采用了与随机森林类似的策略， 支持对数据采样   梯度提升和梯度下降的区别 利用损失函数相对于模型的负梯度信息来对模型进行更新， 梯度下降过程中， 模型以参数化形式表示，而梯度提升， 模型不是以参数化形式表示，直接定义在函数空间中， 大大提高了可以使用的模型种类。\n随机森林如何处理缺失值  数值型变量：给缺失值一些估计值，例如众数和中位数 描述型变量：缺失部分用出现次数最多的数代替 利用中位数和出现次数做最多的数代替，引入权重， 需要替换的数据先和其他数据做相似度测量, 补全缺失的点是相似的点会具有较高的权重 (所有数据放进随机森林运行一遍，记录每一组数据在决策树中分类路径，判断哪组数据和缺失数据路径最相似， 引入相似度矩阵，记录数据之间的相似度)  AdaBoost 和 XGBoost 区别 详细解释 AUC  TP: 实际这正类， 预测为正类 TN: 实际为负类，预测为负类 FN: 实际为正类， 预测为负类 漏报 FP: 实际为负类，预测为正类 误报\n TPR = TP / (TP + FN) // 正实例被预测正确的比例 (RECALL)\n FPR = FP / (FP + TN) // 负实例被误报的比例\n 选定不同的阈值， 判断是否为正例， 不同的阈值对应不同的指标， 把这些指标连接起来，即为 ROC 曲线\n 解释： 从所有为1的样本从随机选择一个样本， 从所有为0的样本中选择一个样本， 根据分类器对两个样本进行预测，样本1预测为1 的概率为 p1, 样本2预测的概率为 p2, p1 \u0026gt; p2 的概率为AUC\n  随机森林如何评估特征的重要性， Xgboost 如何寻找最优特征  基于基尼指数 看每个特征在随机森林中的每棵树做了多少共享， 取平均值，利用Gini指数, 算利用这个特征分离后的基尼指数之差，取上平均值即可判定\n 基于带外数据 未被抽取样本的集合，使用袋外数据计算第 T_i 颗决策树的校验误差 e1，随机改变 OOB 中的第 J 列， 保持其他列不变， 对第j列进行随机上下置换， 得到误差e2, 利用 e1 - e2 来刻画特征 $j$ 的重要性. 这里可以选择不同的树来加权平均。\n Xgboost xgboost 在训练过程中可以给出各个特征的评分， 最大的增益会被选出作为依据， 从而记忆了每个特征在模型训练时的重要性 xgboost 属于 boosting 集成方法， 样本不放回，每轮计算样本不重复。\n  欧氏距离和其他距离的区别  欧氏距离样本不同属性之间的差别等同看待， 这有时候不满足条件, 当其他特征比较大是，很多特征接近于 0， 没有考虑维度之间的相关性 曼哈顿距离： 异常值的分类结果影响比欧式距离药效， 各维度的贡献基本一样。  Logistic Regression 为什么需要对特征进行离散化  LR属于广义线性模型， 表达能力受限， 离散化为 N 个后， 每个变量有单独的权重， 为模型引入了非线性， 提升了模型的表达能力 速度快，稀疏向量内积乘法运算速度较快， 计算结果更容易存储和扩展 更具有鲁棒性，数据偏离后也不会造成较大影响 离散后的特征可以进行交叉， 提升表达能力 特征离散化后，会更加稳定  LR 与 SVM 区别和联系  相同点  都是分类算法， 本质上都是找超平面 监督学习算法 判别式模型， 不关心数据怎么生成， 只关心数据的差别， 然后利用数据的差别来进行分类  不同点  SVM 只考虑了 support vectors, 也就是和分类最相关的少数点去学习分类器。而逻辑回归通过非线性映射减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重； LR 的损失函数是交叉熵， SVM 的损失函数是 hinge loss, SVM 对样本数据的依赖减少， 提高了训练效率 LR 是参数模型， SVM 是非参数化模型， 参数模型是指数据服从某一分布， 该分布由一些参数确定。非参数模型不需要对数据的总体分布做假设，只知道总体是一个随机变量，不需要知道分布的具体形式。 LR 可以得到每一个类的概率， SVM 无法得到 LR 不依赖与样本之间的距离， SVM 基于样本之间的距离 解决非线性问题时， SVM 可以采用核函数机制， LR 通常不采样核函数计算 SVM 损失函数自带正则， 而 LR 需要在损失函数之外添加正则项    什么是熵， 联合熵， 条件熵， 相对熵， 互信息  熵 衡量系统的有序成都， 熵越大，系统越混乱， 信息熵表示离散事件的出现概率， 一个系统越是有序，其信息熵则越低。 联合熵 两个变量的联合分布，H(x,y) 条件熵 在随机变量 X 发生的前提下， 随机变量 Y 发生so带来的熵的定义则为 条件熵 H(Y|X) H(Y|X) = H(X,Y) - H(X) 相对熵， KL-Divergence D(q||p) 互信息 两个随机变量X，Y的联合分布和各自独立分布乘积的相对熵  牛顿法和梯度梯度下降法  牛顿法是二阶收敛， 梯度法是一阶收敛， 两种都只是局部算法， 梯度法仅考虑了方向， 牛顿法不仅考虑了方向，还兼顾了步长 牛顿法需要没次计算 Hessian 矩阵， 计算比较复杂 拟牛顿法使用正定矩阵来近似 Hessian 矩阵， 从而简化了运算的复杂度。  Kmeans 算法复杂度  O(tKmn): repeat t times, has K centroids, m data, n features; O((m + k)n) : m data, n feautres, K centroids.  协方差和相关性区别 相关性是协方差的标准化形式， 协方差本身很难作比较， 而相关性可以把数值scale 到 -1， 1 之间， 因此可以比较其相关性\nNavie Bayes 因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的.\n详细说些 EM 算法 在概率模型中寻找参数的极大似然估计算法, 最大化后验概率，概率模型依赖于无法观测的隐变量\n E步: 假设参数已知，计算隐变量的后验概率。 M步：带入隐变量的后验概率，最大化似然估计，计算参数值 M 步得到的参数值用于 E 步计算中  似然函数意义： 通过观测的数据推测参数值, 假设参数已经知道，使得观测的数据概率最大。\nKNN 中 K 如何选择 选择较小的 K 值， 较小领域中的训练实例来预测，学习的近似误差变小， 学习的估计误差增大， 即模型容易发生过拟合。 选择较大的 K 值， 容易发生欠拟合， 增大意味着模型变得简单 KNN 为有监督学习的算法， 一般用交叉验证的方法选择最优的K值\n最大似然估计和贝叶斯估计的区别 最大似然是对点估计， 贝叶斯推断是对分布的估计 最大似然估计求出的是最有可能的参数值， 而贝叶斯推断求解的是参数的分布 另外，贝叶斯推断加入了先验概率， 通过先验和似然来求解后验分布，最大似然直接使用似然函数\n什么是最大熵模型 ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"ac015744b0ca8dfe6670a6e51a6ce62b","permalink":"/post/ml/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/post/ml/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/","section":"post","summary":"做一些汇总，方便面试","tags":["machine learning","math"],"title":"机器学习面试总结","type":"post"},{"authors":null,"categories":["Learn"],"content":" 定义  标量 $f$ 对矩阵 $\\mathbf{X}$ 的导数， 定义为 $$\\frac{\\partial f}{\\partial \\mathbf{X}} = \\left[\\frac{\\partial f}{\\partial X_{ij}}\\right]$$ 在求导时不宜拆开矩阵， 需要找到一个整体的算法 一元微积分中的导数与微分的关系有 $$df = f\u0026rsquo;(x) dx$$ 多元的导数与微分的关系 $$df = \\sum_{i=1}^{n}\\frac{\\partial f}{\\partial x_i} dx_i = \\frac{\\partial f}{\\partial \\mathbf{x}}^{t} d\\mathbf{x}$$ 矩阵导数与微分建立联系： $$d f=\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{\\partial f}{\\partial X_{i j}} d X_{i j}=\\operatorname{tr}\\left(\\frac{\\partial f^{T}}{\\partial X} d X\\right)$$ $tr$ 表示方阵对角元素之和   运算法则  加减法 $d(\\mathbf{X}\\pm \\mathbf{Y}) = d\\mathbf{X} \\pm d\\mathbf{Y}$ 乘法 $d(\\mathbf{XY}) = (d\\mathbf{X})\\mathbf{Y} + \\mathbf{X}d\\mathbf{Y}$ 转置 $d(\\mathbf{X}^T) = (d\\mathbf{X})^T$ 参考 matrix cookbook  计算 建立矩阵与微分联系时， 在求出左侧的微分 $df$, 需要写出右侧形式的导数 $$df = tr\\left(\\frac{\\partial f}{\\partial \\mathbf{X}}^T d\\mathbf{X}\\right)$$\n 标量上迹 $a = tr(a)$ 转置 $tr(A^T) = tr(A)$ 加减法 $tr(A\\pm B) = tr(A) \\pm tr(B)$ 矩阵乘法交换 $tr(AB) = tr(BA)$ 矩阵乘法/逐元乘法交换 $\\text{tr}(A^T(B \\odot C))=\\text{tr}((A\\odot B)^TC)$, 其中 A, B, C 尺寸相同  Example  Linear Regression $l = || \\mathbf{Xw - y}||^2$, solve $\\mathbf{w}, \\mathbf{y}$ is a $m\\times 1$ vector, $\\mathbf X$ has the shape $m\\times n$, and $w$\u0026rsquo;s shape is $n\\times 1$, and the $l$ is a scalar\nIn this example, we need to solve a scalar\u0026rsquo;s derivate with respect to a vector. As the defintion, we can\u0026rsquo;t calcuate the derivate of a vector or a matrix directly. Instead, we need to calcuate the differential of a matrix. $$\\begin{aligned}l \u0026= \\mathbf{(Xw - y)}^T(\\mathbf{Xw - y}) \\\\s dl \u0026= (X d \\boldsymbol{w})^{T}(X \\boldsymbol{w}-\\boldsymbol{y})+(X \\boldsymbol{w}-\\boldsymbol{y})^{T}(X d \\boldsymbol{w}) \\end{aligned}$$ As the first and second form is a dot product between two vector, so we can get $$dl = 2(X \\boldsymbol{w}-\\boldsymbol{y})^{T} \\boldsymbol{X} d \\boldsymbol{w}$$ The differential form of $dl$ $$dl = \\frac{\\partial l}{\\partial \\mathbf{w}}^T d\\mathbf{w}$$ The formula transforms to $$\\frac{\\partial l}{\\partial \\boldsymbol{w}}=2 X^{T}(X \\boldsymbol{w}-\\boldsymbol{y})$$ Set the partial form to $0$, we could get $$\\mathbf{X^TXw} = \\mathbf{X^Ty}$$ and the $w$ will be $$\\mathbf{w = (X^TX)^{-1}X^Ty}$$\n  Reference  矩阵求导术（上）知乎\n matrix cookbook  ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"8dfdc45b98414e8d4c2eb7cc78d17846","permalink":"/post/ml/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/post/ml/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/","section":"post","summary":"计算学习中涉及到很多矩阵求导运算， 这里给出简单的矩阵求导","tags":["machine learning","math"],"title":"矩阵求导","type":"post"},{"authors":null,"categories":["Learn"],"content":" Combine multiple base-learners to imporve applicability across different domains or generalization performance in a specific task\nEnsemble Strategy  将多个学习器结合可以带来三个方面的好处\n 增强泛化性能 计算有可能陷入局部最小， 降低局部最小的风险 扩大假设空间   Three advantages     Average\n Simple averaging $$H(X) = \\frac1T \\sum_{i=1}^{T} h_i(x)$$  Weighted averaging $$H(x) = \\sum_{i=1}^{T} \\omega_i h_i(x)$$  Voting\n learner $h_i$ predicte a label from the collection of class ${c_1, c_2, \\dots, c_N}$. The learner $h_i$ generate a vector $(h_i^1(x); h_i^2(x); \\dots; h_i^N(x))$ from the sample $x$. Marority voting   $$H(\\boldsymbol{x})=\\left\\{\\begin{array}{ll}{c_{j},} \u0026 {\\text { if } \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})0.5 \\sum_{k=1}^{N} \\sum_{i=1}^{T} h_{i}^{k}(\\boldsymbol{x})} \\\\ {\\text { reject, }} \u0026 {\\text { otherwise. }}\\end{array}\\right.$$ 可以拒绝， 即保证要提供可靠的结果 - Plurality voting (预测为票数最多的标记) $$H(\\boldsymbol{x})=c_{\\arg \\max_j \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})}$$ - Weighted Voting (加权平均) $$H(\\boldsymbol{x})=c_{\\arg \\max_j \\sum_{i=1}^{T} \\omega_i h_{i}^{j}(\\boldsymbol{x})}$$  学习法(stacking) Stacking 从初始数据集训练出初级学习器， 然后生成一个新数据集用于训练次级学习器。 在新数据集中， 初级学习器的输出被当做样例输入特征， 而初始样本的标记被当做样例标记   Stacking Method    若直接使用初级学习器的训练集产生次级训练集， 则过拟合的风险较大 K-Fold： 初始训练集被划为$k$ 个大小相似的集合 $D_1, D_2, \\dots D_k$, 令 $\\overline{D}_j = D \\backslash D_j$ 给定 T 个 base learner, 初级学习器 $h_t^j$ 通过在 $\\overline{D_j}$ 上使用 第$t$ 个学习算法 对 $D_j$ 中每个样本 $x_i$, 令$z_{it} = h_t^j(x_i)$, 则由 $x_i$ 产生的次级训练样例的示例部分为 $z_i = (z_{i1}; z_{i2}; \\dots z_{iT})$, 标记部分为 $y_i$ 交叉验证过程结束后， 从T个初级学习器产生的次级训练集为 $D\u0026rsquo;=(Z_i, y_i)_{i=1}^m$ 将 $D\u0026rsquo;$ 用于训练次级学习器    Bagging  Bagging is a voting method, but base-learners are made different deliberately  Train them using slightly different training sets Generate $L$ slightly different samples from a given sample is done by bootstrap: given $X$ of size N, we draw N points randomly from $X$ with replacement to get $X^{(j)}$ Train a base-learner for ecah $X^{(j)}$   Boosting  In bagging, generate \u0026ldquo;uncorrelated\u0026rdquo; base-learners is left to chance and unstability of the learning method. (让下面的 learner train 前几个learner错误的地方) 考虑 binary classification: $d^{(j)}(x) \\in {1, -1}$ Cominber three weak learners to generate a strong learner  A week learner has error probability less than 1\u0026frasl;2 A strong learner has arbitrarily small error probability  Traning Algorithm  Given a large traning set, randomly divide in into three Use $X^{(1)}$ to train the first learner $d^{(1)}$ and feed $X^{(2)}$ to $d^{(1)}$\n Use all points misclassified by $d^{d(1)}$ and $X^{(2)}$ to train $d^{(2)}$. Then feed $X^{(3)}$ to $d^{(1)}$ and $d^{(2)}$ Use the points on which $d^{(1)}$ and $d^{(2)}$ disgree to train $d^{(3)}$  Tesing Algorithm  Feed a point it to $d^{(1)}$ and $d^{(2)}$ first. If outpus agree, use them as final predction Otherwise the output of $d^{(3)}$ is taken  Example   Boosting example    Disadvantages Requires a large traning set to afford the three-way split   AdaBoost  Use the same training set over and over again, but how to make points \u0026ldquo;larger\u0026rdquo;. Modify the probabilities of drawing the instances as a function of error Notation:  $\\text{Pr}^{(i,j)}$: probability that an example $x^{(i)}, y^{(i)}$ is drawn to train the $j$th base-learner $d^{(j)}$ $\\varepsilon^{(j)} = \\sum_i \\text{Pr}^{(i,j)} 1(y^{(i)} \\neq d^{(j)}x^{(i)})$ error rate of $d^{(j)}$ on its training set.  Algorithm  Initalize $\\text{Pr}^{(i,j)} = \\frac1N$ for all i start from $j = 1$:  Randomly draw $N$ examples for $X$ with probabilities $\\text{Pr}^{(i,j)}$ and use them to train $d^{(j)}$ Stop adding new base-learners if $\\varepsilon^{(j)} \\geq \\frac12$ Define $\\alpha_j=\\frac12 \\log \\left( \\frac{1-\\varepsilon^{(j)}}{\\varepsilon^{(j)}}\\right)\u0026gt;0$ and set $\\text{Pr}^{(i,j+1)} = \\text{Pr}^{(i,j)} \\exp(-\\alpha_j y^{(i)} d^{(j)} (x^{(i)}))$ for all $i$ Normalize $\\Pr^{(i, j+1)}$   Testing  Given $x$, calcualte $\\hat{y}^{(j)}$ for all $j$ Make final prediction $\\hat{y}$ by voting $\\hat{y} = \\sum_j \\alpha_j d^{(j)}(x)$  Example  Why AdaBoost Works\n Empirical study: AdaBoost reduces overfitting as $L$ grows, even when there is no training error  AdaBoost increases margin (Same as SVM) A large margin imporves generalizability Define margin of prediction of an example $(x^{(i)}, y^{(i)}) \\in X$ as: $$margin(x^{(i)}, y^{(i)}) = y^{(i)}f(x^{(i)}) = \\sum_{j:y^{(i)} = d^{(j)}(x^{(i)})} \\alpha_j - \\sum_{j:y^{(i)} \\neq d^{(j)}(x^{(i)})} \\alpha_j$$    ","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569542400,"objectID":"8c3d3fa9292523b6995eada9f26922af","permalink":"/post/ml/ensemble/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/post/ml/ensemble/","section":"post","summary":"集成方法","tags":["machine learning"],"title":"Ensemble Method","type":"post"},{"authors":["Haomiao Zhang","Changjun Chen*"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"4debd1877fd206f6217ba833715b1130","permalink":"/publication/mixing-remd/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/mixing-remd/","section":"publication","summary":"A mixing-REMD method to fast sample the configurations of protein and obtain the free energy landscape. It combines the REMD and Metadynamis/ABMD which makes the sampling method more powerful.","tags":null,"title":"Combining the Biased and Unbiased Sampling Strategy into One Convenient Free Energy Calculation Method","type":"publication"},{"authors":["Haomiao Zhang","Changjun Chen*"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"b1d1898c0ac996b6e6667151ce68cd51","permalink":"/publication/fsatool/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/fsatool/","section":"publication","summary":"Implementing the Mixing-REMD method by our developing FSATOOL program. It contains the sampling module and Markov State Model(MSM) which it's easy to use.","tags":null,"title":"FSATOOL: A useful tool to do the conformational sampling and trajectory analysis work for biomolecules","type":"publication"},{"authors":null,"categories":null,"content":" Computing devices  Hosts (= end systems) (PC workstations, servers, Phones) running network apps Communication Links (fiber, copper, radio, satellite) physical link  Transmission rate = bandwidth  routers: forward packets (chunks of data) protocols: control sending, receiving of messages (TCP, IP, HTTP, FTP, PPP) Internet: network of networks, loosely hierarchical, public Internet(有限的) versus private Intranet Internet standards (定标准的组织)  RFC request for comments IETF: Internet Engineering Task Force  communication infrastructure enables distributed applications (Web, email, games) communication services provided to applications(connectionless, connection-oriented) Protocol  all communication activity in Internet governed by protocols format and order of messages sent and received among network entities as well as actions taken on the transmission/receipt of a message  network edge : applications and hosts network core: routers, network of networks access networks(连接 edge 和 core, 连接 physical media, communication links)  The network edge  end systems(hosts) run application programs client/server model: client host requests, receives service from always-on server peer-peer model : minimal(no) use of dedicated server TCP services (Transmission Control protocol)  reliable, in-order byte-stream data transfer(loss: 掉的做法是 acknowledgements and retransmissions) Flow control, sender won\u0026rsquo;t overwhelm receiver 流量控制 (sender 可以送多快是由 receiver 决定的) congestion control, senders slow down sending rate when networks congested (connection太多时， 网络发生拥挤， 会放慢速度, 逐渐增大封包量， 如果封包掉了，认为网络拥挤) E.g. HTTP(web), FTP(File transfer), SMTP(email)  UDP (User Datagram Protocol) 不需要建连线  unreliable data transfer no flow control no congestion control E.g. streaming media(掉了不会影响), teleconferencing, DNS, Internet telephony   Network access and physical media  How to connect end systems to edge router?  residential access nets institutional access networks (school, company) mobile access networks  keep in mind:  bandwidth (bits per second) of access network shared or dedicated   Residential access  dialup via modem  up to 56Kbps direct access to router (often less) can\u0026rsquo;t surf and phone at same time  ADSL (asymmetric digital subscriber line)  up to 1 Mpbs upstream up to 8 Mpbs downstream FDM 50KHz - 1MHz for downstream, 4KHz-50kHZ for upstream, 0kHZ-4kHZ for ordinary telephone  HFC: hybrid fiber coaxial cable, asymmetric, up to 30 Mbps downstream, 2Mpbs upstream  network of cable and fiber attaches homes to ISP router, shared access to router among home, issues(congestion, dimensioning) deployment: available via cable components, e.g. MediaOne   Company access  company/university local area network (LAN) connects end system to edge router Ethernet: shared or dedicated link connects end system and router(10Mbps, 100Mpbs, Gigabit Ethernet) deployment: institutions, home  Wireless access networks  shared wireless access network connects end system to router, via base station wireless LANs: 802.11b (WiFI): 11 Mbps, 802.11g: 54Mbps wider-area wireless access, provided by telecom operator, 3G/4G, WAP/GPRS  Physical Media  Bit: propagates between transmitter/receiver pairs Physical link: what lies between transmitter \u0026amp; receiver  Guided media, by solid media Twisted Pairs(tow insulated copper wires) Coaxial cable, two concentric copper conductors, bidirectional(baseband:single channel on cable, broadband: multiple channels on cable) Fiber optic cable: glass fiber carrying light pulses, each pulse a bit, high-speed operation, low error rate: repeaters spaced far apart, immune to electromagnetic noise unguided media, no physical line (effected by reflection, obstruction of objects, interference) terrestrial microwave (45 Mpbs channels) LAN (e.g. Wifi) wide-area (e.g. cellular) satellite(50 Mpbs channel, 270 millisecond end-end delay)   Network core  连接成网状 mesh of interconnected routers how is data transferred through net?  Circuit Switching: dedicated circular per call: telephone net packet-switching: data sent through net in discrete \u0026ldquo;chunks\u0026rdquo;   Circuit Switching  end to end resources reserved(保留一定的频宽) 保留 link bandwidth, switch capacity, divided into pieces dedicated resources: no sharing (别的core无法使用) circuit-like (guaranteed) performance call setup required(建连线)  two splits (TDM)   Packet Switching each end-end data stream divided into packets, user A,B packets share network resources, each packet uses full link bandwidth, resources used as needed\n resource contention (竞争)  aggregate resource demand can exceed amount available congestion: pockets queue, wait for link use store and forward, packets move one hop at a time(先排队， 后发送), transmit over link, wait turn at next link sequence of A \u0026amp; B packets done not have fixed pattern $\\rightarrow$ statistical multiplexing, in TDM each host get some slot in revolving TDM frame  move packets through routers form source to destination (path selection algorithms) datagram network:  destination address in packet determines next hop routes may change during session analogy, driving, asking directions  virtual circuit network  each packet carries tag (virtual circuit ID), tag determines next hop fixed path determined at call setup time, remains fixed through call routers maintain per-call state (每一个标签需要记住)   Packet Switching VS. Circuit Switching Packet switching allows more user to use network Example: 1Mbit link, each user 100kbps when \u0026ldquo;active\u0026rdquo;, active 10% of per time\n Circuit-Switching : 10 users Packet-Switching: with 35 users, probability \u0026gt; 10 active less than 0.004 Packet Switching: greater for bursty data, resource sharing, simpler, no call setup Excessive congestion: packet delay and loss, protocols needed for reliable data transfer, congestion control How to provide circuit-like behavior? bandwidth guarantees needed for audio/video apps, still an unsolved problem  Internet Structure: network of network  roughly hierarchical Tier 1 ISP(internet service provider) (e.g. UUNet, Sprint, AT\u0026amp;T) national/international coverage (treat each other as equals) regional ISP, connect to one or more tire-1 ISPs IXP (Internet Exchange point): meeting point where multiple ISPs can peer together   Delay, Loss in Packet-Switched Networks  Packets queue in router buffers  packet arrive rate to link exceeds output link capacity(packets queue, wait for turn)  types  Processing delay, the time required to examine the packet\u0026rsquo;s header and determine where to direct to packet queuing delay, the packet waits to be transmitted onto the link, depends on congestion level of router transmission delay, time to send bits into link L/R (L: packet length, R: link bandwidth), 多快的速度可以从(router)送出去 propagation delay, d/s = (s: propagation speed in medium, d = length of physical link)  Transmission and Propagation Delay  nodal delay $$d{nodal} = d{proc} + d{queue} + d{trans} + d_{prop}$$  Queueing delay  L: packet length (bits) a: average packet arrival rate R: link bandwidth(bps) transfer intensity = (L*a) / R (L*a) / R ~ 0 : average queueing delay small (L*a) / R $\\rightarrow$ 1: delays become large (L*a) /R \u0026gt; 1: more work arriving than can be serviced, average delay infinite, or packet loss  traceroute program: provides delay measurement from source to router along end-end internet path towards destination  Packet Loss  Queue(also known as buffer) preceding link in buffers has finite capacity when packet arrives to full queue, packet is dropped lost packet may be retransmitted by pervious node, by source and system, or not retransmitted at all  Protocol Layers  Is there any hope of organizing structure of network Layers: each layer implements a service  application: supporting network applications (FTP, SMTP, HTTP) transfer: host-host data transfer (TCP, UDP) network: routing of datagrams from source to destination. IP, routing protocols Link: data transfer between neighboring network elements (PPP, Ethernet) physical: bits \u0026ldquo;on the wire\u0026rdquo;  why layering:  explicit structure allows identification, relationship of complex system\u0026rsquo;s pieces modularization eases maintenance, updating of system, change of implementation of layer\u0026rsquo;s service transparent to rest of system.   Encapsulation ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"3d5587f7517c22667af727893ea96060","permalink":"/courses/computer_network/introduction/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/computer_network/introduction/","section":"courses","summary":"Computing devices  Hosts (= end systems) (PC workstations, servers, Phones) running network apps Communication Links (fiber, copper, radio, satellite) physical link  Transmission rate = bandwidth  routers: forward packets (chunks of data) protocols: control sending, receiving of messages (TCP, IP, HTTP, FTP, PPP) Internet: network of networks, loosely hierarchical, public Internet(有限的) versus private Intranet Internet standards (定标准的组织)  RFC request for comments IETF: Internet Engineering Task Force  communication infrastructure enables distributed applications (Web, email, games) communication services provided to applications(connectionless, connection-oriented) Protocol  all communication activity in Internet governed by protocols format and order of messages sent and received among network entities as well as actions taken on the transmission/receipt of a message  network edge : applications and hosts network core: routers, network of networks access networks(连接 edge 和 core, 连接 physical media, communication links)  The network edge  end systems(hosts) run application programs client/server model: client host requests, receives service from always-on server peer-peer model : minimal(no) use of dedicated server TCP services (Transmission Control protocol)  reliable, in-order byte-stream data transfer(loss: 掉的做法是 acknowledgements and retransmissions) Flow control, sender won\u0026rsquo;t overwhelm receiver 流量控制 (sender 可以送多快是由 receiver 决定的) congestion control, senders slow down sending rate when networks congested (connection太多时， 网络发生拥挤， 会放慢速度, 逐渐增大封包量， 如果封包掉了，认为网络拥挤) E.","tags":null,"title":"Introduction","type":"docs"},{"authors":null,"categories":null,"content":" Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\n模块创建显示接口 在模块中编译和调用程序时, 过程接口的所有细节对编译器都是有效的. 当调用程序时, 编译器可以自动检测过程调用中的参数个数, 类型, 是否参数是数组, 已经每个参数的 INTENT 属性. 在模块内, 编译过程和用USE关联访问过程具有一个显示接口 (explicit interface). Fortran 编译器清楚的知道过程每个参数的所有细节. 不在模块内的过程称为隐式接口 (implicit interface). Fortran 编译器调用程序时, 不知道这些过程的信息.\n函数/子程序作为参数传递 在调用和被调用函数中，函数被声明为外部量时(external), 用户自定义函数才可以当做调用参数传递．　当参数表中的某个名字被声明为外部变量，相当于告诉编译器在参数表中传递的是独立的已编译的函数.\n 函数作为参数传递\nprogram main implicit none interface ! must specify the interface subroutine runfunc(fun, array) real*8 :: array(:, :) real*8, external :: fun end subroutine end interface real*8 :: array(3,3) real*8,external :: fun1 array = 2d0 call runfunc(fun1, array) end program subroutine runfunc(fun, array) implicit none interface !must specify the interface real*8 function fun(array) real*8 :: array(:, :) end function end interface real*8 :: array(:, :) print*, fun(array) end subroutine real*8 function fun1(array) implicit none real*8 :: array(:, :) integer :: i, j fun1 = 0d0 do i = 1, size(array, 1) do j = 1, size(array, 2) fun1 = fun1 + array(i,j) enddo enddo end function  子程序作为参数传递\nprogram main implicit none integer :: n = 5 external :: add, prod real*8 :: array(5) = (/1,2,3,4,5/) call passsub(add, array, n) call passsub(prod, array, n) end program subroutine passsub(sub, array, n) integer :: n external :: sub real*8 :: array(n) call sub(array, n) end subroutine subroutine add(array, n) integer :: n real*8 :: array(n) integer :: i real*8 :: sum = 0 do i = 1, n sum = sum+ array(i) enddo print*, sum end subroutine subroutine prod(array, n) integer :: n real*8 :: array(n) integer :: i real*8 :: cuml = 1 do i = 1, n cuml = cuml * array(i) enddo print*, cuml end subroutine   子程序传递多维数组  显示结构的形参数组 (explicit-shape dummy arrays)\nsubroutine process1(data1, data2, n, m) integer, intent(in) :: n, m real,intent(in),dimension(n,m) :: data1 real,intent(out), dimension(n,m) :: data2 data2 = 3 * data1 end subroutine  不定结构形参数组 (assumed-shape dummy arrays)\n  数组中的每个下标都用冒号代替, 只有子程序或者函数具有显示接口时, 才能使用这种数组. 通常采用将子程序放入模块中, 然后在程序中使用该模块. 编译器可以从接口信息判断每个数组的大小和结构.\nmodule test_module contains subroutine process2(data1, data2) real, intent(in), dimension(:,:) :: data1 real, intent(out), dimension(:, :) :: data2 data2 = 3 * data1 end subroutine end module  关键字参数和可选参数  关键字参数 如果过程接口是显示的, 可以改变参数表中调用参数的顺序, 可以用关键字参数(keyword arguments) 提供更强的灵活性\nmodule procs contains real, function calc(first, second, third) implicit none real, intent(in) :: first, second, third calc = (fisrt - second) / third end function calc end module   在调用时, 即可以用\nprogram test_keywords implicit none write(*, *) calc(3., 1., 2.) write(*, *) calc(first=3., second=1., third=2.) write(*, *) calc(3., third=2., second=1.)   可选参数  通过在形参申明中加入 optional 属性, 指定可选参数 integer, intent(in), optioncal :: upper_limit 可选参数是否出现, 可以用 fortran 中的 present 来判断\nmodule process contains subroutine extremes(a,n, maxval, pos_maxval) integer, intent(in) :: n real, intent(in), dimension(n) :: a real, intent(out), optional :: maxval integer, intent(out), optional :: pos_maxval integer :: i real :: real_max integer :: pos_max real_max = a(1) pos_max = 1 do i = 2, n max: if (a(i) \u0026gt; real_max) then real_max = a(i) pos_max = i end if max enddo if (present(maxval)) then maxval = real_max endif if (present(pos_maxval)) then pos_maxval = pos_max endif end subroutine end module  过程接口和接口块 想要使用不定结构形参数组或者可选参数这类 Fortran 高级特性, 程序必须要有显示接口. 创建显示接口最简单的方法是将过程放在模块中, 但是将过程放在模块中, 在一些场合不可能. 如过程和函数是用早期的 fortran 语言编写, 或者外部函数库是用 C 或者其他语言编写, 这样将过程放在一个模块中并不可能.\n当不能把过程放在模块中, fortran 允许在调用程序中定义一个接口块, 接口快指定了过程所有的接口特征, 编译器根据接口快中的信息执行一致性检查. 接口的一般形式\ninterface interface_body_1 inteface_body_2 ... end interface  使用接口时, 将它放在调用调用单元的最前面\nprogram interface_example implicit none interface subroutine sort(a,n) implicit none real, dimension(:), intent(inout) :: a integer, intent(in) :: n end sobroutine sort end interface real, dimension(6) :: array = (/1., 5., 3., 2., 6., 4.,/) integer :: nvals = 6 call sort(n = nvals, a = array) end program   当为大型旧版程序或者函数库创建接口时, 可以将它们放在调用一个模块中, 可以使用 Use 访问\nmodule interface_def ! no contains statement interface subroutine sort(array, n) ... end surtoutine subroutine sort2(array, n) ... end subroutine end interface end module  接口是独立的作用域, 接口块中的形参变量必须单独声明, 即使这些变量已经在相关的作用域中已经被声明过了. Fortran 2003 含有 import 语句, 如果 import 语句出现在接口定义中, 那么import 语句中指明的变量会被导入, 不需要在接口中再次申明. 如果 import 语句中没有带变量, 所有变量都会被导入.\n Named entities from the host scoping unit are not accessible in an interface body that is not a module procedure interface body. The IMPORT statement makes those entities accessible in such interface body by host association.\n  Generic 使用通用接口块定义过程，过程之间通过不同类型的输入参数区分，可以处理不同类型的数据. 给　interface 加入一个通用名，那么在接口快定义的每个过程接口可以看作一个通过过程．通用过程中所有过程要么都是子程序，要么都是函数.\n例如对于不同类型的数据进行排序，创建一个通用子程序 sort 实现排序，可以使用通用接口块\ninterface sort subroutine sorti(array, nvals) ... end subroutine sorti subroutine sortr(array, nvals) ... end subroutine sortr ... subroutine end interface  模块中用于过程的通用接口 如果子程序都在一个模块中，并且已经有显示接口，如果再加入过程接口，这样是非法的，不能用上述方式添加通用接口．fortran 包含了一个可用在通用接口模块中的特殊module procedure 语句．\n如果４个排序子程序定义在一个模块中，　子程序sort 通用接口是\ninterface sort module procedure sorti module procedure sortr module procedure sortd ... end interface sort  例如，找出一个数组的最大值和最大值位置，不管数组的类型\nmodule generic_maxval implicit none interface maxval module procedure maxval_i module procedure maxval_r ... end interface contains subroutine maxval_i (array, nvals, value_max, pos_maxval) implicit none integer, intent(in) :: nvals integer, intent(in), dimension(nvals) :: array integer, intent(out) :: value_max integer, intent(out), optional :: pos_maxval integer :: i integer :: pos_max value_max = array(i) pos_max = 1 do i = 2, nvals if (array(i) \u0026gt; value_max) then value_max = array(i) pos_max = i end if enddo if (present(pos_maxval)) then pos_maxval = pos_max end if end sobroutine maxval_i subroutine maxval_r(array, nvals, value_max, pos_maxval) ... end subroutine ... end module generic_maxval  内置模块 ortran 2003 定义了内置模块(intrinsic module), 这种模块是由 fortran 编译器创造者预定义和编写的．fortran　2003 中，有三种内置模块\n ISO_FORTRAN_ENV 定义了描述特定计算机中存储器特征的向量(标准的整型包含多少bit, 标准字符包含多少bit),以及该机器所定义的I/O 单元 ISO_C_BINDING 包含了FORTRAN 编译其和特定处理器的c 语言互操作是必要数据 IEEE 处理器运行浮点数的特征  Fortran 指针  Fortran 申明变量为指针时, 需要使用 Pointer 属性, 并申明指针的类型 在 Fortran 变量的类型定义语句中, 加入Target属性, 将其声明为目标变量\nProgram test_ptr implicit none real, pointer::p real, target :: t1 = 10, t2=-17 p =\u0026gt; t1 print*, p, t1, t2 p =\u0026gt; t2 print*, p, t1, t2 end program  指针有三种状态, undefined, associated, disassociated, 通过使用 NuLLIFY 语句将指针和所有目标变量断开\n 指针数组. 指向数组的指针必须声明其指向数组的类型和维数, 不需要申明每一维的宽度\n   Mixing programming with C/C++ Prerequisite  在C中, 所有子程序都是函数, void 类型函数不返回值 在 fortran 中, 函数会传递一个返回值, 子程序不传递返回值 Fortran 调用 C 函数时  被调用的C函数返回一个值, 会从Fortran 中将其作为函数来调用 被调用的C函数不返回值, 将其作为子程序来调用  C 函数调用 Fortran 子程序时  如果被调用Fortran 子程序是一个函数, 会从 C 中将其作为一个返回兼容数据类型的函数来调用 如果被调用的 Fortran 子程序是一个子例程, 会从 C 中将其作为一个返回 int 或 void 值的函数来调用. 如果Fortran 子例程使用交替返回 (返回值为 return 语句中的表达式), 会返回一个值. 如果 return 语句中没有出现表达式, 在Subroutine 语句中申明了交替返回, 则返回 0  C 区分大小写, Fortran 忽略大小写  在 C 子程序中, 使C函数名全部小写 用 -U 选项编译 Fortran 程序, 会告知编译器保留函数/子程序名称现有大小写  Fortran 在编译时,在子程序名和末尾加入下划线, C 编译时函数名称和用户指定名称一致. (Fortran 对于块过程名有两个前导下划线, 减少与用户指定子例程名的冲突)  在 C 函数中, 通过在函数名末尾添加下划线更改名称 使用 BIND(C) 属性声明表明外部函数 是C 语言函数 使用 -ext_names 选项编译对于无下划线外部名称引用  Fortran 传递字符型参数时, 会传递一个附加参数, 指定字符串的长度, 这个参数在 C 中为 long int 量, 按值进行传递 C 数组从 0 开始, Fortran 数组默认以 1 开始 C 数组按行主顺序存储, Fortran 按列主顺序存储  数据类型兼容  C 数据类型 int, long int, long 在 32 位环境下是等价的 (4 字节). 在 64 位环境中, long 和指针为8 字节 数组和结构的元素及字段需要兼容 不能按值传递数组, 字符串和结构 按值传递时, 可以使用 %VAL(arg), 或者Fortran95 程序具有显含的接口块, 使用 VALUE 属性声明了伪参数 数据大小与对齐, 参考 https://docs.oracle.com/cd/E19205-01/819-5262/6n7bvdr18/  按引用传递参数  简单数据类型 string structure array 按值传递参数  代码 example, 参考 https://github.com/zhanghaomiao/C_FORTRAN_MIX\nISO_C_BINDING 模块 Fortran 2003 提供了内置模块,处理C 和 fortran 数据类型的转化\n c_associated(c_ptr1[, c_ptr_2]): determines the status of the C pointer c_ptr_1 or if c_ptr_1 is associated with the target c_ptr_2\nsubroutine association_test(a,b) use iso_c_binding, only: c_associated, c_loc, c_ptr implicit none real, pointer :: a type(c_ptr) :: b if(c_associated(b, c_loc(a))) \u0026amp; stop 'b and a do not point to same target' end subroutine association_test  c_f_pointer(cptr, fptr[,shape]) Assign the target, the C pointer, cptr to Frotran pointer.\n c_f_procpointer(cptr, fptr) assigns the target of the C function pointer cptr to the Fortran procedure pointer fptr\n c_funloc(x) determine the C address of the argument, return type is c_funptr\n c_loc(x) determine the C address of the argument, return type is c_ptr\n c_sizeof(x) calculate the number of bytes of storage the expression x occupies\nuse iso_c_binding real(c_float) :: r, s(5) print *, (c_sizeof(s)/c_sizeof(r) == 5) end   数据类型的对应关系,参考 Fortran wiki\nWorking with C++  在使用C++程序时, 需要加入 extern \u0026lsquo;C\u0026rsquo; 关键字 在使用到 C++ 标准库的数据结构时, 链接时需要加入标准C++库 (-lstdc++) Example   Reference  http://fortranwiki.org/fortran/show/iso_c_binding https://gcc.gnu.org/onlinedocs/gfortran/Interoperability-with-C.html#Interoperability-with-C https://stackoverflow.com/questions/22813423/c-f-pointer-does-not-work http://www.yolinux.com/TUTORIALS/LinuxTutorialMixingFortranAndC.html https://docs.oracle.com/cd/E19205-01/819-5262/6n7bvdr18/  ","date":1555347585,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555347585,"objectID":"562b0146bd6ce3bae22cb7045971f067","permalink":"/post/fortran-programming/","publishdate":"2019-04-15T16:59:45Z","relpermalink":"/post/fortran-programming/","section":"post","summary":"Fortran 易混淆语法","tags":["Fortran","C"],"title":"fortran programming","type":"post"},{"authors":null,"categories":["Learn"],"content":" 要求\n 所有的程序编译时需要使用 -g 参数， 这样gdb程序可以识别 编译时不要使用优化，使用优化时一些变量的值无法跟踪， 即优化等级设置为 O0  命令\n b 后加行数， 设置断点 li(start, [end]) 显示在start 和 end 之间的代码， end 为可选参数， 如果没有 end, 则显示以start 为中心前后5句代码 r 重新开始调试程序, 在遇到断点处终止 c 开始调试程序，在遇到断点处终止, 与 step into my code 功能类似 s 可后接参数 N ， 表示step n 次， 与 step into 功能类似 n 可后接参数 N ， 表示next n 次， 与 step over 功能类似 fin 从子程序跳出， 与 step out 功能类似 p 打印变量值  对于一维数组， 使用 p temp(n) 会显示temp第n个元素 对于二维数组， 使用 p temp(i:j) 会显示第 i 和 第 j 行的元素  where 显示当前程序所处位置 info b 显示所设置的断点 del breakpoints 删除断点   ","date":1552348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552348800,"objectID":"1e2c1f5b902d2a905bbf1de4a067881f","permalink":"/post/gdb%E8%B0%83%E8%AF%95%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/","publishdate":"2019-03-12T00:00:00Z","relpermalink":"/post/gdb%E8%B0%83%E8%AF%95%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/","section":"post","summary":"主要介绍有关GDB一些基本命令","tags":["computer"],"title":"GDB 调试","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"9a6cb9348361050ffbcc0117246adb56","permalink":"/tag/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/tag/","section":"","summary":"Hello!","tags":null,"title":"Landing Page","type":"widget_page"}]