[{"authors":["haomiao"],"categories":null,"content":"I\u0026rsquo;m a PhD in biophysics from the Huazhong University of Science and Technology. By combing the machine learning and molecuar dynamics, my subject focus on the conformation change of protein or RNA. For the sampling biomolecule is diffcult, I develop a method which could improve the sampling efficiency. Then using machine-realted method to analyze the trajectory, the valuable information from the sampling trajectory could be obtained.\nIn my spare time, I learn a lot about computer, such as the algorithm, operating system and computer network, parallel computing. I\u0026rsquo;m interested in programming and had developed the software for my projects.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1598336322,"objectID":"8d1b69e0a4e68e6cbb053f0bf1696d93","permalink":"/authors/haomiao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haomiao/","section":"authors","summary":"I\u0026rsquo;m a PhD in biophysics from the Huazhong University of Science and Technology. By combing the machine learning and molecuar dynamics, my subject focus on the conformation change of protein or RNA.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"General 吴恩达深度学习课程，来自 coursera\nPrepare  Math Python Keras Tensorflow  ","date":1577836800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1597196259,"objectID":"16003ed5db801fca7527d7903007220d","permalink":"/courses/deep_learning/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/courses/deep_learning/","section":"courses","summary":"Deep Learning from the coursera (2020/01 ~ 2020/03)","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"General 计算机网络笔记，来自 国立清华大学公开课\nPrepare  Book: Kurose, Ross, Computer Networking: A TOP-DOWN approach, 7th Edition time: 2019 / 03 ~ 2019 / 06  ","date":1552089600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1583915506,"objectID":"43e729fcaf99ff01f5b63ff3020f3fd9","permalink":"/courses/computer_network/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/courses/computer_network/","section":"courses","summary":"Computer network basic concepts (2019/03 ~ 2019/06)","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"General 操作系统概论笔记， 来自\r国立清华大学公开课\nPrepare  Program: Nachos Project Book: Silberschatz, P. Galvin, and G. Gangne, Operating System Concepts, 9th Edition time: 2019 / 06 ~ 2019 / 09  ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1583915506,"objectID":"ba275b4cb201c30f47d240f2a119939d","permalink":"/courses/operating_system/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/operating_system/","section":"courses","summary":"Operating system basic concepts (2018/10~2019/01)","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"General 并行计算， 来自\r国立清华大学公开课\nPrepare  Program: Pthread、OpenMP, MPI, CUDA, MapReduce Book: Parallel Programming in C with MPI and OpenMP, Michael J. Quinn, McGraw- Hill, 2003. time: 2019 / 06 ~ 2019 / 09  ","date":1552089600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1597196259,"objectID":"16dabac5323a62dc8c4fe733b9b8dcba","permalink":"/courses/parallel_computing/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/courses/parallel_computing/","section":"courses","summary":"Parallel Computing from the open course (2019/03~2019/05)","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Overview 一般计算机视觉问题包含以下几类:\n 图像分类(Image classification) 目标检测(Object Detection) 风格转换(Neural Style Transfer)  面临计算机视觉问题时，如何处理图像数据变得很重要，对于一般图片 $1000 \\times 1000 \\times 3$, 神经网络的输入层将有 $300,000,000$ 数据, 那么网络权重 $W$ 则很大，这会导致两个后果:\n 数据量很少，但是神经网络很复杂，容易出现过拟合 所需要的内存和计算量很大  卷积运算 神经网络从浅层到深层，可以检测出图片的边缘特征，局部特征，最后一层则识别整体的面部轮廓， 这些都是通过卷积神经网络实现的\n边缘检测 垂直检测(Vertical Edges) and 水平检测(Horizontal Edges), 如下图所示:\n边缘检测可以通过相应的 filter 实现，如下图所示\n原始图片尺寸为 6x6, 通过中间的矩阵之后，尺寸为 4x4. 按照如图所示的方式进行计算，卷积运算的求解是在原始矩阵中选择与 filter 相同大小的区域，一一对应相乘后求和，将所得的结果构成一个矩阵\n  下面是一个示例 将矩阵看做一个图像，最右边中间的亮的区域对应于最左边图像中间的黑白分界线\n下面一张图，表示的更加明显   vertical filter and horizontal filter   Sobel filter and Scharr filter, 增加了中间行的权重， 可以提高结果的稳定性   filter 中的值可以设置为参数，通过模型训练得到，神经网络通过反向传播学习到一些低级特性，使得可以对图像的边缘进行检测，不仅仅局限于水平和垂直方向\n  Padding 假设输入图像大小为 $n\\times n$, filter 大小为 $f\\times f$, 则输出图片大小为 $(n-f+1) \\times (n-f+1)$\n因此，存在两个问题\n 卷积运算后，图像的尺寸会减小 原始图片的角落，边缘地区的输出采样很少，因此很多边缘地区的信息被丢失  一般可以使用 padding 方式，对图像进行填充, 通常填充的值为0 如果对每个方向的padding 数为 p, 则padding后的图像大小 $(n+2p) \\times (n+2p)$, 通过 $f\\times f$的filter 之后， 大小则为 $(n+2p-f+1)\\times (n+2p-f+1)$\n进行卷积操作时，有两种选择\n valid 卷积： 没有 padding, 直接卷积， $(n-f+1) \\times (n-f+1)$ same 卷积，进行 padding, 使得卷积后结果大小与输入大小一致，此时有 $p = \\frac{f-1}{2}$  f 通常为奇数， 原因包括 same 卷积 中可以得到整数，并且 filter 有一个可以表示其所在位置的中心点\nStride 通过设置步长来压缩一些信息， 步长表示 filter 在原始图像的水平方向和垂直方向上每次移动的距离。步长设置为2的卷积过程表示为 设步长为 $s$, 填充长度为 $p$, 输入图片大小为 $n\\times n$, filter 大小为 $f\\times f$, 则卷积后的图片大小为 $$\\left\\lfloor\\frac{n+2 p-f}{s}+1\\right\\rfloor \\times\\left\\lfloor\\frac{n+2 p-f}{s}+1\\right\\rfloor$$ 向下取整表示原始矩阵的蓝框完全包含在图像内部时，才进行计算.\n目前为止我们学习的“卷积”实际上被称为互相关（cross-correlation），而非数学意义上的卷积。真正的卷积操作在做元素乘积求和之前，要将滤波器沿水平和垂直轴翻转（相当于旋转 180 度）\n高维卷积 如果对三通道的RGB进行卷积运算，对应的 filter 也是三通道的，这有点类似我们的眼镜，是 see through 这三个维度， 对每个通道 (R, G, B) 与对应的 filter 进行卷积运算求和，然后将三个通道的值相加，如下图所示，是对 27 个乘积和作为一个输出\n如果想进行更多的边缘检测，可以增加 filter 的数量，如下图所示\n设输入图片的尺寸 $n \\times n \\times n_c$ , filter 的尺寸为 $f\\times f \\times n_c$, 则卷积运算后的图片尺寸为 $(n-f+1) \\times (n-f+1) \\times n_c'$, $n_c'$ 为 filter 的数目\n单层卷积神经网络 单层卷积神经网络多了激活函数和偏移量，filter 的数值对应 $W^{[l]}$, 卷积运算对应 $W^{[l]}$ 与 $A^{[l-1]}$的乘积 $$\\begin{array}{c}Z^{[l]}=W^{[l]} A^{[l-1]}+b \\\\ A^{[l]}=g^{[l]}\\left(Z^{[l]}\\right)\\end{array}$$\n对于 $3 \\times 3$的 filter, 总共有 28 各参数，不予图片尺寸相关，因此 CNN 的参数相较于标准神经网络要小很多\n符合表示 设 $l$ 层为卷积层：\n  $f^{[l]}$：滤波器的高（或宽）\n  $p^{[l]}$：填充长度\n  $s^{[l]}$：步长\n  $n^{[l]}_c$：滤波器组的数量\n  输入维度：$n^{[l-1]}_H \\times n^{[l-1]}_W \\times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_H$表示输入图片的高，$n^{[l-1]}_W$表示输入图片的宽。之前的示例中输入图片的高和宽都相同，但是实际中也可能不同，因此加上下标予以区分。\n  输出维度：$n^{[l]}_H \\times n^{[l]}_W \\times n^{[l]}_c$ 。其中\n  $$n^{[l]}_H = \\biggl\\lfloor \\frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \\biggr\\rfloor$$\n$$n^{[l]}_W = \\biggl\\lfloor \\frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \\biggr\\rfloor$$\n 每个滤波器组的维度：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c$ 。其中$n^{[l-1]}_c$ 为输入图片通道数（也称深度）。 权重维度：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c \\times n^{[l]}_c$ 偏置维度：$1 \\times 1 \\times 1 \\times n^{[l]}_c$  由于深度学习的相关文献并未对卷积标示法达成一致，因此不同的资料关于高度、宽度和通道数的顺序可能不同。有些作者会将通道数放在首位，需要根据标示自行分辨。\n简单神经网络 典型的神经网络包含三层， Convolution layer, Pooling layer and Fully Connected Layer.\nPooling Layer 作用是减小模型大小，提高计算速度， 以及减小噪声，提高提取特征的稳健性。 采用最多的一种 Pooling 方式是 Max Pooling, 如下图所示 另外一种是 Average Pooling, 求区域的平均值 Pooling 设有一组超参数，即 filter 大小，步长 s, 选用 MaxPooling 或者 AveragePooling 但并没有参数需要学习，\n设 Pooling 之前输入的维度为 $n_H \\times n_W \\times n_c$ 则输出为\n$$\\left\\lfloor\\frac{n_{H}-f}{s}+1\\right\\rfloor \\times\\left\\lfloor\\frac{n_{W}-f}{s}+1\\right\\rfloor \\times n_{c}$$\nExample 各个层的参数表示为\n   Activation shape Activation Size #parameters      Input: (32, 32, 3) 3072 0   CONV1(f=5, s=1) (28, 28, 6) 4704 158   POOL1 (14, 14, 6) 1176 0   CONV2(f=5, s=1) (10, 10, 16) 1600 416   POOL2 (5, 5, 16) 400 0   FC3 (120, 1) 120 48120   FC4 (84, 1) 84 10164   Softmax (10, 1) 10 850    为什么使用卷积 卷积过程有效的减小了CNN 参数数量，主要通过两种方式\n 参数共享（Parameter sharing）：特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。 稀疏连接（Sparsity of connections）：在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值 池化过程则在卷积后很好地聚合了特征，通过降维来减少运算量。  ","date":1578092400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597807871,"objectID":"b04210900de4364a90299ede207da0d2","permalink":"/courses/deep_learning/cnn/","publishdate":"2020-01-04T00:00:00+01:00","relpermalink":"/courses/deep_learning/cnn/","section":"courses","summary":"Overview 一般计算机视觉问题包含以下几类: 图像分类(Image classification) 目标","tags":null,"title":"Foundations of Convolutional Neural Networks","type":"docs"},{"authors":null,"categories":null,"content":"Creating a network application, run on different end systems and communicate over a network, no software written for devices in network core, network core devices do not function at application layer (e.g. Web server software communicate with browser software)\nPrinciple of network application Application architectures  Client-server  server: always-on host, permanent IP address, server forms for scaling clients: communicate with server, may be intermittently connected, may have dynamic IP address, do not communicate directly with each other   peer-to-peer  no always on server arbitrary end systems directly communicate peers are intermittently connected and change IP address, highly scalable, but difficult to manage   Hybrid of client-server and P2P  Napster, file transfer P2P, file search centralized(peer register content at central server, peers query same central server to locate content) Instant message: chatting between two users is P2P, presence detection/location centralized (User registers its IP address with central server when it comes online,User contacts central server to find IP addresses of buddies)    Processes Communicating  process: program running within a host with same host, two processes communicate using inter-process communication (defined by OS) processes in different hosts communicated by exchanging messages (Client process: process that initiates communication, Server process: process that waits to be contacted) figure  process sends/receives messages to/from its socket socket analogous to door, sending process relies on transport infrastructure on other side of door which brings message to socket at receiving process API (1) choice of transport protocol (2) ability to fix a few parameters addressing process(判断哪一个process需要沟通)  For a process to receive messages, it must have an identifier Every host has a unique 32-bit IP address Identifier includes both the IP address and port numbers(16-bit) associated with the process on the host (HTTP server: 80, Mail server: 25)    App-layer protocol defines  Types of messages exchanged (e.g. request \u0026amp; response) Syntax of message types (what fields in message \u0026amp; how field are delineated) Semantics of the field (i.e. meaning of information in fields) Rules for when and how processes send \u0026amp; respond to messages Public-domain protocols:  defined in RFCs allows for interoperability(相容)    What transport service does provide  Data loss  some apps can tolerate some loss other apps require 100% reliable data transfer   Timing  some apps require low delay to be \u0026ldquo;effective\u0026rdquo;   Bandwidth  some apps require minimum amount of bandwidth to be (games) effective(multimedia) other apps(\u0026ldquo;elastic apps\u0026rdquo;) make use of whatever bandwidth they get    TCP service: connection-oriented, reliable transport, flow control, congestion control (does not providing: timing, minimum bandwidth guarantees) UDP service: unreliable data transfer between sending and receiving process, dons not provide connection setup, reliability, flow control, congestion control, timing or bandwidth guarantee   Web and HTTP  Web  web page consists of objects An object is a file such as HTML, file ,a JPEG image A web page consists of a bast HTML-file and several referenced object The base HTML file references the other objects in the page with the object\u0026rsquo;s URLs (Uniform Resource Locators)    HTTP Overview  HTTP: hypertext transfer protocol web\u0026rsquo;s application layer protocol client/server model (HTTP1.0, HTTP1.1) procedure  client initiates TCP connection (creates socket) to server server accepts TCP connection from client HTTP message exchanged between browser and web server TCP connection closed   HTTP is stateless, server maintains no information about post client requests  HTTP connections  Non-persistent HTTP  At most one object is sent over a TCP connection HTTP/1.0 uses non-persistent HTTP   Persistent HTTP  Multiple objects can be sent over singe TCP connection between client and server A new connection need not be set up for the transfer of each Web object HTTP/1.1 use persistent connections in default mode    Response time modeling (Non-persistent HTTP)  RTT: time to send a small packet to travel from client to server and back Response Time: one RTT to initiate TCP connection, one RTT for HTTP request and first few bytes of HTTP response to return, file transmission time file transmission time = 2RTT + transmit time   Two versions of persistent connections  Persistent without pipelining  client issues new request only when previous response has been received One RTT for each referenced object   Persistent with pipelining  default in HTTP/1.1 client sends requests as soon as it encounters a referenced object as little as one RTT for all the referenced objects    HTTP request message  two types of HTTP messages: request, response HTTP request message (ASCII format)  GET /somedir/page.html HTTP/1.1\rHost: www.someschool.edu\rConnection: close\rUser-agent: Mozilla/5.0\rAccept-language: fr\r Method Types  HTTP/1.0  GET: return the object POST: send information to be stored on the server Head: return only information about the object, such as how old it is, but not the object itself   HTTP/1.1  GET, POST, HEAD PUt: uploads a new copy of existing object in entity body to path specified in URL field DELETE: delete object specified in the URL field    HTTP response message  A status line, which indicates the success of failure of the request Header line, A description of the information in the response, this is the metadata or meta information actual information response status code  200 OK 300 Moved permanently 400 Bad request 404 Not found 505 HTTP version not supported     User-Server Interaction: Cookies  it is often desirable for a Web site to identify users Four components of cookie technology:  cookie header line in the HTTP response message cookie header line in HTTP request message cookie file kept on user\u0026rsquo;s host and managed by user\u0026rsquo;s browser back-end database at web site    advantages  authorization shopping carts recommendations user session state (Web e-mail)   Cookies and privacy:  Permit sites to learn a lot about you you may supply name and e-mail to sites search engines use redirection \u0026amp; cookies to learn yet more advertising companies obtain info across sites    Web caches (proxy server) Satisfy client request without involving origin server  User sets browser: web accesses via cache browser sends all HTTP requests to cache  object in cache: cache returns object else cache requests object from origin server, then returns object to client   cache acts as both client and server Typically cache is installed by ISP why web caching  Reduce response for client request Reduce traffic Internet dense with caches enables poor content providers to effectively deliver content   Example  Conditional GET: client-side caching Don\u0026rsquo;t send object if client has up-to-date cached version\n client: specify date of cached copy in HTTP (If-Modified-Since \u0026lt;data\u0026gt;) server: response contains no object if cached copy is up-to-date HTTP/1.1 304 Not Modified  FTP (File transfer protocol)  transfer file to/from remote host client/server model  client side: the side that initiates transfer server side: remote host   ftp : RFC 959 ftp server port 21 Procedure  FTP client contacts FTP server at port 21, specifying TCP as transport protocol Client obtains authorization over control connection Client browses remote directory by sending commands over control connection When server receives a command for a file transfer, the server opens a TCP data connection to client After transferring one file, server closes connection Servers open a second TCP data connection to transfer another file   FTP server maintains \u0026ldquo;state\u0026rdquo;: current directory  FTP Commands, responses  commands  USER, username PASS, password LIST, return list of file in current directory RETR filename \u0026ndash; retrieves (gets) file STOR filename \u0026ndash; stores file onto remote host   response  331 Username OK, password required 125 data connection already open, transfer starting 425 can\u0026rsquo;t open data connection 452 Error writing file    Electronic Mail (SMTP, POP3, IMAP) Three major components of a mail system user agents, mail servers ,simple mail transfer protocol: SMTP  User Agent:  known as \u0026ldquo;mail reader\u0026rdquo; composing, editing, reading mail messages outgoing, incoming message stored on server   Mail server:  mailbox: contains incoming messages for user message queue: of outgoing (to be sent) mail messages   protocol  SMTP protocol  client: sending mail receiver server: receiving mail server use TCP to reliably transfer email message from client to server, port25 direct transfer: sending server to receiving server three phases of transfer  handshaking transfer of messages closure   command/response interaction  command: ASCII text response: status code and phrase   SMTP use persistent connections SMTP requires message to be in 7-bit ASCII SMTP server uses CRLF.CRLT to determine end of message （CRLF 表示换行）  comparison with HTTP  HTTP pull protocol (client\u0026rsquo;s point of view), SMTP(push protocol) both have ASCII command/response interaction, status codes HTTP dons not require message to be in 7-bit ASCII HTTP: one object in one response message SMTP: multiple objects can be set in one message    Message format  From, To, Subject MIME(Multipurpose Internet Mail extensions) additional lines in message header declare MIME content type MIME types: (TEXT, Image, Audio, Video, Application, Multipart, Message)  Mail access protocols  SMTP: delivery/storage to receiver\u0026rsquo;s server Mail access protocol: retrieval from server  POP3: Post Office Protocol, version 3 (authorization (agent -\u0026gt; server) and download) IMAP: Internet Mail Access Protocol (RFC 2060)  more features (more complex) manipulation of stores messages on server   HTTP: Hotmail, Gmail etc     POP3 protocol  client opens a TCP connection to the mail server on port 110 authorization phase (client: declare username, password, server response: +OK) transaction phase: client  list: list message number retr: retrieve message by number dele quit   update phase: mail server deletes the message marked for deletion   download and delete download and keep: copies of messages on different clients POP3 is stateless across sessions  IMAP  keep all messages in one place: the server Allows user to organize messages in folders IMAP keeps user state across sessions (names of folders and mappings between message IDs and folder name)  DNS (domain name system) MAP between IP addresses and name\n A distributed database implemented in hierarchy of many name servers no server has all name-to-IP address mappings Ap application-layer protocol that allows host, routers, name servers to communicate to resolve names(address/name translation)  DNS provides a core Internet function, implemented as application-layer protocol DNS is an example of the Internet design philosophy of placing complexity at network\u0026rsquo;s edge   Services  Mapping Host aliasing (Canonical and alias name) Mail server aliasing Load distribution, Replicated Web servers: set of IP addresses for one canonical name (负载平衡)   Structures  Client wants IP for www.amazon.com  client queries a root server to find com DNS server client queries com DNS server to get amazon.com DNS server Client queries amazon.com DNS server to get IP address for www.amazon.com   Four types of name services  root name servers top level name servers authoritative name servers  for a host: stores that host\u0026rsquo;s IP address, name   local name servers, each ISP, company has local(default) name server, host DNS query first goes to local name server   Top-level domain(TLD) servers: responsible for com, org, net, edu, etc, and all top-level country domains uk, fr, ca, jp  The company Network solutions maintains servers for com TLD The company Educause maintains servers for edu TLD   Authoritative DNS servers: organization\u0026rsquo;s DNS servers, providing authoritative hostname to IP mappings for organization\u0026rsquo;s servers, can be maintained by organization or service provider local name server: does not strictly belong to hierarchy, each ISP has one Example (iterative query)  recursive query (puts burden of name resolution on contacted name server)  caching and updating  once name server learns mapping, it caches mapping, caches entries timeout(disappear) after some time the contents of each DNS servers were configured statically from a configuration file created by a system manager An update option has been added to the DNS protocol to allow data to be added or deleted from the database vid DNS messages   DNS records distributed database storing resource records (RR), RR format: (name, value, type, ttl)  DNS protocol, messages  DNS protocol query and reply messages, both with same message format   Peer-to-Peer Applications File distribution problem (client-server model) Bit Torrent Structure  P2P centralized directory  when peer connects, it informs central server (IP address, content) Single point of failure performance bottleneck copyright infringement    Socket Programming build client/server application communication using sockets\n client/server paradigm types of transport service via socket API (unreliable datagram, reliable, byte stream-oriented) socket: a host-local, application-created, OS-controlled interface(\u0026ldquo;door\u0026rdquo;) into which application can both send and receive message to/from another application process Socket Programming using Java, cross platform without recompiling, easy programming with high-level API  Socket-programming using TCP  Socket: a door between application process and end-to-end transport protocol (UCP or TCP) TCP service: reliable transfer of bytes from one process to another   Client must contact server  server process must first be running server must have created socket (door) that welcomes client\u0026rsquo;s contact client contacts server by  creating client-local TCP socket specifying IP address, port number of server process when client creates socket: client TCP established connection to server TCP     when contacted by client, server TCP creates new socket for server process to communicate with client  allows server to talk with multiple clients source port numbers used to distinguish clients    \rStream\r\rA stream is a sequence of characters that flow into or out of a process\rAn input stream is attached to some input source for the process (e.g. keyboard or socket)\rAn output stream is attached to an output source (e.g. monitor or socket)\r\r\rTCP client  A \u0026ldquo;Socket\u0026rdquo; object, making connection requests Parameters  Remote ip address Remote tcp port Local ip address (optional) Local port (optional)   Example  import java.net.*;\rimport java.io.*;\rpublic class Client{\rprivate Socket socket = null;\rprivate DataOutputStream out = null;\rprivate BufferedReader input;\rpublic Client(String address, int port)\r{\rtry {\rsocket = new Socket(address, port);\rSystem.out.println(\u0026quot;Connected\u0026quot;);\rinput = new BufferedReader(new InputStreamReader(System.in));\rout = new DataOutputStream(socket.getOutputStream());\r}\rcatch (UnknownHostException u){\rSystem.out.println(u);}\rcatch (IOException i){\rSystem.out.println(i);}\rString line = \u0026quot;\u0026quot;;\rwhile(!line.equals(\u0026quot;Over\u0026quot;))\r{\rtry{\rline = input.readLine();\rout.writeUTF(line);\r}\rcatch(IOException i)\r{\rSystem.out.println(i);\r}\r}\rtry{\rinput.close();\rout.close();\rsocket.close();\r}\rcatch(IOException i)\r{\rSystem.out.println(i);\r}\r}\rpublic static void main(String[] args) {\rClient client = new Client(\u0026quot;127.0.0.1\u0026quot;, 5000);\r}\r}\r import java.net.*;\rimport java.io.*;\rpublic class Server{\rprivate Socket socket = null;\rprivate ServerSocket server = null;\rprivate DataInputStream in = null;\rpublic Server (int port){\rtry{\rserver = new ServerSocket(port);\rSystem.out.println(\u0026quot;Server started\u0026quot;);\rsocket = server.accept();\rSystem.out.println(\u0026quot;client accepted\u0026quot;);\rin = new DataInputStream(socket.getInputStream());\rString line = \u0026quot;\u0026quot;;\rwhile (!line.equals(\u0026quot;Over\u0026quot;))\r{\rtry{\rline = in.readUTF();\rSystem.out.println(line);\r}\rcatch (IOException i)\r{\rSystem.out.println(i);\r}\r}\rSystem.out.println(\u0026quot;Closing connected\u0026quot;);\rsocket.close();\rin.close();\r}\rcatch (IOException i){\rSystem.out.println(i);\r}\r}\rpublic static void main(String[] args){\rServer server = new Server(5000);\r}\r}\r Java socket Programming With thread import java.io.*;\rimport java.text.*;\rimport java.util.*;\rimport java.net.*;\rpublic class Server {\rpublic static void main(String[] args) throws IOException\r{\rServerSocket ss = new ServerSocket(5056);\rwhile (true)\r{\rSocket s = null;\rtry{\rs = ss.accept();\rSystem.out.println(\u0026quot;A new client is connected\u0026quot;);\rDataInputStream dis = new DataInputStream(s.getInputStream());\rDataOutputStream dos = new DataOutputStream(s.getOutputStream());\rSystem.out.println(\u0026quot;Assign new thread for this client\u0026quot;);\rThread t = new ClientHandler(s, dis, dos);\rt.start();\r}\rcatch (Exception e){\rs.close();\re.printStackTrace();\r}\r}\r}\r}\rclass ClientHandler extends Thread{\rprivate DateFormat fordate = new SimpleDateFormat(\u0026quot;yyyy/MM/dd\u0026quot;);\rprivate DateFormat fortime = new SimpleDateFormat(\u0026quot;hh:mm:ss\u0026quot;);\rfinal private DataInputStream dis;\rfinal private DataOutputStream dos;\rfinal private Socket s;\rClientHandler(Socket s, DataInputStream dis, DataOutputStream dos){\rthis.dis = dis;\rthis.dos = dos;\rthis.s = s;\r}\r@Override\rpublic void run(){\rString received;\rString toreturn;\rwhile(true){\rtry{\rdos.writeUTF(\u0026quot;what do you want? [Date|TIme]..\\n\u0026quot; + \u0026quot;Type exit to terminate connection\u0026quot;);\rreceived = dis.readUTF();\rif(received.equals(\u0026quot;Exit\u0026quot;))\r{\rSystem.out.println(\u0026quot;Client\u0026quot; + this.s + \u0026quot;Sends exist\u0026quot;);\rSystem.out.println(\u0026quot;Closing this connection\u0026quot;);\rthis.s.close();\rSystem.out.println(\u0026quot;Connection closed\u0026quot;);\rbreak;\r}\rDate date = new Date();\rswitch (received) {\rcase \u0026quot;Date\u0026quot;:\rtoreturn = fordate.format(date);\rdos.writeUTF(toreturn);\rbreak;\rcase \u0026quot;Time\u0026quot;:\rtoreturn = fortime.format(date);\rdos.writeUTF(toreturn);\rbreak;\rdefault:\rdos.writeUTF(\u0026quot;Inavalid input\u0026quot;);\rbreak;\r}\r}\rcatch (IOException e)\r{\re.printStackTrace();\r}\r}\rtry {\rthis.dis.close();\rthis.dos.close();\r} catch(IOException e)\r{\re.printStackTrace();\r}\r}\r}\r import java.io.*;\rimport java.net.*;\rimport java.util.Scanner;\rpublic class Client {\rpublic static void main(String[] args)\r{\rtry {\rScanner scn = new Scanner(System.in);\rInetAddress ip = InetAddress.getByName(\u0026quot;localhost\u0026quot;);\rSocket s = new Socket(ip, 5056);\rDataInputStream dis = new DataInputStream(s.getInputStream());\rDataOutputStream dos = new DataOutputStream(s.getOutputStream());\rwhile (true) {\rSystem.out.println(dis.readUTF());\rString tosend = scn.nextLine();\rdos.writeUTF(tosend);\rif (tosend.equals(\u0026quot;Exit\u0026quot;)) {\rSystem.out.println(\u0026quot;Closing the connection\u0026quot;);\rs.close();\rSystem.out.println(\u0026quot;Connection closed\u0026quot;);\rbreak;\r}\rString received = dis.readUTF();\rSystem.out.println(received);\r}\rscn.close();\rdis.close();\rdos.close();\r}\rcatch (IOException e)\r{\re.printStackTrace();\r}\r}\r}\r Socket programming with UDP  no handshaking sender explicitly attaches IP address and port of destination to each packet server must extract IP address, port of sender from received packet transmitted data may be received out of water or lost  Exception Handling during initializing sockets, establishing connection, data transmission  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"26ef14007f80c5f869c4e53a650b4865","permalink":"/courses/computer_network/application_layer/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/computer_network/application_layer/","section":"courses","summary":"Creating a network application, run on different end systems and communicate over a network, no software written for devices in network core, network core devices do not function at application","tags":null,"title":"Application Layer","type":"docs"},{"authors":null,"categories":null,"content":"OS services  OS services: Communication, Error detection, User Interface, Resource allocation, Program Execution User Interface  CLI (Command Line Interface) Shell: Command-line interpreter (CSHELL, BASH) GUI (Graphic User Interface) Icons, Mouse   Communication Models  Message passing 例如， process A 与 process B 交换信息： 把 processA 的 信息 copy 到 os 的 memory, 再从 os 把 process A 的信息 copy 到 process B。(Protection 机制, 需要使用 system call)， 缺点是较慢 Shared memory 有一块公用的 memory, 两个 process 可以直接使用，需要先通过 system call 分配好。 (Multi-thread programming)， 缺点是有可能有 dead-lock problem    System Calls \u0026amp; API Request OS services: Process control, File management, Device management, Information maintenance, Communication\n System calls (简单， bug-free)  OS interface to running program An explicit request to the kernel made via a software interrupt Available as assembly-language instructions   API: Application Program Interface (方便使用者, simplicity, portability, efficiency)  Users mostly program against API instead of system call Commonly implemented by language libraries, e.g. C library An api call could involve zero or multiple system call  malloc() and free() use system call brk() abs() don\u0026rsquo;t need to involve system call     不会直接产生 Interrupt, 由 system call 产生 interrupt Three most common APIs:  Win32API for windows POSIX API for POSIX-based systems (Portable Operating System Interface for Unix) 所有的 API 都一样， 但是实作(Library)不一样 Java API for the java virtual machine   System calls: passing parameters  Pass parameters in registers Store the parameters in a table in memory, and the table address is passed as a parameter in a register Push the parameters onto the stack by the program, and pop off the stack by operating system    System Structure User goals and system goals is different User goals: easy to use and learn, as well as reliable, safe and fast System goals: easy to design, implement and maintain as well as reliable, error-free, and efficient\n Simple OS Architecture  unsafe, difficult to enhance    Layered OS Architecture  lower levels independent of upper levels Easy to debugging and maintenance less efficient, difficult to define layers    Microkernel OS (有新的系统朝着这个方向做）  Moves as much from the kernel info \u0026ldquo;user\u0026rdquo; space Communications is provided by message passing, 避免 synchronization Easier for extending and porting    Modular OS Architecture  Modern OS implement kernel modules Object-oriented Approach Each core component is separate Each to talk each other over known interfaces Each is loadable as needed within the kernel    Virtual Machine Hard: Critical Instruction，指的是一些指令在 User space 和 Kernel space 执行的结果不一样， 因此需要知道一些指令是否是 critical instruction.  虚拟化指的是如何加 virtual-machine implementation layer。 工作流程: 虚拟机的Kernel 在运行的时候是假设是在 kernel space, 因此才可以执行privileged instruction. 但从架构来说，虚拟机的 kernel 应该是存在于 user space 中。 Privileged instruction 在 虚拟机的 Kernel space 执行时，系统会产生 一个 signal (exception)， interrupt 会回到底层的OS， 因此系统会得知虚拟机想要执行privileged instruction， 底层的OS 会重复执行这个命令。 缺点: 虚拟机执行效率比较低。 现在很多 CPU 都支持虚拟机， 除了 User mode, kernel mode, 还有 VM mode。 Usage: Protection, compatibility problems (系统兼容)， research and development， honeypot (A virtual honeypot is software that emulates a vulnerable system or network to attract intruders and study their behavior), cloud computing (不需要直接用虚拟机提供， container) Full Virtualization (VMware, 需要有 hardware support, 执行效率才会快)  Run in user mode as an application on top of OS Virtual machine believe they are running on bare hardware but in fact are running inside a user-level application   Para-virtualization: Xen (存在一个 global zone)  Presents guest with system similar but not identical to the guest\u0026rsquo;s preferred systems (Guest must be modified) Hardware rather than OS and its devices are virtualized   Java Virtual Machine  Code translation, compile 执行完成后, 会有自己的 binary code. 再进行一次translation, 在host os执行。 Just-In-Time(JIT) compliers, 会记录执行的命令，然后 reuse.      ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"be3f882d38f59ee357a5355cb0a3f3f5","permalink":"/courses/operating_system/os_structures/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/os_structures/","section":"courses","summary":"OS services OS services: Communication, Error detection, User Interface, Resource allocation, Program Execution User Interface CLI (Command Line Interface) Shell: Command-line interpreter (CSHELL, BASH) GUI (Graphic User Interface) Icons, Mouse","tags":null,"title":"OS structures","type":"docs"},{"authors":null,"categories":null,"content":"Concept  Program: passive entity: binary stored in disk Process: active entity: a program in execution in memory A process includes:  Code segment Data section \u0026ndash;global variables Stack \u0026ndash; temporary local variables and functions Heap \u0026ndash; dynamic allocated variables or classes Current activity (program counter, register contents) 用来管理 process A set of associated resources (e.g. open file handlers)   Process in memory  Threads  lightweight process All threads belonging to the same processes share code section, data section and OS resources (e.g. open files and signals) Each thread has it own thread ID, program counter, register set and stack   Process States  New: process is being created Ready: process is in the memory waiting to be assigned to a processor, 放到 waiting queue 中 Running: instructions are being executed by CPU Waiting: the process is waiting for events to occur (e.g. IO) Terminated: the process has finished execution     Process switch  Process control Block  context switch Context-switch time is purely overhead Switch time depends on memory speed, number of registers, existence of special instructions (a single instruction to save/load all registers), hardware support (multiple sets of registers) \r  Process Scheduling  Concept  Multiprogramming: CPU runs process at all times to maximize CPU utilization Time sharing: switch CPU frequently such that users can interact with each program while it is running Processes will have to wait until the CPU is free and can be re-scheduled   Scheduling Queues  Job queue (New State) \u0026ndash; set of all processes in the system ready queue (Ready state) \u0026ndash; set of all processes residing in main memory, ready and waiting to execute device queue (Wait state) \u0026ndash; set of processes waiting for an I/O device   Diagram  Schedulers  Short- term scheduler (CPU scheduler) \u0026ndash; selects which process should be executed and allocated CPU (Ready state $\\longrightarrow$ Run state) 很频繁 Long-term scheduler (job scheduler) \u0026ndash; selects which processes should be loaded into memory and brought into ready queue (New state $\\longrightarrow$ Ready state) Middle-term scheduler \u0026ndash; selects which processes should be swapped in/out memory (Ready state $\\longrightarrow$ Wait state) 与 virtual memory 相结合    Long-Term Scheduler (现在memory 很大， 现在变为 middle-term scheduler)  control degree of multiprogramming (Degree 很少时，cpu 会 idle， Degree 很多时， 会竞争CPU 资源) Execute less frequently (e.g. invoked only when a process leaves the system or once several minutes) Select a good mix of CPU-bound \u0026amp; I/O- bound processes to increase system overall performance   Short-Term Scheduler  Execute quite frequently (e.g. once per 100ms) Must be efficient (averaging wait time)    Medium-Term Scheduler  Swap out: removing processes from memory to reduce the degree of multiprogramming Swap in: reintroducing swap-out processes into memory Purpose: improve process mix, free up memory Most modern OS doesn\u0026rsquo;t have medium-term scheduler because having sufficient physical memory or using virtual memory    Operations on Processes Tree of process Each process is identified by a unique processor identifier (pid)\n `ps-ael` will list complete info of all active processes in unix \rProcess Creation  Resource sharing  Parent and child processes share all resources Child process shares subset of parent\u0026rsquo;s resources parent and child share no resources   Two possibilities of execution  Parent and children execute concurrently parent waits until children terminate   Two possibilities of address space  Child duplicate of parent (sharing variables) Child has a program loaded into it (message passing)    UNIX/LINUX Process Creation  fork system call  Create a new child process The new process duplicates the address space of its parent Child \u0026amp; parent execute concurrently after fork Child: return value of fork is 0 Parent: return value of fork is PID of the child process   execlp system call Load a new binary file into memory, destroying the old code (memory content reset) wait system call The parent waits for one of its child processes to complete Memory space of fork(), A 调用 fork() 时  old implementation: A\u0026rsquo;s child is an extra copy of parent current implementation: use copy-on-write technique to store differences in A\u0026rsquo;s child address space     Process Termination  Terminate when the last statement is executed or exit() is called Parent may terminate execution of children processes by specifying its PID (abort) Cascading termination killing (exiting) parent $\\longrightarrow​$ killing all its children  Control-C , OS在启动时，会产生 console process, 在执行程序时，是 console 再去产生其他程序，按 Control-C 时是用 console 来终止程序\n\rkill, 通过 OS 来终止程序, 需要权限\n\rInterprocess Communication (IPC)  IPC: a set of methods for the exchange of data among multiple threads in one or more processes Independent process: cannot affect or be affected by other process Cooperating process: otherwise Purposes: information sharing, computation speedup, convenience, modularity  Communication Methods  shared memory:  Require more careful user synchronization implemented by memory access: faster speed use memory address to access data   Message passing:  No conflict : more efficient Use send/recv message Implemented by system call : slower speed   Sockets:  A network connection identified by IP \u0026amp; port (port number is process) Exchange unstructured stream of bytes   Remote Procedure Calls:  Cause a procedure to execute in another space Parameters and return values are passed by message     Shared Memory  Establishing a region of shared memory  Typically, a shared-memory region resides in the address space of the process creating the shared-memory segment Participating processes must agree to remove memory access constraint from OS   Determining the form of the data and the location Ensuring data are not written simultaneously by processes Consumer and Producer problem (系统里面都有很多这样的问题, Compile(Producer), Link(Consumer))  Producer process produces information that is consumed by a Consumer process Buffer as a circular array with size B  next free: in first available : out empty: in = out full (in + 1) % B = out   The solution allows at most (B-1) item in the buffer, otherwise, cannot tell the buffer is fall or empty     while(1){\rwhile (((in+1) % BUFFER_SIZE) == out); // wait if buffer is full\rbuffer[in] = nextProduced;\rin = (in + 1) % BUFFER_SIZE;\r}\rwhile(1){\rwhile(in == out); // wait if buffre is empty\rnextConsumed = buffer[out];\rout = (out+1) % BUFFER_SIZE\r}\r Message-Passing System  Mechanism for processes to communicate and synchronize their actions IPC facility provides two operations:  Send (Message) \u0026ndash; message size fixed or variable Receive(Message)   Message system \u0026ndash; process communicate without resorting to shared variables To communicate, processes need to  Establish a communication link Physical (HW bus, network)  Logical (logical properties) 1. Direct or indirect communication 2. Blocking and non-blocking   Exchange a message via send/receive   Direct Communication  Process must name each other explicitly Links are established automatically One-to-One relationship between links and processes The link may be unidirectional, but is usually bi-directional   Limited modularity, if the name of a process is changed, all old names should be found\n\r Indirect communication (E-mail)  Messages are directed and received from mailboxes Each mailbox has a unique ID Processes can communicate if the share a mailbox Send(A, message) , Receive(A, message), communicate through mailbox A Many-to-Many relationship between links and processes Link established only if processes share a common mailbox Mailbox can be owned either by OS or processes MailBox 怎么解决一对一的问题？ Solutions:  Allow a link to be associated with at most two processes Allow only one process at a time to execute a receive operation Allow the system to select arbitrarily a singe receiver. Sender is notified who the receiver was     Synchronization  Message passing may be either blocking (synchronous) or non-blocking (asynchronous) Blocking send: send is blocked until the message is received by receiver or by the mailbox Nonblocking send: sender sends the message and resumes operation Blocking receive: receiver is blocked until the message is available Nonblocking receive: receiver receives a valid message or a null (存在一个 token, 来判断是否收到信息)   Buffer implementation (中间存在一个 buffer 来进行消息的储存)  Zero capacity: blocking send/receive Bounded capacity: if full, sender will be blocked Unbounded capacity: sender never blocks   Sockets  Remote Procedure Calls: RPC Stubs, client-side proxy for the actual procedure on the server   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"914e48f3e3ba98aa77f952fc8f3da09f","permalink":"/courses/operating_system/process/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/process/","section":"courses","summary":"Concept Program: passive entity: binary stored in disk Process: active entity: a program in execution in memory A process includes: Code segment Data section \u0026ndash;global variables Stack \u0026ndash; temporary local","tags":null,"title":"Process","type":"docs"},{"authors":null,"categories":null,"content":"主要包括下面几个\n LeNet-5 AlexNet VGG ResNet Inception Neural Network  Classical CNN LeNet-5 特点：\n LeNet-5 针对灰度图像而训练，因此输入图片的通道数为 1。 该模型总共包含了约 6 万个参数，远少于标准神经网络所需。 典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 CONV layer-\u0026gt;POOL layer-\u0026gt;CONV layer-\u0026gt;POOL layer-\u0026gt;FC layer-\u0026gt;FC layer-\u0026gt;OUTPUT layer。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用。 当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数  相关论文：\rLeCun et.al., 1998. Gradient-based learning applied to document recognition\nAlexNet  AlexNet 模型与 LeNet-5 模型类似，但是更复杂，包含约 6000 万个参数。另外，AlexNet 模型使用了 ReLU 函数。 当用于训练图像和数据集时，AlexNet 能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据。  相关论文：\rKrizhevsky et al.,2012. ImageNet classification with deep convolutional neural networks\nVGG  VGG 又称 VGG-16 网络，“16”指网络中包含 16 个卷积层和全连接层。 超参数较少，只需要专注于构建卷积层。 结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。 VGG 需要训练的特征数量巨大，包含多达约 1.38 亿个参数。  相关论文：\rSimonvan \u0026amp; Zisserman 2015. Very deep convolutional networks for large-scale image recognition。\nResNet 网络越深，容易存在梯度消失问题，使得网络不好训练\n上图的结构被称为残差块（Residual block）。通过**捷径（Short cut，或者称跳远连接，Skip connections）**可以将 $a^{[l]}$添加到第二个 ReLU 过程中，直接建立 $a^{[l]}$与 $a^{[l+2]}$之间的隔层联系。表达式如下：\n$$z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$$\n$$a^{[l+1]} = g(z^{[l+1]})$$\n$$z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}$$\n$$a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$$\n构建一个残差网络就是将许多残差块堆积在一起，形成一个深度网络。\n为了便于区分，在 ResNets 的论文\rHe et al., 2015. Deep residual networks for image recognition中，非残差网络被称为普通网络（Plain Network）。将它变为残差网络的方法是加上所有的跳远连接。\n在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，训练错误会先减少，然后开始增多。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。\n残差网络有效的原因 假设有一个大型神经网络，其输入为 $X$，输出为 $a^{[l]}$。给这个神经网络额外增加两层，输出为 $a^{[l+2]}$。将这两层看作一个具有跳远连接的残差块。为了方便说明，假设整个网络中都选用 ReLU 作为激活函数，因此输出的所有激活值都大于等于 0。\n则有：\n$$ \\begin{equation} \\begin{split} a^{[l+2]} \u0026amp;= g(z^{[l+2]}+a^{[l]})\n\\\\ \u0026amp;= g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]}) \\end{split} \\end{equation} $$\n当发生梯度消失时，$W^{[l+2]}\\approx0$，$b^{[l+2]}\\approx0$，则有：\n$$a^{[l+2]} = g(a^{[l]}) = ReLU(a^{[l]}) = a^{[l]}$$\n因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。\n注意，如果 $a^{[l]}$与 $a^{[l+2]}$的维度不同，需要引入矩阵 $W_s$与 $a^{[l]}$相乘，使得二者的维度相匹配。参数矩阵 $W_s$既可以通过模型训练得到，也可以作为固定值，仅使 $a^{[l]}$截断或者补零。\n上图是论文提供的 CNN 中 ResNet 的一个典型结构。卷积层通常使用 Same 卷积以保持维度相同，而不同类型层之间的连接（例如卷积层和池化层），如果维度不同，则需要引入矩阵 $W_s$。\n1x1 卷积 1x1 卷积（1x1 convolution，或称为 Network in Network）指滤波器的尺寸为 1。当通道数为 1 时，1x1 卷积意味着卷积操作等同于乘积操作。\n而当通道数更多时，1x1 卷积的作用实际上类似全连接层的神经网络结构，从而降低（或升高，取决于滤波器组数）数据的维度。\n池化能压缩数据的高度（$n_H$）及宽度（$n_W$），而 1×1 卷积能压缩数据的通道数（$n_C$）。在如下图所示的例子中，用 32 个大小为 1×1×192 的滤波器进行卷积，就能使原先数据包含的 192 个通道压缩为 32 个。\n虽然论文\rLin et al., 2013. Network in network中关于架构的详细内容并没有得到广泛应用，但是 1x1 卷积的理念十分有影响力，许多神经网络架构（包括 Inception 网络）都受到它的影响。\nInception 网络 在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 Inception 网络的作用即是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层。\n如图，Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。\n相关论文：\rSzegedy et al., 2014, Going Deeper with Convolutions\n计算成本问题 在提升性能的同时，Inception 网络有着较大的计算成本。下图是一个例子：\n图中有 32 个滤波器，每个滤波器的大小为 5x5x192。输出大小为 28x28x32，所以需要计算 28x28x32 个数字，对于每个数，都要执行 5x5x192 次乘法运算。加法运算次数与乘法运算次数近似相等。因此，可以看作这一层的计算量为 28x28x32x5x5x192 = 1.2亿。\n为了解决计算量大的问题，可以引入 1x1 卷积来减少其计算量。\n对于同一个例子，我们使用 1x1 卷积把输入数据从 192 个通道减少到 16 个通道，然后对这个较小层运行 5x5 卷积，得到最终输出。这个 1x1 的卷积层通常被称作瓶颈层（Bottleneck layer）。\n改进后的计算量为 28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。\n只要合理构建瓶颈层，就可以既显著缩小计算规模，又不会降低网络性能。\n完整的 Inception 网络 上图是引入 1x1 卷积后的 Inception 模块。值得注意的是，为了将所有的输出组合起来，红色的池化层使用 Same 类型的填充（padding）来池化使得输出的宽高不变，通道数也不变。\n多个 Inception 模块组成一个完整的 Inception 网络（被称为 GoogLeNet，以向 LeNet 致敬），如下图所示：\n注意黑色椭圆圈出的隐藏层，这些分支都是 Softmax 的输出层，可以用来参与特征的计算及结果预测，起到调整并防止发生过拟合的效果。\n经过研究者们的不断发展，Inception 模型的 V2、V3、V4 以及引入残差网络的版本被提出，这些变体都基于 Inception V1 版本的基础思想上。顺便一提，Inception 模型的名字来自电影《盗梦空间》。\n使用开源的实现方案 很多神经网络复杂细致，并充斥着参数调节的细节问题，因而很难仅通过阅读论文来重现他人的成果。想要搭建一个同样的神经网络，查看开源的实现方案会快很多。\n迁移学习 在“搭建机器学习项目”课程中，\r迁移学习已经被提到过。计算机视觉是一个经常用到迁移学习的领域。在搭建计算机视觉的应用时，相比于从头训练权重，下载别人已经训练好的网络结构的权重，用其做预训练，然后转换到自己感兴趣的任务上，有助于加速开发。\n对于已训练好的卷积神经网络，可以将所有层都看作是冻结的，只需要训练与你的 Softmax 层有关的参数即可。大多数深度学习框架都允许用户指定是否训练特定层的权重。\n而冻结的层由于不需要改变和训练，可以看作一个固定函数。可以将这个固定函数存入硬盘，以便后续使用，而不必每次再使用训练集进行训练了。\n上述的做法适用于你只有一个较小的数据集。如果你有一个更大的数据集，应该冻结更少的层，然后训练后面的层。越多的数据意味着冻结越少的层，训练更多的层。如果有一个极大的数据集，你可以将开源的网络和它的权重整个当作初始化（代替随机初始化），然后训练整个网络。\n数据扩增 计算机视觉领域的应用都需要大量的数据。当数据不够时，**数据扩增（Data Augmentation）**就有帮助。常用的数据扩增包括镜像翻转、随机裁剪、色彩转换。\n其中，色彩转换是对图片的 RGB 通道数值进行随意增加或者减少，改变图片色调。另外，PCA 颜色增强指更有针对性地对图片的 RGB 通道进行主成分分析（Principles Components Analysis，PCA），对主要的通道颜色进行增加或减少，可以采用高斯扰动做法来增加有效的样本数量。具体的 PCA 颜色增强做法可以查阅 AlexNet 的相关论文或者开源代码。\n在构建大型神经网络的时候，数据扩增和模型训练可以由两个或多个不同的线程并行来实现。\n计算机视觉现状 通常，学习算法有两种知识来源：\n 被标记的数据 手工工程  **手工工程（Hand-engineering，又称 hacks）**指精心设计的特性、网络体系结构或是系统的其他组件。手工工程是一项非常重要也比较困难的工作。在数据量不多的情况下，手工工程是获得良好表现的最佳方式。正因为数据量不能满足需要，历史上计算机视觉领域更多地依赖于手工工程。近几年数据量急剧增加，因此手工工程量大幅减少。\n另外，在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：\n 集成（Ensembling）：独立地训练几个神经网络，并平均输出它们的输出 Multi-crop at test time：将数据扩增应用到测试集，对结果进行平均  但是由于这些方法计算和内存成本较大，一般不适用于构建实际的生产项目。\n","date":1578092400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597196259,"objectID":"636b91499608e7b2d671c8e857008616","permalink":"/courses/deep_learning/cnn_example/","publishdate":"2020-01-04T00:00:00+01:00","relpermalink":"/courses/deep_learning/cnn_example/","section":"courses","summary":"主要包括下面几个 LeNet-5 AlexNet VGG ResNet Inception Neural Network Classical CNN LeNet-5 特点： LeNet-5 针对灰度图像而训","tags":null,"title":"Examples of CNN","type":"docs"},{"authors":null,"categories":null,"content":"Background  Main memory and registers are the only storage CPU can access directly Collection of processes are waiting on disk to be brought into memory and be executed Multiple programs are brought into memory to improve resource utilization and response time to users A process may be moved between disk and memory during its execution Multistep processing of a program   Address Binding  Address Binding - Compile Time  Program is written as symbolic code Compiler translates symbolic code into absolute code If starting location changes (recompile)   Address Binding - Load Time  Complier translates symbolic code into relocatable code Relocatable code: machine language that can be run from any memory location If starting location changes (reload the code)   Address Binding - Execution Time  Compiler translates symbolic code into logical-address (virtual-address) code Special hardware (MMU memory management unit) is needed for this scheme Most general-purpose OS use this method   MMU(Memory-management unit)  Hardware device that maps virtual to physical address The value in the relocation register is added to every address generated by a user process at the time it is sent to memory    Logical VS. Physical Address  Logical Address — generated by CPU (a.k.a virtual address) Physical address — seen by the memory module Compile-time \u0026amp; load time address binding (logical address = physical address) Execution-time address binding (logical address $\\neq$ physical address) The user program deals with logical addresses; it never sees the real physical addresses    Dynamic loading  The entire program doesn\u0026rsquo;t need all memory for it to execute, it\u0026rsquo;s a routine is loaded into memory when it is called Better memory-space utilization, unused routine is never loaded, particularly useful when large amounts of code are infrequently used (e.g., error handling code) No special support from OS is required implemented through program (library, API calls) Dynamic Loading Example in C  dlopen(): opens a library and prepares it for use desym(): looks up the value of a symbol in a given opened library dlclose(): close a DL library    #include \u0026lt;dlfcn.h\u0026gt;\rint main() {\rdouble (*cosine)(double);\rvoid* handle = dlopen(\u0026quot;/lib/libm.so.6\u0026quot;, RTLD_LAZY);\rcosine = dlsym(handle, \u0026quot;cos\u0026quot;);\rprintf(\u0026quot;%f\\n\u0026quot;, (*cosine)(2.0)); // load into memory\rdlclose(handle);\r}  Static/Dynamic Linking  Static Linking: libraries are combined by the loaded into the program in-memory image  Waste memory: duplicated code Faster during execution time    Dynamic Linking: Linking postponed until execution time  Only one code copy in memory and shared by everyone A stub is included in the program in-memory image for each lib reference Stub call $\\rightarrow$ check if the referred lib is in memory $\\rightarrow$ if not, load the lib $\\rightarrow$ execute the lib DLL (Dynamic link library) on Windows     Swapping  A process can be swapped out of memory to a backing store, and later brought back into memory for continuous execution, also used by midterm scheduling, different from context switch Backing store: a chunk of disk, separated from file system, to provide direct access to these memory images Free up memory, roll out, roll in, swap lower-priority process with a higher one Swap back memory location  if binding is done at compile / load time, swap back memory address must be same if binding is done at execution time, swap back memory address can be different   A process to be swapped == must be idle (不能做I/O)  Imagine a process that is waiting for I/O is swapped? 1. Never swap a process with pending I/O 2. I/O operations are done through OS buffers (i.e. a memory space not belongs to any user processes)   Major part os swap time is transfer time; total transfer time is directly proportional to the amount of memory swapped  Memory allocation  Fixed-partition allocation (规划停车场) Each process loads into one partition of fixed-size, degree of multi-programming is bounded by the number of partitions Variable-size partition Hole: block of contiguous free memory, holes of various size are scattered in memory  Multiple Partition (Variable-size) method  First-fit allocate the 1st hole that fits Best-fit allocate the smallest hole that fits (must search through the whole list) Worst-fit allocate the largest hole (must also search through the whole list) First-fit and best-fit better than worst-fit in terms of speed and storage utilization  Fragmentation (存在零碎的空间)  external fragmentation Total free memory space is big enough to satisfy a request, but is not contiguous, occur in variable-size allocation Internal fragmentation  Memory that is internal to a partition but is not being used, occur in fixed-partition allocation   Solution: compaction  Shuffle the memory contents to place all free memory together in one large block at execution time Only if binding is done at execution time     Non-contiguous memory Allocation - Paging  Divide physical memory into fixed-sized blocks called frames Divide logical address space into blocks of the same size called pages To run a program of n pages, need to find n free frames and load the program keep track of free frames Set up a page table to translate logical to physical addresses Benefit  Allow the physical-address space of a process to be noncontiguous Avoid external fragmentation Limited internal fragmentation Provide shared memory/pages    Page table  Each entry maps to the base address of a page in physical memory A structure maintained by OS for each process  page table includes only pages owned by a process A process cannot access memory outside its space     Address Translation Scheme  Logical address is divided into two parts  Page number (p) Use as an index into a page table which contains base address of each page in physical memory, N bits means a process can allocate at most $2^{N}$ pages Page offset(d) Combines with base address to define the physical memory address that is sent to the memory unit, N bits means the page size is $2^{N}$   Physical address = page base address + page offset example: If page size is 1KB(2^10) and page 2 maps to frame 5. Given 13 bits logical address: (p=2, d=20), what is physical address? $$5*(1KB) + 20 = 1,010,000,000,000 + 0,000,010,100 = 1,010,000,010,100$$ Figure  total number of pages dose not need to be the same as the total number of frames Given 32 bits logical address, 36 bits physical address and 4KB page size, what does it mean?  Page table size: $2^{32} / 2^{12} = 2^{20}$ entries Max program memory: $2^{32} = 4GB$ Total physical memory size: $2^{36} = 64GB$ Number of bits for page number: $2^{20}$ pages $\\rightarrow 20$ bits Number of bits for frame number: $2^{24} \\text{frames} \\rightarrow 2^24$ bits number of bits for page offset: 4KB page size = $2^{12}$ bytes $\\rightarrow 12$   Page / Frame Size  Typically power of 2 Ranging from 512 bytes to 16 MB/ page, 4KB/8KB page is commonly used Larger page size $\\rightarrow$ More space waste page sizes have grown over time , memory, process, data sets have become larger    Paging Summary  Paging helps separate user\u0026rsquo;s view of memory and the actual physical memory User view\u0026rsquo;s memory: one single contiguous space OS maintains a copy of the page table for each process OS maintains a frame table for managing physical memory  One entry for each physical frame Indicate whether a frame is free or allocated If allocated, to which page of which process or processes    Implementation of Page Table  Page table is kept in memory Page table base register (PTBR) (需要load到MMU的register)  The physical memory address of the page table The PTBR value is stored in PCB (Process Control Block) Changing the value of PTBR during Context-switch   With PTBR, each memory reference results in 2 memory reads, one for the page table and one for the real address The 2-access problem can be solved by, Translation Look-aside Buffers(TLB) which is implemented by Associative memory Associative Memory All memory entries can be accessed at the same time, lookup time is O(1), each entry corresponds to an associative register, the number of entries are limited (64 ~ 1024)  TLB is a cache for page table shared by all processes \r TLB must be flushed after a context switch, otherwise, TLB entry must has a PID field (address-space identifiers (ASIDs)), the flush method is preferred. Effective Memory-Access Time (EMAT)  $20 ns$ for TLB search $100 ns$ for memory access $70%$ TLB hit-ratio  $70$ TLB hit-ratio $\\text{EMAT} = 0.7\\times (20+100) + (1-0.7) \\times (20+100+100) = 150ns$ $98%$ TLB hit-ration $\\text{EMAT} = 0.98\\times 120+0.02 \\times 220=122ns$      Memory Protection  Each page is associated with a set of protection bit in the page table (A bit to define read/write/execution permission)\n\r Common use : valid-invalid bit  Valid: the page/frame is in the process\u0026rsquo; logical address space, and is thus a legal page Invalid: the page/frame is not in the process\u0026rsquo; logical address space potential issues: Un-used page entry cause memory-waste, use page table length register(PTLR) Process memory may NOT be on the boundary of a page, memory limit register is still needed    Shared Pages Paging allows processes share common code, which must be reentrant  Reentrant code (pure code): it never change during execution, text editors, compilers, web servers, etc Only one copy of the shared code needs to be kept in physical memory Two (several) virtual addresses are mapped to one physical address Process keeps a copy of its own private data and code Shared code must appear in the same location in the logical address space of all processes     Page Table Memory Structure  Page table could be huge and difficult to be loaded  4GB ($2^{32}$) logical address space with 4KB ($2^{12}$) page, needs 1 million $2^{20}$ page table entry Assume each entry need 4 bytes (32 bits), total size = 4MB (MMU 读的时候是需要连续的4MB memory) Need to break it into several smaller page tables, better within a single page size (i.e. 4KB) Or reduces the total size of page table    Hierarchical Paging  Break up the logical address space into multiple page tables  12-bit offset (d) $\\rightarrow$ 4KB($2^{12}$) page size 10-bit outer page number $\\rightarrow$ 1K $2^{10}$ page table entries 10-bit inner page number $\\rightarrow$ 1K $2^{10}$ page table entries    Two-level paging example(32-bit address with 4KB ($2^{12}$) page size)  example:  64-bit Address?  2 level: 42(p1) + 10 (p2) + 12 (offset), outer table requires $2^{42} \\times 4B = 16 {TB}$ contiguous memory 6 level: $12(p1) + 10(p2) + 10(p3) + 10(p4) + 10 (p5) + 12 (offset)$, needs 6 memory accesses  \rSPAPC(32-bit) and linux use 3-level paging, Motorola 68030 (32-bit) use 4-level paging\r\r  Hashed Page Table  Commonly-used for address \u0026gt; 32 bits Virtual page number is hashed into a hash table The size of the hash table varies, larger hash table $\\rightarrow$ smaller chains in each entry Each entry in the hashed table contains  Virtual Page Number, Frame Number, Next Pointer Pointers waster memory Traverse linked list waste time \u0026amp; cause additional memory references    将 entries group 到一起， 存入连续的空间，MMU一次性读进去，可以减少 LinkedList traverse 的时间\n\r  Inverted Page Table (很少见）  Maintains NO page table for each process ( 节省 memory 空间） Maintains a frame table for the whole memory, one entry for each real frame of memory Each entry in the frame table has (PID Page number) Eliminate the memory needed for page tables but increase memory access time, each access needs to search the whole frame table (use hashing for the frame table) Hard to support shared/page memory   Segmentation  Memory-management scheme that supports user view of memory A program is a collection of segments. A segment is a logical unit includes following:   Segmentation Table  Logical address: (segmentation #, offset), offset has the same length as physical address Maps two-dimensional physical addresses, each table entry has:  Base (4 bytes): the start physical address Limit (4 bytes): the length of the segment   Segment-table base register(STBR), the physical address of the segmentation table Segment-table length register (STLR), the number of segments example   Segmentation Hardware  Limit register is used to check offset length MMU allocate memory by assigning an appropriate base address for each segment (physical address cannot overlap between segments)  Sharing and Protection  Protection bits associated with segments  Read-only segment (code) Read-write segments (data, heap, stack) Code sharing occurs at segment level (memory communication, shared library) Share segment by having same base in two segment tables      Segmentation \u0026amp; paging  Apply segmentation in logical address space Apply Paging in physical address space   Address Translation  CPU generates logical address  Given to segmentation unit $\\rightarrow$ produces liner address Linear address given to paging unit $\\rightarrow$ generates physical address in main memory   Segmentation and paging units form equivalent of MMU   Example  Let the physical memory size is 521B, the page size is 32B and the logical address of a program can have 8 segments. Given a 12 bits hexadecimal logical address \u0026ldquo;448\u0026rdquo;, translate the address with below page and segment tables   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"1085b2813425cd3a80d37cbe197b3cab","permalink":"/courses/operating_system/memory_manage/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/memory_manage/","section":"courses","summary":"Background Main memory and registers are the only storage CPU can access directly Collection of processes are waiting on disk to be brought into memory and be executed Multiple programs","tags":null,"title":"Memory Management","type":"docs"},{"authors":null,"categories":null,"content":"Transport services and protocols  provide logical communication between app processes running on different hosts transport protocols run in end systems  send side: breaks app message into segments, passes to network layer receive side: reassembles segments into message passes to app layer   more than one transport protocol available to apps (Internet: TCP and UDP)  network layer vs. transport layer  network layer: logical communication between hosts transport layer: logical communication between processes   reliable, in-order delivery(TCP), congestion control, flow control, connection setup unreliable, unordered delivery(UDP): no-frills extension of \u0026ldquo;best-effort\u0026rdquo; IP services not available: delay guarantees, bandwidth guarantees  Multiplexing / demultiplexing  demultiplexing: delivering received segments to correct socket multiplexing: gathering data from multiple sockets, enveloping data with header(later used for demultiplexing)  Demultiplexing  Host receive IP datagrams:  each datagram has source IP address, destination IP address each datagram carries 1 transport-layer segment each segment has source, destination port number   host uses IP addresses \u0026amp; port numbers to direct segment to appropriate socket   Connectionless demultiplexing  create sockets with port number UDP socket identified by two-tuple(destination IP address and destination port number) when host receives UDP segment: checks destination port number in segment, directs UDP segment to socket with that port number IP datagrams with different source IP addresses and/or source port numbers directed to same socket  Connection-oriented demultiplexing  TCP socket identified by 4-tuple: source/destination IP address, source/destination port number receiving host uses all four segment to appropriate socket Server host may support many simultaneous TCP sockets, each socket identified by its own 4-tuple Web servers have different sockets for each connection client, non-persistent HTTP will have different socket for each request   UDP: User Datagram Protocol (RFC 768)  \u0026ldquo;no frills\u0026rdquo;, \u0026ldquo;bare bones\u0026rdquo; Internet transport protocol \u0026ldquo;best effort\u0026rdquo; service connectionless, no handshaking between UDP sender, receiver, each UDP segment handled independently of others why using UDP:  no connection establishment simple: no connection state at sender, receiver small segment header no connection control: UDP can blast away as fast as desired   often used for streaming multimedia apps other UDP uses (DNS, SNMP(Simple network manager protocol) 网络管理资料收集) reliable transfer over UDP: add reliability at application layer(application-specific error recovery)   UDP checksum  Goal: detect \u0026ldquo;errors\u0026rdquo; in transmitted segment sender:  treat segment contents as sequence of 16-bit integers checksum: addition (1\u0026rsquo;s complement sum) of segment contents sender puts checksum value into UDP checksum fields   Receiver:  compute checksum of received segment check if computed checksum equals checksum field value (No, error-detected, Yes, no error-detected)    Principles of Reliable data transfer  top-10 list of important networking topics  characteristics of unreliable channel will determine complexity of reliable data transfer protocol rdt_send(): called from above, passed data to deliver to receiver upper layer udt_send(): called by rdt, to transfer packet over unreliable channel to receiver rdt_rcv(): called when packet arrives on rcv-side of channel deliver_data() called by rdt to deliver data to upper  Reliable data transfer  incrementally develop sender, receiver sides of reliable data transfer protocol(rdt) consider only unidirectional data transfer(but control info will flow on both directions) use finite state machines(FSM) to specify sender, receiver  Rdt1.0: reliable transfer over a reliable channel  underlying channel perfectly reliable  no bit errors no loss of packets   separate FSMs for sender, receiver  sender sends data into underlying channel receiver read data from underlying channel     Rdt2.0: channel with bit errors  underlying channel may flip bits in pocket: checksum to detect bit errors how to recover from errors:  acknowledgements(ACKs) receiver explicitly tells sender that packet received OK negative acknowledgements(NAKs) receiver explicitly tells sender that packet had errors sender retransmits packet on receipt of NAK   new mechanisms in rdt2.0  error detection receiver feedback: control msgs(ACK, NAK)    rdt2.0 has a fatal flow(if ACK/NAK corrupted?)  sender doesn\u0026rsquo;t know what happened at receiver can\u0026rsquo;t just retransmit: possible duplicate   Handling duplicates:  sender retransmits current packet if ACK/NAK corrupted sender adds sequence number to each packet receiver discards (doesn\u0026rsquo;t deliver up) duplicate packet   Stop and wait (sender sends one packet, then waits for receiver response)  Rdt2.1: sender, handles garbled ACK/NAKs  Sender  Receiver   Rdt2.2: a NAK-free protocol  same functionality as rdt2.1, using ACKS only instead of NAK, receiver sends ACK for last packet received OK, receiver must explicitly include sequence number of packet being ACKed duplicate ACK at sender results in same action as NAK: retransmit current packet  Rdt3.0: channels with errors and loss  New assumption:  underlying channel can also pockets(data or ACKs) checksum, sequence number, ACKs, retransmission will be of help, but not enough   Approach: sender waits \u0026ldquo;reasonable\u0026rdquo; amount of time for ACK retransmits if no ACK received in this time if packet (or ACK) just delayed (not lost):  retransmission will be duplicate, but use of sequence number receiver must specify sequence number of packet being ACKed   requires countdown timer  performance of rdt3.0 1Gbps link, 15 ms prop. delay, 8000 bit packet:  sender time $$ d = L/R = \\frac{8000 bits}{10^9bps} = 8 ms$$ utilization - fraction of time sender busy sending $$ \\frac{L/R}{RTT + L/R} = 0.008/30.0008 = 0.00027$$   stop and wait operation   Pipelined protocols  sender allows multiple, \u0026ldquo;in-flight\u0026rdquo; yet-to-be-acknowledged` packets  range of sequence numbers must be increased buffering at sender and/or receiver    Two generic forms of pipelined protocols: go-Back-N, selective repeat  Go-back-N: big picture  sender can have up to N unacked packets in pipeline Receiver only sends cumulative ACKs, doesn\u0026rsquo;t ACK packet if there\u0026rsquo;s gap Sender has timer for oldest unacked packet, if timer expires, retransmit all unlocked packets  ACK(n): ACKs all packet up to including sequence number n \u0026ndash; \u0026ldquo;cumulative ACK\u0026rdquo; (may receive duplicate ACKs) timer for each in-flight packet timeout(n): retransmit packet n and all higher sequence number packets in window  sender  receiver  example   Selective Repeat: big picture  Sender can have up to N unacked packets in pipeline Receiver ACKs individual packets Sender maintains timer for each unacked packet, when timer expires, retransmit only unacked packet  Sender  if next available sequence number in window, send packet timeout(n): resend packet n, restart timer ACK(n) in (sendbase, sendbase + N): mark packet n as reader, if n smallest unACked packet, advance window base to next unACKed sequence number   receiver  send ACK(N) out-of-order : buffer in-order: deliver, advance window to next not-yet-received packet if packet n in [revbase-revbase+N], ACK(n), otherwise ignore    problem window size must be less than or equal t0 half the size of the sequence number space  Connection-Oriented Transport: TCP Overview  Point-to-Point reliable: in-order byte stream: no \u0026ldquo;message boundaries\u0026rdquo; pipelined: TCP congestion and flow control set window size send \u0026amp; receive buffer flow control: sender will not overwhelm receiver full duplex data: bi-directional data flow in some connection, maximum segment size(MSS， 控制流量，控制传输速度) connection-oriented: handshaking (Exchange of control messages) TCP segment structure  Sequence number: byte stream \u0026ldquo;number\u0026rdquo; of first byte in segment\u0026rsquo;s data ACKs: sequence number of next byte expected from other side, cumulative ACK   Round-Trip Time Estimation and Timeout  how receiver handles out-of-order segments: TCP spec doesn\u0026rsquo;t say, up to implementor how to set TCP timeout value?  longer than RTT(Round Trip Time) too short: premature timeout (unnecessary retransmission) too long: slow reaction to segment loss   how to estimate RTT?  SampleRTT: measured time from segment transmission until ACT receipt (ignore retransmissions) SampleRTT will vary, want estimated RTT \u0026ldquo;smoother\u0026rdquo;, average several recent measurement, not just current SampleRTT   EstimatedRTT = $(1-\\alpha)\\cdot \\text{EstimatedRTT} + \\alpha \\cdot \\text{SampleRTT} \\text{ typically}, \\alpha$ is 0.125 estimate of how much SampleRTT deviates from EstimatedRTT $DevRTT = (1-\\beta) \\cdot \\text{DevRTT} \\cdot \\beta \\cdot |\\text{SampleRTT}-\\text{Estimated RTT}|$, typically, $\\beta = 0.25$ set timeout interval TimeoutInterval = EstimatedRTT + 4 * DevRTT  TCP sender events  data received from application:  create segment with sequence number sequence number is a byte-stream number of first data byte in segment start timer if not already running (think of timer as for oldest unacked segment) expiration interval: timeout Interval   timeout: retransmit segment that caused timeout, restart timer ACK received: if acknowledges previously unacked segments, update what is known to be acked, start timer if there are outstanding segments figure:  TCP retransmission Scenarios   TCP ARK Generation Fast retransmit  Time-out period often relatively long (long delay before resending lost packet) detect lost segment via duplicate ACKs (If segment is lost, there will likely be many duplicate ACKs) if Sender receives ACKs for the same data, it supposes that segment after ACKed data was lost fast retransmit: resend segment before timer expires  Algorithm   Flow Control  Sender won\u0026rsquo;t overflow receiver\u0026rsquo;s buffer by transmitting too much or too fast Receive side of TCP connection has a receive buffer  Suppose TCP receiver discards out-of-order segments Receiver advertises spare room by including value of RcvWindow in segments Sender limits unACKed data to RcvWindow rwnd(receive window) = RcvBuffer - [LastByteRcvd - LastByteRead]  TCP Connection Management  Three way handshake  Client host sends TCP SYN segment to server, specifies initial sequence number (no data) Server host receivers SYN, replies with SYNACK segment, server allocates buffers, specifies server initial sequence number client receives SYNACK, replies with ACK segment, which may contain data    Closing a connection  client end system send TCP FIN control segment to server server receives FIN, replies with ACK, closes connection, sends FIN client receives FIN, replies with ACK, enters \u0026ldquo;timed wait\u0026rdquo; will respond with ACK to received FINs server, receives ACK, connection closed    Figure   TCP congestion control Principles of Congestion Control  too many sources sending too much data too fast for network to handle manifestations: lost packets, long delays a top-10 problem Cause/costs of congestion: scenario 1, two connections sharing a single hop with infinite buffers, maximum achievable throughput, large delays when congested  scenario 2, one router, finite buffers, and retransmission  always: $\\lambda_{in} = \\lambda_{out}$ \u0026ldquo;perfect\u0026rdquo; retransmission only when loss: $\\lambda_{in}\u0026rsquo; = \\lambda_{out}$ retransmission of delayed (not lost) packet makes $\\lambda_{in}\u0026lsquo;$ larger (than perfect case) for same $\\lambda_{out}$   Scenario 3, four senders, multi-hop paths, timeout/retransmit  even more crowded when packet dropped, any upstream transmission capacity used for that packet was wasted (之前走过的路径全部浪费掉)    Approaches towards congestion control  End-end congestion control  no explicit feedback from network congestion inferred from end-system observed loss, delay approach taken by TCP   Network-assisted congestion control:  routers provide feedback to end system single bit indicating congestion (SNA, TCP/IP ECN, ATM) explicit rate sender should send   CASE: ATM (ABR congestion control)  ABR: available bit rate, elastic service  if sender\u0026rsquo;s path \u0026ldquo;underloaded\u0026rdquo;, sender should use available bandwidth if sender\u0026rsquo;s path congested: sender throttled to minimum guaranteed rate   RM (resource management) cells:  sent by sender, interspersed with data cells bit in RM cell set by switches: NI bit: no increase in rate; CI bit: congestion indication RM cells returned to sender by receiver, with bits intact    EFCI bit in data cells: set to 1 in congested switch, if data cell preceding RM cell has EFCI set, receiver sets CI bit in returned RM cell    TCP congestion control  Congestion window(cwnd): imposes a constraint on the rate at which a TCP sender can send traffic into the network, LastByteSent – LastByteAcked \u0026lt;= cwnd MSS: maximum segment size: the maximum amount of data that can be grabbed and placed in a segment roughly rate = CongWin / RTT (Bytes/sec) How does sender perceive congestion?  loss event = timeout or 3 duplicate ACKs TCP sender reduces rate (cwnd) after loss event   three mechanisms:  AIMD slow start conservative after timeout events    AIMD  Approach: increase transmission rate (window size), probing for usable bandwidth, until loss occurs Additive increase: increase CongWin by 1 MSS every RTT until loss detected multiplicative decrease: cut CongWin in half after loss   Slow start  when connections begins, CongWIn = 1MSS available bandwidth may be \u0026raquo; MSS/RTT when connection begins, increase rate exponentially fast until first loss event  double congwin every RTT done by incrementing congwin for every ACK received    inferring loss  After 3 duplicate ACKs, cwnd is cut in half, window the grows linearly after timeout event, cwnd set to 1 MSS, window then grows exponentially to a threshold, then grows linearly     TCP Summary  TCP throughput: Let W be the window size when loss occurs, when window is W, throughput is W/RTT, just after loss, windows drops to W/2, throughput is W/(2RTT), average throughput 0.75W/RTT TCP future: TCP over \u0026ldquo;long, fat pipes\u0026rdquo; 1500 byte segments, 100ms RTT, want 10Gbps throughputs, requires window size W = 83.333 in flight ($W \\cdot 1500 \\cdot 8 / 100ms = 10Gbps$), throughput in terms of loss rate: $1.22\\cdot MSS/(RTT\\sqrt{L})$ , $L = 2\\cdot 10^{-10}$ TCP Fairness, if K TCP sessions share same bottleneck link of bandwidth R, each should have average rate of R/K, the TCP is fair   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"001142840c0377c3efc57ca5483dfda3","permalink":"/courses/computer_network/transport_layer/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/computer_network/transport_layer/","section":"courses","summary":"Transport services and protocols provide logical communication between app processes running on different hosts transport protocols run in end systems send side: breaks app message into segments, passes to network","tags":null,"title":"Transport Layer","type":"docs"},{"authors":null,"categories":null,"content":"目标检测是计算机视觉领域中一个新兴的应用方向，其任务是对输入图像进行分类的同时，检测图像中是否包含某些目标，并对他们准确定位并标识。\n目标定位 定位分类问题不仅要求判断出图片中物体的种类，还要在图片中标记出它的具体位置，用**边框（Bounding Box，或者称包围盒）**把物体圈起来。一般来说，定位分类问题通常只有一个较大的对象位于图片中间位置；而在目标检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。\n为了定位图片中汽车的位置，可以让神经网络多输出 4 个数字，标记为 $b_x$、$b_y$、$b_h$、$b_w$。将图片左上角标记为 (0, 0)，右下角标记为 (1, 1)，则有：\n 红色方框的中心点：($b_x$，$b_y$) 边界框的高度：$b_h$ 边界框的宽度：$b_w$  因此，训练集不仅包含对象分类标签，还包含表示边界框的四个数字。定义目标标签 Y 如下：\n$$\\left[\\begin{matrix}P_c\\\\ b_x\\\\ b_y\\\\ b_h\\\\ b_w\\\\ c_1\\\\ c_2\\\\ c_3\\end{matrix}\\right]$$\n则有：\n$$P_c=1, Y = \\left[\\begin{matrix}1\\\\ b_x\\\\ b_y\\\\ b_h\\\\ b_w\\\\ c_1\\\\ c_2\\\\ c_3\\end{matrix}\\right] $$\n其中，$c_n$表示存在第 $n$个种类的概率；如果 $P_c=0$，表示没有检测到目标，则输出标签后面的 7 个参数都是无效的，可以忽略（用 ? 来表示）。\n$$P_c=0, Y = \\left[\\begin{matrix}0\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\end{matrix}\\right]$$\n损失函数可以表示为 $L(\\hat y, y)$，如果使用平方误差形式，对于不同的 $P_c$有不同的损失函数（注意下标 $i$指标签的第 $i$个值）：\n  $P_c=1$，即$y_1=1$：\n$L(\\hat y,y)=(\\hat y_1-y_1)^2+(\\hat y_2-y_2)^2+\\cdots+(\\hat y_8-y_8)^2$\n  $P_c=0$，即$y_1=0$：\n$L(\\hat y,y)=(\\hat y_1-y_1)^2$\n  除了使用平方误差，也可以使用逻辑回归损失函数，类标签 $c_1,c_2,c_3$ 也可以通过 softmax 输出。相比较而言，平方误差已经能够取得比较好的效果。\n特征点检测 神经网络可以像标识目标的中心点位置那样，通过输出图片上的特征点，来实现对目标特征的识别。在标签中，这些特征点以多个二维坐标的形式表示。\n通过检测人脸特征点可以进行情绪分类与判断，或者应用于 AR 领域等等。也可以透过检测姿态特征点来进行人体姿态检测。\n目标检测 想要实现目标检测，可以采用**基于滑动窗口的目标检测（Sliding Windows Detection）**算法。该算法的步骤如下：\n 训练集上搜集相应的各种目标图片和非目标图片，样本图片要求尺寸较小，相应目标居于图片中心位置并基本占据整张图片。 使用训练集构建 CNN 模型，使得模型有较高的识别率。 选择大小适宜的窗口与合适的固定步幅，对测试图片进行从左到右、从上倒下的滑动遍历。每个窗口区域使用已经训练好的 CNN 模型进行识别判断。 可以选择更大的窗口，然后重复第三步的操作。  滑动窗口目标检测的优点是原理简单，且不需要人为选定目标区域；缺点是需要人为直观设定滑动窗口的大小和步幅。滑动窗口过小或过大，步幅过大均会降低目标检测的正确率。另外，每次滑动都要进行一次 CNN 网络计算，如果滑动窗口和步幅较小，计算成本往往很大。\n所以，滑动窗口目标检测算法虽然简单，但是性能不佳，效率较低。\n基于卷积的滑动窗口实现 相比从较大图片多次截取，在卷积层上应用滑动窗口目标检测算法可以提高运行速度。所要做的仅是将全连接层换成卷积层，即使用与上一层尺寸一致的滤波器进行卷积运算。\n如图，对于 16x16x3 的图片，步长为 2，CNN 网络得到的输出层为 2x2x4。其中，2x2 表示共有 4 个窗口结果。对于更复杂的 28x28x3 的图片，得到的输出层为 8x8x4，共 64 个窗口结果。最大池化层的宽高和步长相等。\n运行速度提高的原理：在滑动窗口的过程中，需要重复进行 CNN 正向计算。因此，不需要将输入图片分割成多个子集，分别执行向前传播，而是将它们作为一张图片输入给卷积网络进行一次 CNN 正向计算。这样，公共区域的计算可以共享，以降低运算成本。\n相关论文：\rSermanet et al., 2014. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks\n边框预测 在上述算法中，边框的位置可能无法完美覆盖目标，或者大小不合适，或者最准确的边框并非正方形，而是长方形。\nYOLO（You Only Look Once）算法可以用于得到更精确的边框。YOLO 算法将原始图片划分为 n×n 网格，并将\r目标定位一节中提到的图像分类和目标定位算法，逐一应用在每个网格中，每个网格都有标签如：\n$$\\left[\\begin{matrix}P_c\\\\ b_x\\\\ b_y\\\\ b_h\\\\ b_w\\\\ c_1\\\\ c_2\\\\ c_3\\end{matrix}\\right]$$\n若某个目标的中点落在某个网格，则该网格负责检测该对象。\n如上面的示例中，如果将输入的图片划分为 3×3 的网格、需要检测的目标有 3 类，则每一网格部分图片的标签会是一个 8 维的列矩阵，最终输出的就是大小为 3×3×8 的结果。要得到这个结果，就要训练一个输入大小为 100×100×3，输出大小为 3×3×8 的 CNN。在实践中，可能使用更为精细的 19×19 网格，则两个目标的中点在同一个网格的概率更小。\nYOLO 算法的优点：\n 和图像分类和目标定位算法类似，显式输出边框坐标和大小，不会受到滑窗分类器的步长大小限制。 仍然只进行一次 CNN 正向计算，效率很高，甚至可以达到实时识别。  如何编码边框 $b_x$、$b_y$、$b_h$、$b_w$？YOLO 算法设 $b_x$、$b_y$、$b_h$、$b_w$ 的值是相对于网格长的比例。则 $b_x$、$b_y$ 在 0 到 1 之间，而 $b_h$、$b_w$ 可以大于 1。当然，也有其他参数化的形式，且效果可能更好。这里只是给出一个通用的表示方法。\n相关论文：\rRedmon et al., 2015. You Only Look Once: Unified, Real-Time Object Detection。Ng 认为该论文较难理解。\n交互比 **交互比（IoU, Intersection Over Union）**函数用于评价对象检测算法，它计算预测边框和实际边框交集（I）与并集（U）之比：\n$$IoU = \\frac{I}{U}$$\nIoU 的值在 0～1 之间，且越接近 1 表示目标的定位越准确。IoU 大于等于 0.5 时，一般可以认为预测边框是正确的，当然也可以更加严格地要求一个更高的阈值。\n非极大值抑制 YOLO 算法中，可能有很多网格检测到同一目标。**非极大值抑制（Non-max Suppression）**会通过清理检测结果，找到每个目标中点所位于的网格，确保算法对每个目标只检测一次。\n进行非极大值抑制的步骤如下：\n 将包含目标中心坐标的可信度 $P_c$ 小于阈值（例如 0.6）的网格丢弃； 选取拥有最大 $P_c$ 的网格； 分别计算该网格和其他所有网格的 IoU，将 IoU 超过预设阈值的网格丢弃； 重复第 2~3 步，直到不存在未处理的网格。  上述步骤适用于单类别目标检测。进行多个类别目标检测时，对于每个类别，应该单独做一次非极大值抑制。\nAnchor Boxes 到目前为止，我们讨论的情况都是一个网格只检测一个对象。如果要将算法运用在多目标检测上，需要用到 Anchor Boxes。一个网格的标签中将包含多个 Anchor Box，相当于存在多个用以标识不同目标的边框。\n在上图示例中，我们希望同时检测人和汽车。因此，每个网格的的标签中含有两个 Anchor Box。输出的标签结果大小从 3×3×8 变为 3×3×16。若两个 $P_c$ 都大于预设阈值，则说明检测到了两个目标。\n在单目标检测中，图像中的目标被分配给了包含该目标中点的那个网格；引入 Anchor Box 进行多目标检测时，图像中的目标则被分配到了包含该目标中点的那个网格以及具有最高 IoU 值的该网格的 Anchor Box。\nAnchor Boxes 也有局限性，对于同一网格有三个及以上目标，或者两个目标的 Anchor Box 高度重合的情况处理不好。\nAnchor Box 的形状一般通过人工选取。高级一点的方法是用 k-means 将两类对象形状聚类，选择最具代表性的 Anchor Box。\n如果对以上内容不是很理解，在“3.9 YOLO 算法”一节中视频的第 5 分钟，有一个更为直观的示例。\nR-CNN 前面介绍的滑动窗口目标检测算法对一些明显没有目标的区域也进行了扫描，这降低了算法的运行效率。为了解决这个问题，R-CNN（Region CNN，带区域的 CNN）被提出。通过对输入图片运行图像分割算法，在不同的色块上找出候选区域（Region Proposal），就只需要在这些区域上运行分类器。\nR-CNN 的缺点是运行速度很慢，所以有一系列后续研究工作改进。例如 Fast R-CNN（与基于卷积的滑动窗口实现相似，但得到候选区域的聚类步骤依然很慢）、Faster R-CNN（使用卷积对图片进行分割）。不过大多数时候还是比 YOLO 算法慢。\n相关论文：\n R-CNN：\rGirshik et al., 2013. Rich feature hierarchies for accurate object detection and semantic segmentation Fast R-CNN：\rGirshik, 2015. Fast R-CNN Faster R-CNN：\rRen et al., 2016. Faster R-CNN: Towards real-time object detection with region proposal networks  ","date":1578092400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597196259,"objectID":"235603e13279f9036ba4c23a428fdc52","permalink":"/courses/deep_learning/object_detect/","publishdate":"2020-01-04T00:00:00+01:00","relpermalink":"/courses/deep_learning/object_detect/","section":"courses","summary":"目标检测是计算机视觉领域中一个新兴的应用方向，其任务是对输入","tags":null,"title":"目标检测","type":"docs"},{"authors":null,"categories":null,"content":"Introduction  transport segment from sending to receiving host on sending side encapsulates segments into datagrams on receiving side, delivers segments to transport layer network layer protocols in every host, router router examines header fields in all IP datagrams passing through it  Two keys  Forwarding: move packets from router\u0026rsquo;s input to appropriate router output  Routing: determine route taken by packets from source to destination, routing algorithm  Connection setup  3rd important function in some network architectures: ATM, frame relay, X.25 before datagrams flow, two end hosts and intervening routers establish virtual connection (routers get involved) network vs transport layer connection service:  network: between two hosts(may also involve intervening routers in case of VCs) transport: between two processes    Network Service model  example services for individual datagrams: guaranteed delivery, guaranteed delivery with less than 40 milliseconds delay example services for a flow of datagrams: in order datagram delivery, guaranteed minimum bandwidth to flow, restrictions on change in inter-packet spacing (封包和封包的间隔的变化(时间差), 变异度) 建连线有上面的这些机制  Virtual circuit and datagram networks connection and connection-less service  datagram network provides network-layer connectionless service VC network provides network-layer connection service analogous to transport-layer services, service: host-to-host; no choice: network providers one or the other(只能选一种); implementation: in network core  Virtual circuits  source-to-dest path behaves like telephone circuit (performance-wise, network actions along source-to-dest path) call setup, teardown for each call before data can flow each packet carries VC identifier (not destination host address) every router on source-dest path maintains \u0026ldquo;state\u0026rdquo; for each passing connection link, router resources(bandwidth, buffers) may be allocated to VC (dedicated resources = predictable service)   VC implementation  VC consists of  path from source to destination VC numbers, one number for each link along path entries in forwarding tables in routers along path   packet belonging to VC carries VC number(rather than destination address) VC number can be changed on each link (New VC number comes from forwarding table)   Datagram Networks  no call setup at network layer routers: no state about end-to-end connections, no network-level concept of \u0026ldquo;connection\u0026rdquo; packets forwarded using destination host address, packets between some source-destination pair may take different paths  forwarding table  longest prefix matching     Datagram or VC network: Why  Internet (datagram)  data exchange among computers, elastic service, no strict timing request smart end systems (computer), can adapt, perform control error recovery, simple inside network, complexity at \u0026ldquo;edge\u0026rdquo; many link types, different characteristics, uniform service difficult   ATM (VC)  evolved from telephony human conversation, strict timing, reliability requirements, need for guaranteed service \u0026ldquo;dumb\u0026rdquo; end systems (telephones, complexity inside network)    What\u0026rsquo;s inside a router - routing algorithms / protocol (RIP, OSPF, BGP)\r- forwarding algorithms from incoming to outgoing link\rInput Port Functions  given datagram destination, lookup output port using forwarding table in input port memory goal: complete input port processing at line speed queuing: if datagrams arrive faster than forwarding rate into switch fabric  Switching fabrics  switching via a bus:  datagram from input port memory to output port memory via a shared bus bus connection: switching speed limited by bus bandwidth 32 Gbps bus, Cisco 5600: sufficient speed for access and enterprise routers   switching through a crossbar switching via An interconnection network  overcome bus bandwidth limitations Banyan networks, other interconnection nets initially developed to connect processors in multiprocessor advanced design: fragmenting datagram into fixed length cell    Output ports  buffering: required when datagrams arrive from fabric faster than the transmission rate scheduling discipline: chooses among queued datagrams for transmission  queueing (delay) and loss due to output port buffer overflow How much buffering  RFC 3439 rule of thumb: average buffering equal to \u0026ldquo;typical\u0026rdquo; RTT (250 millisecond) times link capacity C (C = 10 Gpbs link, 2.5 Gbit buffer) Recent recommendation: with N flows, buffering equal to $RTT*C/(\\sqrt(N))$   Input port Queuing problem  HOL (head-of-the-line) blocking    IP: Internet Protocol IP datagram format  16-bit identifier, flags, 13-bit fragmentation offset: fragmentation/reassembly time to live: max number remaining hops (decremented at each router) upper layer: upper layer protocol to deliver header checksum  IP Datagram Fragmentation IPv4 addressing  IP address: 32-bit identifier for host, router interface interface: connection between host/router and physical link  router\u0026rsquo;s typically have multiple interfaces host typically has one interface ip address associated with each interface    Subnets  subnet part (high order bits) host part (low order bits) device interfaces with same subnet part of IP address can physically reach other without intervening router  to determine the subnets, detach each interface from its host or router, creating islands of isolated networks, each isolated network is called a subnet six subnets  CIDR: Classless Inter Domain Routing  subnet portion of address of arbitrary length address format: a.b.c.d/x where x is number bits in subnet portion of address   Get IP  hard-coded by system admin in a file, unix: /etc/rc.config DHCP: Dynamic Host Configuration Protocol: dynamically get address from as server, \u0026ldquo;plug-and-play\u0026rdquo;   Get subnet part of IP address: Gets allocated portion of its provider ISP\u0026rsquo;s address space ISP get block of address? ICANN (Internet Corporation for Assigned Names and Numbers)  DHCP: Dynamic Host Configuration Protocol  Goal: allow host to dynamically obtain its IP address from network server when it joins network, can renew its lease on address in use, allows reuse of address (only hold address while connected an \u0026ldquo;on\u0026rdquo;), support for mobile users who want to join network (more shortly) DHCP overview:  host broadcasts \u0026ldquo;DHCP discover\u0026rdquo; message DHCP server respond with \u0026ldquo;DHCP offer\u0026rdquo; message host requests IP address \u0026ldquo;DHCP request\u0026rdquo; message DHCP server sends address \u0026ldquo;DHCP ACK\u0026rdquo; message     Hierarchical addressing more specific route\rNetwork Address Translation (NAT)  Motivation: local network use just one IP address as far as outside world is concerned:  range of addresses not needed from ISP: just one IP address for all devices can change addresses of devices in local network without notifying outside world can change ISP without changing addresses of devices in local network devices inside local net not explicitly addressable, visible by outside world (a security plus)   Implementation: NAT router must:  outgoing datagrams: replace (source IP address, port number) of every outgoing datagram to (NAT IP address, new port number), remote clients/ servers will respond using (NAT IP address, )     16 bit port-number field (65536) NAT is controversial  routers should only process up to layer 3 (but involved layer 4) violates end-to-end argument, NAT possibility must be taken into account by app designers (eg. P2P applications) address shortage should instead be solved by IPv6   NAT traversal problem  client wants to connect to server with address 10.0.0.1, server address 10.0.0.1 local to LAN (client can\u0026rsquo;t use it as destination address), only one externally visible NAT address solution 1: statically configure NAT to forward incoming connection requests at given port to server (e.g. 123.76.29.7 port 2500 always forwarded to 10.0.0.1 port 2500) solution2: Universal Plug and Play (UPnP), Internet Gateway Device(IGD) protocol. Allows NATed host to: learn public IP address add /remove port mappings (with lease times) automate static NAT port map configuration solution3: relaying (used in Skype)  NATed client establishes connection to relay external client connects to relay relay bridges packets between connections      ICMP (Internet Control Message Protocol)  used by host \u0026amp; routers to communicate network-level information  error reporting unreachable host, network, port, protocol echo request/reply (used by ping)   network-layer above \u0026ldquo;IP\u0026rdquo;, ICMP messages carried in IP datagrams ICMP message: type, code plus first 8 bytes of IP datagram causing error  Traceroute and ICMP  source sends series of UDP segment to destination  first has TTL=1 second has TTL=2, etc unlikely port number   when nth datagram arrives to nth router  router discards datagram and sends to source an ICMP message (type 11, code 0) Message includes name of routers \u0026amp; IP address   when ICMP message arrives, source calculates RTT traceroute does this 3 times Stoping criterion  UDP segment eventually arrives at destination host destination returns ICMP \u0026ldquo;port unreachable\u0026rdquo; packet (type 3, code 3) when source gets this ICMP, stops    IPv6  Initial motivation: 32-bit address space soon to be completely allocated additional motivation:  header format helps speed processing/forwarding  header changes to facilitate QoS IPv6 datagram format: fixed-length 40 byte header, no fragmentation allowed      priority: identify priority among datagrams in flow flow label: identify datagrams in same \u0026ldquo;flow\u0026rdquo; next header: identify upper layer protocol for data checksum: removed entirely to reduce processing time at each step options: allowed, but outside of header, indicated by \u0026ldquo;Next Header\u0026rdquo; field ICMPv6: new version of ICMP, additional message types (\u0026ldquo;packet too big\u0026rdquo;), multicast group management functions  Transition From IPv4 to IPv6  not all routers can be upgraded simultaneous, no \u0026ldquo;flag days\u0026rdquo; operate mixed IPv4 and IPv6 routers: Tunneling, IPv6 carried as payload in IPv4 datagram among IPv4 routers   Routing Algorithm  Graph abstraction  Routing Algorithm: algorithm that finds least-cost path Global: all routers have complete topology, link cost info (link-state algorithm) Decentralized  router knows physically-connected neighbors, link costs to neighbors  iterative process of computation, exchange info with neighbors \u0026ldquo;distance vector\u0026rdquo; algorithms     static or dynamic?  static: routes change slowly over time dynamic: routes change more quickly (periodic update, in response to link cost changes)    Link-State Routing Algorithm Dijkstra\u0026rsquo;s algorithm (See Algorithm)\n Oscillations with congestion-sensitive routing (link cost = amount of carried traffic)   The Distance-Vector (DV) Routing Algorithm Distance Vector Algorithm (see Algorithm)\n Node x maintains distance vector Node x also maintains its neighbors\u0026rsquo; distance vectors Bellman-Ford equation (dynamic programming) $d_x(y) = min_v (c(x,v) + d_v(y))$ where min is taken over all neighbors v of x  good news travels fast, bad news travels slow  Figure b needs 44 iterations before algorithm stabilizes   Poisoned reverse  If Z routes through Y to get to X: Z tells Y its (Z\u0026rsquo;s) distance to X is infinite (So y won\u0026rsquo;t route to X via Z) loops involving three or more nodes (rather than simply two immediately neighboring nodes) will not be detected by the poisoned reverse technique    Comparison of LS and DV  Message complexity  LS: with n nodes, E links O(nE) message send DV: exchange between neighbors only, convergence time varies   Speed of Convergence  LS: $O(n^2)$ algorithm requires $O(nE)$ messages, may have oscillations DV: convergence time varies, may be routing loops, count-to-infinity   Robustness: what happens if router malfunctions?  LS: node can advertise incorrect link cost, each node computes only its own table DV node can advertise incorrect path cost, each node\u0026rsquo;s table used by others, error propagate through network    Hierarchical routing  scale: with 200 million destinations:  can\u0026rsquo;t store all destination\u0026rsquo;s in routing tables routing table exchange would swamp links   administrative autonomy  internet = network of networks each network admin may want to control routing in its own network   aggregate routers into regions \u0026ldquo;autonomous systems (AS)\u0026rdquo; routers in same AS run same routing protocol  \u0026ldquo;intra-AS\u0026rdquo; routing protocol routers in different AS can run different intra-AS running protocol   Gateway router : direct link to router in another AS  Inter-AS tasks, suppose router in AS1 receives datagram destined outside of AS1: router should forward packet to gateway router, but which one?  AS1 must learn which destination are reachable through AS2, which through AS3  propagate this reachability info to all routers in AS1      Hot-potato routing Suppose AS1 learns from inter-AS protocol that subnet X is reachable from AS3 and from AS2, to configure forwarding table, router 1d must determine towards which gateway it should forward packets for destination x.(hot-potato routing- send packet towards closest of two routers) Routing in the Internet Interior Gateway Protocols (IGP)\nRIP (Routing Information Protocol)  distance vector algorithm included in BSD-UNIX distribution in 1982 distance metric: number of hops (max = 15hops)  exchanged among neighbors every 30 seconds via Response Message (also called advertisement) each advertisement: list of up to 25 destination subnets within AS Link Failure and Recovery: if no advertisement heard after 180 seconds $\\rightarrow$ neighbor/link declared dead  routes via neighbor invalidated new advertisements send to neighbors neighbors in turn send out new advertisements link failure info quickly (?) propagates to entire network poison reverse: used to prevent ping-pong loops (infinite distance = 16 loops)   RIP table processing  RIP routing tables managed by application-level process called route-d (daemon) (Send distance vector) advertisements send in UDP packets, periodically repeated     OSPF (Open Shortest Path First)  open: publicly available uses Link State algorithm:  LS packet dissemination topology map at each node route computation using Dijkstra\u0026rsquo;s algorithm   OSPF advertisement carries one entry per neighbor router advertisements disseminated to entire AS (via flooding), carried in OSPF messages directly over IP (rather than TCP or UDP) advanced features  security: all OSPF messages authenticated(to prevent malicious intrusion) multiple same-cost paths allowed (only one path in RIP) For each link, multiple cost metrics for different TOS (type of service) (e.g. satellite link cost set \u0026ldquo;low: for best effort: high for real time) integrated uni- and multicast support: Multicast OSPF (MOSPF) uses same topology data base as OSPF hierarchical OSPF in large domains    BGP (Border Gateway Protocol) Inter-AS Routing  the de facto standard BGP provides each AS a means to:  Obtain subnet reachability information from neighboring ASs Propagate reachability information to all AS-internal routers Determine \u0026ldquo;good\u0026rdquo; routers to subnets based on reachability information and policy   allows subnet to advertise its existence to rest of Internet Pairs of routers (BGP peers) exchange routing info over semi-permanent TCP connections: BGP sessions (BGP sessions need not correspond to physical links)  When As2 advertise to a prefix to AS1:  AS2 promises it will forward datagrams towards that prefix AS2 can aggregate prefixes in its advertisement    Distributing reachability information:\n Using eBGP(external) session between 3a and 1c, AS3 sends prefix reachability info to AS1  1c can then use iBGP do distribute new prefix info to all routers in AS1 1b can then re-advertise new reachability info to AS2 over 1b-to-2a eBGP session   when router learns of new prefix, it creates entry for prefix in its forwarding table advertised prefix includes BGP attributes: prefix + \u0026ldquo;attributes\u0026rdquo; = \u0026ldquo;route\u0026rdquo; two important attributes  AS-PATH: contains ASs through which prefix advertisement has passed NEXT-HOP: indicates specific internal-AS router to next-hop AS(may be multiple links from current AS to next-hop-AS)   when gateway router receives route advertisement, use import policy to accept/decline  BGP route selection  router may learn about more than 1 route to some prefix, router must select route elimination rules:  local preference value attribute: policy decision shortest AS-PATH closest NEXT-HOP router: hot potato routing additional criteria    BGP messages exchanged using TCP  OPEN: open TCP connections to peer and authenticates sender UPDATE: advertises new path KEEPALIVE: keeps connection alive in absence of UPDATES NOTIFICATION: reports error in previous message also used to close connection  BGP routing policy  A, B, C are provider networks X, W, Y are customer (of provider networks) X is dual-homed: attached to two networks  X dose not want to route from B via X to C, so X will not advertise to B a route to C   A advertises path AW to B B advertises path BAW to X should B advertise path BAW to C?  No, B gets no \u0026ldquo;revenue\u0026rdquo; for routing CBAW since neither W nor C are B\u0026rsquo;s customers B wants to force C route to w via A B wants to route only to/from its customers    Different Intra- and Inter-AS routing  Policy  Inter-AS: admin wants control over how its traffic routed, who routes through its net Intra-AS: single admin, no policy   Scale: hierarchical routing saves table size, reduced update traffic Performance:  Intra-AS: can focus on performance Inter-AS: policy may dominate over performance    Broadcast and multicast routing Broadcast Routing  deliver packets from source to all other nodes source duplication is inefficient  In-network duplication:  flooding: when node receives broadcast packet, sends copy to all neighbors (problem: cycles \u0026amp; broadcast storm) controlled flooding: node only broadcast packet if it hasn\u0026rsquo;t broadcast same packet before, Node keeps track of packet ids already broadcasted; Reverse Path Forwarding(RPF): only forward packet if it arrived on shortest path between node and source spanning tree: no redundant packet received by any node    Spanning Tree - First construct a spanning tree\r- Nodes forward copies only along spanning tree\r- Creation:\r- center node\r- each node send unicast join message to center node\r- message forwarded until it arrives at a node already belonging to spanning tree\rMulticast Routing: Problem statement  find a tree (or trees) connecting routers having local multicast group members  tree: not all paths between routers used source-based: different tree from each sender to receivers shared-tree: same tree used by all group members   sourced-based tree: one tree per source, shorted path trees, reverse path forwarding group-shared tree: group uses one tree (minimal spanning tree, center-based trees)  Shortest Path tree Dijkstra\u0026rsquo;s algorithm\nReverse path forwarding Pruning: forwarding tree contains subtrees with no multicast group members: no need to forward datagrams down subtree, \u0026ldquo;prune\u0026rdquo; message send upstream by router with no downstream group members.\nShared-Tree: Steiner Tree  minimum cost tree connecting all routers with attached group members problem is NP-complete excellent heuristics exists (近似解) not used in practice  computational complexity information about entire network needed monolithic(庞大): rerun whenever a router needs to join/leave    Center-based trees  single delivery tree shared by all one router identified as \u0026ldquo;center\u0026rdquo; of tree to join:  edge router sends unicast join-message addressed to center router join-message \u0026ldquo;processed\u0026rdquo; by intermediate routers and forwarded towards center join-message either hits existing tree branch for this center, or arrives at center path taken by join-message becomes new branch of tree for this router    Internet Multicasting Routing: DVMRP  DVMRP: distance vector multicast routing protocol, RFC1075 flood and prune: reverse path forwarding, source-based tree soft state: DVMRP router periodically (1min) \u0026ldquo;forgets\u0026rdquo; branches are pruned  multicast data again flows down unpruned branch downstream router: re-prune or else continue to receive data   routers can quickly regraft to tree, following IGMP (internet Group Management protocol) join at leaf  Protocol Independent Multicast: RIP  not dependent on any specific underlying unicast routing algorithm (works with all) Dense: group members densely packed, in \u0026ldquo;close\u0026rdquo; proximity, bandwidth more plentiful  group membership by routers assumed until routers explicitly prune data-driven construction on multicast tree (e.g. RPF1) bandwidth and non-group-router processing profligate (浪费)   Sparse: number of networks with group members small w.r.t. number of interconnected networks, group members \u0026ldquo;widely dispersed\u0026rdquo;, bandwidth not plentiful  no membership until routers explicitly join receiver-driven construction of multicast tree (e.g. center-based) bandwidth and non-group-router processing conservative    ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"9692cd21063b5cd8e082d6c3f50ff309","permalink":"/courses/computer_network/network_layer/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/computer_network/network_layer/","section":"courses","summary":"Introduction transport segment from sending to receiving host on sending side encapsulates segments into datagrams on receiving side, delivers segments to transport layer network layer protocols in every host, router","tags":null,"title":"Network Layer","type":"docs"},{"authors":null,"categories":null,"content":"Background  We don\u0026rsquo;t want to run a program that is entirely in memory  Many code for handling unusual errors or conditions Certain program routines or features are rarely used The same library code used by many programs Arrays, lists and tables allocated but not used   Virtual memory — separation of user logical memory from physical memory  To run a extremely large process, logical address space can be much larger than the physical address space To increase CPU/resources utilization (higher degree of multiprogramming degree) To simplify programming tasks (Free programmer from memory limitation) To run programs faster (less I/O would be needed to load or swap)   Virtual memory can be implemented by  Demand paging Demand segmentation: more complicated due to variable size     Demand Paging  A program rather than the whole process is brought into memory only when it is needed  Less I/O needed $\\rightarrow$ Faster response Less memory needed $\\rightarrow$ More users   Page is needed when there is a reference to the page  Invalid reference $\\rightarrow$ abort Not-in-memory $\\rightarrow$ bring to memory via paging   Pure demand paging  Start a process with no page Never bring a page into memory until it is required   A swapper (midterm scheduler) manipulates the entire process, whereas a pager is concerned with individual page of a process Hardware support  Page table : a valid-invalid bit (1 $\\rightarrow$ page in memory, $0 \\rightarrow$ page no in the memory) Secondary memory (swap space, backing store), usually, a high-speed disk (swap device) is use     Page Fault First reference to a page will trap to OS (page fault trap)\n OS looks at the internal table (PCB) to decide  Invalid reference $\\rightarrow$ abort Just not in memory $\\rightarrow$ continue   Get an empty frame Swap the page from disk (swap) space into the frame Reset page table, invalid-valid bit = 1 Restart instruction   Page replacement If there is no free frame when a page fault occurs\n swap a frame to backing store swap a page from backing store into the frame different page replacement algorithms pick different frames for replacement  Demand Paging Performance  Effective Access Time (EAT): $(1-p) \\times ma + p \\times PFT$  P: page fault rate, ma: memory, access time, PFT: page fault time   Example ma=200ns, PFT=8ms  EAT = 200ns + 7999800 ns $\\times$ p   Access time is proportional to the page fault rate  If one access out of 1000 causes a page fault, then EAT = 8.2 ms $\\rightarrow$ slowdown by a factor of 40   Programs tend to have locality of reference Locality means program often accesses memory addresses that are close together  A single page fault can bring in 4KB memory content Greatly reduce the occurrence of page fault   Major components of page fault time (about 8ms)  serve the page-fault interrupt read in the page from disk (most expensive) Restart the process    Process Creation and Virtual memory  Demand paging: only bring in the page containing the first instruction Copy-on-Write: the parent and the child process share the same frames initially, and frame-copy when a page is written Memory-Mapped File: map a file into the virtual address space to bypass file system calls (e.g. read(), write())  Copy-on-Write  if either process modifies a frame, only then a frame is copied COW allows efficient process creation Free frames are allocated from a pool of zeroed-out frames, the content of a frame is erased to 0 Figure: a child process is forked  After parent modifies page C   Memory-Mapped Files  Approach:  MMF allows file I/O to be treated as routine memory access by mapping a disk block to memory frame A file is initially read using demand paging. Subsequent read/writes to/from the file are treated as ordinary memory accesses   Benefit:  Faster file access by using memory access rather than read() and write() system calls Allows several process to map the SAME file allowing the pages in memory to be SHARED   Concerns:  Security(access control), data lost, more programming efforts     Page replacement  when a page fault occurs with on free frame  swap out a process, freeing all its frames page replacement, find one not currently used add free it. Use dirty bit to reduce overhead of page transfers \u0026ndash; only modified pages are written to disk   Solve two major problems for demand paging  Frame-allocation algorithm, determine how many frames to be allocated to a process Page-replacement algorithm, select which frame to be replaced    Page Replacement Algorithms  Goal: lowest page-fault rate Evaluation: running against a string of memory references (reference string) and computing the number of page faults Reference String: 1,2,3,4,1,2,5,1,2,3,4,5  FIFO algorithm  The oldest page in a FIFO queue is replaced 3 frames (available memory frames = 3) 9 page faults  FIFO illustrating Belady\u0026rsquo;s Anomaly More allocated frames doesn\u0026rsquo;t guaranteed less page fault  Figure illustration   Optimal (Belady) Algorithm  Replace the page that will not be used for the longest period of time, need future knowledge 4 frames (6 page faults) In practice, we don\u0026rsquo;t have future knowledge, only used for reference and comparison   LRU Algorithm (Least Recently Used)  An approximation of optimal algorithm, looking backward, rather than forward It replaces the page that has not been used for the longest period of time It is often used, and is considered as quite good Counter implementation  page referenced: time stamp is copied into the counter replacement: remove the one with oldest counter, but linear search is required   Stack implementation  page referenced: move to top of the double-linked list replacement: remove the page at the bottom     Stack Algorithm  A property of algorithms Stack algorithm: the set of pagers in memory for n frames is always a subset of the set of pages that would be in memory with n+1 frames Stack algorithms do not suffers from Belady\u0026rsquo;s anomaly Both optimal algorithm and LRU algorithm stack algorithm  LRU approximation algorithms Few systems provide sufficient hardware support for the LRU page-replacement\n additional-reference-bits algorithm second-chance algorithm enhanced second-chance algorithm  Counting Algorithms  LFU algorithms (least frequently used)  keep a counter for each page idea: an actively used page should have a large reference count   MFU algorithms (most frequently used)  idea: the page with the smallest count was probably just brought in and has yet to be used    Both counting algorithms are not common\n implementation is expensive (overflow) do not approximate OPT algorithm very well  Allocation of frames  each process needs minimum number of frames Fixed allocation  Equal allocation \u0026ndash; 100 frames, 5 processes $\\rightarrow$ 20 frames/process Proportional allocation \u0026ndash; Allocate according to the size of the process   Priority allocation Using proportional allocation based on priority, instead of size. Local allocation: each process select from its own set of allocated frames Global allocation : process selects a replacement frame from the set of all frames  One process can take away a frame of another process e.g. Allow a high-priority process to take frames from a low-priority process Good system performance and thus is common used A minimum number of frames must be maintained for each process to prevent trashing    Trashing  If a process dost not have \u0026ldquo;enough\u0026rdquo; frames to support pages in active page $\\rightarrow$ very high paging activity A process is trashing if it spending more time paging than executing  Performance problem caused by trashing Processes queued for I/O to swap (page fault) $\\rightarrow$ low CPU utilization $\\rightarrow$ OS increased the degree of multiprogramming $\\rightarrow$ new processes take frames from old processes $\\rightarrow$ CPU utilization drops even further To prevent trashing, must provide enough frames for each process (Working-set model, page-fault frequency)  Working-Set Model (实现起来比较繁琐， 比较少用)  Locality: a set of pages that are actively used together Locality model: as a process executes, it moves from locality to locality (program structure, data structure) Working-set model  working-set window: a parameter $\\Delta$ working set: set of pages in most recent $\\Delta$ page reference (an approximation locality)    Explanation  $WSS_i:$ Working-set size for process i $D = \\sum WSS_i:$ Total demand frames If $D \u0026gt; m$ (available frames) $\\rightarrow$ trashing The OS monitors the $WSS_i$ of each process and allocates to the process enough frames: if $D \u0026laquo; m$ , increase degree of MP, if $D \u0026gt; m$, suspend a process.   Advantages: prevent thrashing while keeping the degree of multiprogramming as high as possible , optimize CPU utilization  Page Fault frequency scheme  page fault frequency directly measures and controls the page-fault rate to prevent trashing Establish upper and lower bounds on the desired page-fault rate of a process If page fault rate exceed the upper limit, allocate another frame to the process if page fault rate fails below the lower limit, remove a frame from the process   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"395bf14557c5cb3c46371bf6f570e3cd","permalink":"/courses/operating_system/virtual_memory/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/virtual_memory/","section":"courses","summary":"Background We don\u0026rsquo;t want to run a program that is entirely in memory Many code for handling unusual errors or conditions Certain program routines or features are rarely used The","tags":null,"title":"Virtual Memory","type":"docs"},{"authors":null,"categories":null,"content":"人脸识别 **人脸验证（Face Verification）和人脸识别（Face Recognition）**的区别：\n 人脸验证：一般指一个一对一问题，只需要验证输入的人脸图像是否与某个已知的身份信息对应； 人脸识别：一个更为复杂的一对多问题，需要验证输入的人脸图像是否与多个已知身份信息中的某一个匹配。  一般来说，由于需要匹配的身份信息更多导致错误率增加，人脸识别比人脸验证更难一些。\nOne-Shot 学习 人脸识别所面临的一个挑战是要求系统只采集某人的一个面部样本，就能快速准确地识别出这个人，即只用一个训练样本来获得准确的预测结果。这被称为One-Shot 学习。\n有一种方法是假设数据库中存有 N 个人的身份信息，对于每张输入图像，用 Softmax 输出 N+1 种标签，分别对应每个人以及都不是。然而这种方法的实际效果很差，因为过小的训练集不足以训练出一个稳健的神经网络；并且如果有新的身份信息入库，需要重新训练神经网络，不够灵活。\n因此，我们通过学习一个 Similarity 函数来实现 One-Shot 学习过程。Similarity 函数定义了输入的两幅图像的差异度，其公式如下：\n$$Similarity = d(img1, img2)$$\n可以设置一个超参数 $τ$ 作为阈值，作为判断两幅图片是否为同一个人的依据。\nSiamese 网络 实现 Similarity 函数的一种方式是使用Siamese 网络，它是一种对两个不同输入运行相同的卷积网络，然后对它们的结果进行比较的神经网络。\n如上图示例，将图片 $x^{(1)}$、$x^{(2)}$ 分别输入两个相同的卷积网络中，经过全连接层后不再进行 Softmax，而是得到特征向量 $f(x^{(1)})$、$f(x^{(2)})$。这时，Similarity 函数就被定义为两个特征向量之差的 L2 范数：\n$$d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||^2_2$$\n相关论文：\rTaigman et al., 2014, DeepFace closing the gap to human level performance\nTriplet 损失 Triplet 损失函数用于训练出合适的参数，以获得高质量的人脸图像编码。“Triplet”一词来源于训练这个神经网络需要大量包含 Anchor（靶目标）、Positive（正例）、Negative（反例）的图片组，其中 Anchor 和 Positive 需要是同一个人的人脸图像。\n对于这三张图片，应该有：\n$$||f(A) - f(P)||^2_2 + \\alpha \\le ||f(A) - f(N)||^2_2$$\n其中，$\\alpha$ 被称为间隔（margin），用于确保 $f()$ 不会总是输出零向量（或者一个恒定的值）。\nTriplet 损失函数的定义：\n$$L(A, P, N) = max(||f(A) - f(P)||^2_2 - ||f(A) - f(N)||^2_2 + \\alpha, 0)$$\n其中，因为 $||f(A) - f(P)||^2_2 - ||f(A) - f(N)||^2_2 + \\alpha$ 的值需要小于等于 0，因此取它和 0 的更大值。\n对于大小为 $m$ 的训练集，代价函数为：\n$$J = \\sum^m_{i=1}L(A^{(i)}, P^{(i)}, N^{(i)})$$\n通过梯度下降最小化代价函数。\n在选择训练样本时，随机选择容易使 Anchor 和 Positive 极为接近，而 Anchor 和 Negative 相差较大，以致训练出来的模型容易抓不到关键的区别。因此，最好的做法是人为增加 Anchor 和 Positive 的区别，缩小 Anchor 和 Negative 的区别，促使模型去学习不同人脸之间的关键差异。\n相关论文：\rSchroff et al., 2015, FaceNet: A unified embedding for face recognition and clustering\n二分类结构 除了 Triplet 损失函数，二分类结构也可用于学习参数以解决人脸识别问题。其做法是输入一对图片，将两个 Siamese 网络产生的特征向量输入至同一个 Sigmoid 单元，输出 1 则表示是识别为同一人，输出 0 则表示识别为不同的人。\nSigmoid 单元对应的表达式为：\n$$\\hat y = \\sigma (\\sum^K_{k=1}w_k|f(x^{(i)})_{k} - x^{(j)})_{k}| + b)$$\n其中，$w_k$ 和 $b$ 都是通过梯度下降算法迭代训练得到的参数。上述计算表达式也可以用另一种表达式代替：\n$$\\hat y = \\sigma (\\sum^K_{k=1}w_k \\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k} + b)$$\n其中，$\\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k}$ 被称为 $\\chi$ 方相似度。\n无论是对于使用 Triplet 损失函数的网络，还是二分类结构，为了减少计算量，可以提前计算好编码输出 $f(x)$ 并保存。这样就不必存储原始图片，并且每次进行人脸识别时只需要计算测试图片的编码输出。\n神经风格迁移 **神经风格迁移（Neural style transfer）**将参考风格图像的风格“迁移”到另外一张内容图像中，生成具有其特色的图像。\n深度卷积网络在学什么？ 想要理解如何实现神经风格转换，首先要理解在输入图像数据后，一个深度卷积网络从中都学到了些什么。我们借助可视化来做到这一点。\n我们通过遍历所有的训练样本，找出使该层激活函数输出最大的 9 块图像区域。可以看出，浅层的隐藏层通常检测出的是原始图像的边缘、颜色、阴影等简单信息。随着层数的增加，隐藏单元能捕捉的区域更大，学习到的特征也由从边缘到纹理再到具体物体，变得更加复杂。\n相关论文：\rZeiler and Fergus., 2013, Visualizing and understanding convolutional networks\n代价函数 神经风格迁移生成图片 G 的代价函数如下：\n$$J(G) = \\alpha \\cdot J_{content}(C, G) + \\beta \\cdot J_{style}(S, G)$$\n其中，$\\alpha$、$\\beta$ 是用于控制相似度比重的超参数。\n神经风格迁移的算法步骤如下：\n 随机生成图片 G 的所有像素点； 使用梯度下降算法使代价函数最小化，以不断修正 G 的所有像素点。  相关论文：\rGatys al., 2015. A neural algorithm of artistic style\n内容代价函数 上述代价函数包含一个内容代价部分和风格代价部分。我们先来讨论内容代价函数 $J_{content}(C, G)$，它表示内容图片 C 和生成图片 G 之间的相似度。\n$J_{content}(C, G)$ 的计算过程如下：\n 使用一个预训练好的 CNN（例如 VGG）； 选择一个隐藏层 $l$ 来计算内容代价。$l$ 太小则内容图片和生成图片像素级别相似，$l$ 太大则可能只有具体物体级别的相似。因此，$l$ 一般选一个中间层； 设 $a^{(C)[l]}$、$a^{(G)[l]}$ 为 C 和 G 在 $l$ 层的激活，则有：  $$J_{content}(C, G) = \\frac{1}{2}||(a^{(C)[l]} - a^{(G)[l]})||^2$$\n$a^{(C)[l]}$ 和 $a^{(G)[l]}$ 越相似，则 $J_{content}(C, G)$ 越小。\n风格代价函数 每个通道提取图片的特征不同，比如标为红色的通道提取的是图片的垂直纹理特征，标为黄色的通道提取的是图片的橙色背景特征。那么计算这两个通道的相关性，相关性的大小，即表示原始图片既包含了垂直纹理也包含了该橙色背景的可能性大小。通过 CNN，“风格”被定义为同一个隐藏层不同通道之间激活值的相关系数，因其反映了原始图片特征间的相互关系。\n对于风格图像 S，选定网络中的第 $l$ 层，则相关系数以一个 gram 矩阵的形式表示：\n$$G^{[l](S)}_{kk\u0026rsquo;} = \\sum^{n^{[l]}_H}_{i=1} \\sum^{n^{[l]}_W}_{j=1} a^{[l](S)}_{ijk} a^{[l](S)}_{ijk\u0026rsquo;}$$\n其中，$i$ 和 $j$ 为第 $l$ 层的高度和宽度；$k$ 和 $k'$ 为选定的通道，其范围为 $1$ 到 $n_C^{[l]}$；$a^{[l](S)}_{ijk}$ 为激活。\n同理，对于生成图像 G，有：\n$$G^{[l](G)}_{kk\u0026rsquo;} = \\sum^{n^{[l]}_H}_{i=1} \\sum^{n^{[l]}_W}_{j=1} a^{[l](G)}_{ijk} a^{[l](G)}_{ijk\u0026rsquo;}$$\n因此，第 $l$ 层的风格代价函数为：\n$$J^{[l]}_{style}(S, G) = \\frac{1}{(2n^{[l]}_Hn^{[l]}_Wn^{[l]}_C)^2} \\sum_k \\sum_{k\u0026rsquo;}(G^{[l](S)}_{kk\u0026rsquo;} - G^{[l](G)}_{kk\u0026rsquo;})^2$$\n如果对各层都使用风格代价函数，效果会更好。因此有：\n$$J_{style}(S, G) = \\sum_l \\lambda^{[l]} J^{[l]}_{style}(S, G)$$\n其中，$lambda$ 是用于设置不同层所占权重的超参数。\n推广至一维和三维 之前我们处理的都是二维图片，实际上卷积也可以延伸到一维和三维数据。我们举两个示例来说明。\nEKG 数据（心电图）是由时间序列对应的每个瞬间的电压组成，是一维数据。一般来说我们会用 RNN（循环神经网络）来处理，不过如果用卷积处理，则有：\n 输入时间序列维度：14 x 1 滤波器尺寸：5 x 1，滤波器个数：16 输出时间序列维度：10 x 16  而对于三维图片的示例，有\n 输入 3D 图片维度：14 x 14 x 14 x 1 滤波器尺寸：5 x 5 x 5 x 1，滤波器个数：16 输出 3D 图片维度：10 x 10 x 10 x 16  ","date":1578092400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597196259,"objectID":"f8042e584f73e3cf3f0846e119799af6","permalink":"/courses/deep_learning/transfer/","publishdate":"2020-01-04T00:00:00+01:00","relpermalink":"/courses/deep_learning/transfer/","section":"courses","summary":"人脸识别 **人脸验证（Face Verification）和人","tags":null,"title":"人脸识别和风格迁移","type":"docs"},{"authors":null,"categories":null,"content":" understand principles behind data link layer services:  error detection, correction sharing a broadcast channel, multiple access (无线网络) link layer addressing (48 bit) reliable data transfer, flow control   instantiation and implementation of various link layer technologies  Introduction  hosts and routers are nodes communication channels that connect adjacent nodes along communication path are links (wired links, wireless links, LANs) layer-2 packet is a frame, encapsulates datagram data-link layer has a responsibility of transferring datagram from one node to adjacent node over a link datagram transferred by different link protocols over different links each link protocol provides different services (may or may not provide rdt(reliable data service) over link)  Services  Framing, link access  encapsulate datagram into frame, adding header, trailer channel access if shared mediums \u0026ldquo;MAC\u0026rdquo; (media access control protocol) addresses used in frame headers to identify source, destination   reliable delivery between adjacent nodes  TCP seldom used on low bit-error link(fiber, some twisted pair) wireless links high error rates   flow control: pacing between adjacent sending and receiving nodes error detections:  error caused by noise receiver detects presence of errors   error correction: receiver identifies and corrects bit error(s) without resorting to retransmission half-duplex and full-duplex with half duplex, nodes at both ends of link can transmit, but not at same time  Where is link layer implemented  in each and every host link layer implemented in \u0026ldquo;adaptor\u0026rdquo; (network interface card)  ethernet card, PCMCI card, 802.11 card implement link, physical layer   attaches into host\u0026rsquo;s system buses combination of hardware, software, firmware sending side : encapsulates datagram in frame, address error checking bits, rdt, flow control, etc receiving side: look for errors, rdt, flow control etc, extracts datagram, passes to upper layer at receiving side  Error detection and error correction  EDC: Error detection and correction bits (redundancy) D: data protected by error checking, may include header files Error detection not 100% reliable  protocol may miss some errors, but rarely larger EDC field yields better detection and correction    Parity Checking  Single Bit Parity (Detect single bit errors)  Two dimensional Bit Parity (Detect and correct single bit errors)   Internet Checksum  detect \u0026ldquo;errors\u0026rdquo; (e.g. flipped bits) in transmitted packet (used at transport layer only) Sender:  treat segment contents as sequence of 16-bit integers checksum: addition (1\u0026rsquo;s complement sum) of segment contents sender puts checksum value into UDP checksum fields   Receiver:  compute checksum of received checksum check if computed checksum equals checksum field value    Cyclic Redundancy Check  view data bits D as a binary number choose r+1 bit pattern (generator) G goal: choose r CRC bits, R, such that  \u0026lt;D,R\u0026gt; exactly divisible by G receiver knows G, divides \u0026lt;D,R\u0026gt; by G can detect all burst errors less than r+1 bits   widely used in practice (Ethernet, 802.11 WIFI, ATM)   Multiple Access Links and Protocols  Two types of \u0026ldquo;links\u0026rdquo;  Point-to-point: PPP for dial-up access, point-to-point link between Ethernet switch and host   broadcast (shared wire or medium)  old-fashioned Ethernet upstream HFC (hybrid fiber coax) 802.11 wireless LAN    single shared broadcast channel two or more simultaneous transmissions by nodes: collision if node receives two or more signals at the same time multiple access protocol:  distributed algorithm that determines how nodes share channel, i.e. determine when node can transmit communication about channel sharing must use channel itself, no out-of-band channel for coordination    Ideal Multiple Access Protocol Broadcast channel of rate R bps\n when one node wants to transmit, it can send at rate R when M nodes want to transmit, each can send at average rate R?M fully decentralized  no special node to coordination transmissions no synchronization of clocks, slots   simple  MAC protocols: a taxonomy Channel Partitioning: TDMA/ FDMA  TDMA: time division multiple access  access to channel in \u0026ldquo;rounds\u0026rdquo; each station gets fixed length slot(length= packet transmission time) in each round unused slots go idle   FDMA: frequency division multiple access  channel spectrum divided into frequency bands each station assigned fixed frequency band unused transmission time in frequency bands go idle     Random access channel not divided, allow collisions, \u0026ldquo;recover\u0026rdquo; from collisions\n when node has packet to send, transmit at full channel data rate R two or more transmitting nodes $\\rightarrow$ collision random access MAC protocol specifies  how to detect collisions how to recover from collisions (e.g. via delayed retransmission)   examples of random access MAC protocols: slotted ALOHA, CSMA, CSMA/CD (ethernet), CSMA/CA  Slotted ALOHA - all frames same size\r- time divided into equal size slots (time to )\r Operation: when node obtains fresh frame, transmits in next slot  if no collision: if collision: node retransmits frame in each subsequent slot with pro   Cons:  collisions, wasting slots idle slots nodes may be able to detect collision in less than time to transmit pocket clock synchronization   Pros  single highly decentralized simple   Efficiency: long-run fraction of successful slots (many nodes, all with many frames to send)  suppose: N nodes with many frames to send, each transmits in slot with probability p prob that given node has success in a slot = $p(1-p)^{N-1}$ prob than any node has a success = $Np(1-p)^{N-1}$ for many nodes, max efficiency $p = 1/e = 0.37$    Pure(unslotted) ALOHA  unslotted Aloha: simpler, no synchronization when frame first arrives: transmit immediately collision probability increases, frame set at $t_0$ collides with other frames send in $[t_0-1, t_0+1]$  prob $p*(1-p)^{2(N-1)}$ for many nodes, max efficiency $p = 1/(2e) = 0.18$  CSMA (Carrier Sense Multiple Access) CSMA: listen before transmit, if channel sensed idle transmit entire frame, if channel sensed busy, defer transmission\n collision: entire packet transmission time wasted role of distance \u0026amp; propagation delay in determining collision probability  CSMA/CD (collision detection)  collision detection:  easy in wried LANS: measure signal strengths, compare transmitted, received signals difficult in wireless LANs; received signal strength overwhelmed by local transmission strength CSMA/CD used in Ethernet CSMA/CA (collision avoidance) used in 802.11    \u0026ldquo;Taking Turns\u0026rdquo; MAC protocols  channel partitioning MAC protocols:  share channel efficiently and fairly at high load inefficient at low load: delay in channel access, 1/N bandwidth allocated even if only 1 active node   Random access MAC protocols:  efficient at low load: single node can fully utilized channel high load: collision overhead   taking turns protocols: look for best of both worlds Polling  master node \u0026ldquo;invites\u0026rdquo; slave nodes to transmit in turn typically used with \u0026ldquo;dumb\u0026rdquo;   Token passing (Taking Turns):  control token passed from one node to next sequentially token message concerns: token overhead, latency, singe point of failure   example: Bluetooth, FDDI, IBM Token Ring  Link-Layer Addressing  32-bit IP address:  network-layer address used to get datagram to destination IP subnet   MAC (or LAN or physical or Ethernet) address:  function: get frame from one interface to another physically-connected interface (same network) 48 bit MAC address (for most LANS) burned in NIC ROM, also sometimes software settable   Each adapter on LAN has unique LAN address (FF-FF-FF-FF-FF-FF: broadcast address) mac address allocation administered by IEEE manufacturer buys portion of MAC address space (to assure uniqueness) MAC flat address $\\rightarrow$ portability, can move LAN card from one LAN to another IP hierarchical address not portable  ARP: address Resolution Protocol  how to determine MAC address of B knowing B\u0026rsquo;s IP address? Each IP node (host, router) on LAN has ARP table ARP table: IP/MAC address mappings for some LAN nodes, TTL(time to live): time after which address mapping will be forgotten (typically 20 minutes)  procedure  A wants to send datagram to B, and B\u0026rsquo;s MAC address not in A\u0026rsquo;s ARP table A broadcast ARP query packet, containing B\u0026rsquo;s IP address, all machines on LAN receive ARP query B receivers ARP packet, replies to A with its (B\u0026rsquo;s) MAC address (frames send to A\u0026rsquo;s MAC address(unicast)) A caches (saves) IP-to-MAC address pair in its ARP table until information becomes old (times out) ARP is \u0026ldquo;plug-and-play\u0026rdquo;, node creates their ARP tables without intervention form net administrator   example:   Ethernet  features:  cheap first widely used LAN technology simpler, cheaper than token LANs and ATM kept up with speed race: 10Mbps - 10 Gbps   bus topology popular through mid 90-s (all nodes in same collision domain, can collide with each other) today: star topology prevails  active, switch in center each \u0026ldquo;spoke\u0026rdquo; run a (separate) Ethernet protocol (nodes do not collide with each other)   Ethernet Frame structure  Preamble: 7 bytes with pattern 10101010 followed by one byte with pattern 10101011 used to synchronize receiver, sender clock rates Addresses: 6 bytes, if adapter receives frame with matching destination address, or with broadcast address (e.g. ARP packet), it passes data in frame to network layer protocol, otherwise, adapter discards frame Type: indicates higher layer protocol (mostly IP but others possible, e.g. Novell IPX, Apple Talk) CRC: checked at receiver, if error is detected, frame is dropped   Unreliable, connectionless  connectionless: No handshaking between sending and receiving NICs unreliable： receiving NIC(network interface card) doesn\u0026rsquo;t send ACKs or NAKs to sending NIC  stream of datagrams passed to network layer can have gaps(missing datagrams) gaps will be filled if app is using TCP, otherwise app will see gaps   Ethernet\u0026rsquo;s MAC protocol: unslotted CSMA/CD    Ethernet CSMA/CD algorithm  NIC receives datagram from network layer, creates frame IF NIC senses channel idle, starts frame transmission, IF NIC senses channel busy, waits until channel idle, then transmits If NIC transmits entire frame without detecting another transmission, NIC is done with frame If NIC detects another transmission while transmitting, aborts and sends jam signal After aborting, NIC enters exponential backoff: after mth collision, NIC chooses K at random from $(0, 1,2, \u0026hellip;, 2^m-1)$ NIC waits K*512 bit times, returns to Step2   Jam signal: make sure all other transmitters are aware of collision: 48 bits Bit time: 1 microsecond for 10 Mbps Ethernet, for K=1023,wait time is about 50 msec Exponential Backoff (最多连续16次)  Goal: adapt retransmission attempts to estimated current load (heavy load: random wait will be longer) first collision: choose K from (0,1), delay is K*512 bit transmission times after second collision: choose K from {0,1,2,3} after ten collisions: choose K from {0,1,2,3,4,\u0026hellip;,1023}   Efficiency (70%)  $T_{prop}$ = max prop delay between 2 nodes in LAN $T_{trans}$ = time to transmit max-size frame $$\\text{efficiency} = \\frac{1}{1+5t_{prop}/t_{trans}}$$ Efficiency goes to 1 as $t_{prop}$ goes to 0, as t_{trans}$ goes to infinity better performance than ALOHA: and simple, cheap, decentralized    802.3 Ethernet Standards: Link \u0026amp; Physical Layers  many different Ethernet standards  common MAC protocol and frame format different speeds: 2Mbps, 10Mbps, 100Mbps, 1Gbps, 10Gbps different physical layer media: fiber, cable    Manchester encoding  used in 10Base T each bit has a transition allows clocks in sending and receiving nodes to synchronize to each other, no need for a centralized, global clock among nodes    Link-Layer switches  Hubs  physical-layer (\u0026ldquo;dumb\u0026rdquo; repeaters, 只处理信号) bits coming in one link go out all other links at same rate all nodes connected to hub can collide with one another no frame buffering no CSMA/CD at hub: host NIC detect collisions   Switch: allow multiple simultaneous transmissions  link-layer device: smarter than hubs, take active role  store, forward Ethernet frames examine incoming frame\u0026rsquo;s MAC address, selectively forward frame to one-or-more outgoing links when frame is to be forwarded on segment, uses CSMA/CD to access segment   transparent, hosts are unaware of presence of switches plug-and-play, self-learning, switch do not need to be configured switch table: Each switch has a switch table, each entry (MAC address of host, interface to reach host, time stamp    Self-Learning  Switch table is initially empty For each incoming frame received on an interface, the switches stores in its table  The MAC address in the frame\u0026rsquo;s source address field the interface from which the frame arrived the current time   the switch deletes an address in the table if no frames are received with that address as the source address after some period of time  Filtering/forwarding  Filtering: whether a frame should be forwarded to some interface or should just be dropped Forwarding: determine the interfaces to which a frame should be directed, and then moves the frame to those interfaces suppose a frame with destination address DD-DD-DD-DD-DD-DD arrives at the switch on interface x  there is no entry in the table for DD-DD-DD-DD-DD-DD, the switch forwards copies of the frame to the output buffers preceding all interfaces (broadcast the frame) there is an entry in the table, associating DD-DD-DD-DD-DD-DD with interface x: there is no need to forward the frame to any of the other interfaces, switch performs filtering function by discarding the frame. (送和收在同一区域） there is an entry in table, associating DD-DD-DD-DD-DD-DD with interface $y \\neq x$, the frame needs to be forwarded to the LAN segment attached to interface y. Switch performs its forwarding function by putting the frame in an output buffer that precedes interface y.    Switches vs. Routers  both store-and-forward devices: routers are network layer devices, switches are link layer address Routers maintain routing tables, implement routing algorithms switches maintain switch tables, implement filtering, learning algorithms  PPP [RFC 1557] Point to Point Data Link Control\n one sender, one receiver, one link: easier than broadcast link no media Access control no need for explicit MAC addressing (e.g. dialup link) popular point-to-point DLC protocols: PPP, HDLC  PPP Frame  packet framing: encapsulation of network-layer datagram in data link frame  carry network layer data of any network layer protocol (not just IP) at same time ability to demultiplex upwards   bit transparency: must carry any bit pattern in the data field error detection (no correction) connection liveness: detect, signal link failure to network layer network layer address negotiation: endpoint can learn/configure each other\u0026rsquo;s network address data frame  Flag: delimiter (framing) Address: does nothing control: does nothing: protocol: upper layer protocol to which frame delivered (e.g. PPP-LCP, IP, IPCP etc) info: information check: cyclic redundancy check for error-detection   PPP non-requirements  no error correction no flow control out of order delivery no need to support multipoint links error recovery, flow control, data re-ordering all relegated to higher layers   Byte Stuffing  \u0026ldquo;data transparency\u0026rdquo; requirement: data field must be allowed to include flag pattern \u0026lt;01111110\u0026gt; Sender: adds extra \u0026lt;01111110\u0026gt; byte after each \u0026lt;01111110\u0026gt; data byte Receiver: two 01111110 bytes in a row: discard first byte, continue data reception; singe 01111110:flag byte   PPP data Control Protocol  before exchanging network-layer data, data link peers must configure PPP link (max frame length, authentication) learn / configure network layer information    ATM  Virtualization of networks  Layering of abstractions: don\u0026rsquo;t sweat the details of the lower layer, only deal with lower layers abstractly two layers of addressing: internetwork and local network new layer (IP) makes everything homogeneous at internetwork layer   Asynchronous Transfer Mode: ATM  Goal: integrated, end-end transport of carry voice, video, data meeting timing / QoS requirements of voice, video (versus Internet best-effort model) \u0026ldquo;next generation\u0026rdquo; telephony (下一代网络电话） packet-switching (fixed length packets, 53 bytes, called \u0026ldquo;cells\u0026rdquo;) using virtual circuits   For more information, see Computer Networking Website  Multi-protocol label switching (MPLS)  Goal: speed up IP forwarding by using fixed length label (instead of IP address) to do forwarding (20bits)  Borrowing ideas from Virtual Circuit (VC) approach but IP datagram still keeps IP address    MPLS-enhanced forwarding   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"5ff4a819c8f52a6f491cda30db245d14","permalink":"/courses/computer_network/link_layer/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/computer_network/link_layer/","section":"courses","summary":"understand principles behind data link layer services: error detection, correction sharing a broadcast channel, multiple access (无线网络) link layer addressing (48 bit) reliable data transfer,","tags":null,"title":"Link Layer","type":"docs"},{"authors":null,"categories":null,"content":"Threads  Threads is a light process, basic unit of CPU utilization All threads belonging to the same process share code section, data section, and OS resources (e.g. open files and signals) Each thread has its own (thread control block) thread ID, program counter, register set, and a stack  Example  web browser, once thread displays contents while the other thread receives data from network web server, one request(thread), better performance as code and resource sharing RPC server    Benefits of Multithreading  Responsiveness: allow a program to continue running even if part of it is blocked or is performing a lengthy operation Resource sharing: several different threads of activity all within the same address space Utilization of MP arch: Several thread may be running in parallel on different processors Economy: Allocating memory and resources for process creation is costly.    Multicore Programming  multithread programming provides a mechanism for more efficient use of multiple cores and improved concurrency (threads can run in parallel) Multicore systems putting pressure on system designers and application program (scheduling algorithms use cores to allow the parallel execution challenges in Multicore Programming  Dividing activities: divide program into concurrent tasks Data splitting: divide data accessed and manipulated by the tasks Data dependency: synchronize data access Balance: evenly distribute tasks to cores Testing and debugging    User vs. Kernel Threads  User thread \u0026ndash; thread management done by user level threads library (Pthreads, Java threads, Win32 threads) Kernel threads \u0026ndash; supported by the kernel(OS) directly (Windows 2000, Linux) User threads  thread library provides support for thread creation, scheduling and deletion Generally fast to create and manage If the kernel is single-threaded, a user-thread blocks $\\rightarrow$ entire process blocks even if other threads are ready to run   Kernel threads  The kernel performs thread creation, scheduling, etc. Generally slower to create and manage If a thread is blocked, the kernel can schedule another thread for execution   Models (user threads to kernel threads) Many-to-one, one-to-one(kernel threads 有限制，大部分系统）, Many-to-Many  Shared-Memory Programming  Definition: processes communicate or work together with each other through a shared memory space which can be accessed by all processes (Faster \u0026amp; more efficient than message passing) Many issues as well, Synchronization, Deadlock, Cache coherence Programming techniques (Parallelizing compiler, Unix processes, Threads(Pthread, Java))  Pthread  Pthread is the implementation of POSIX standard for thread pthread_create(thread, attr, routine, arg)  thread: An unique identifier (token) for the new thread attr: it is used to set thread attributes. NULL for the default values routine: The routine that the thread will execute once it is created arg: A single argument that may be passed to routine    pthread_join(threadId, status)  Blocks until the specified threadId thread terminates One way to accomplish synchronization between threads   pthread_detach(threadId)  Once a thread is detached, it can never be joined Detach a thread could free some system resources    Example\n#include \u0026lt;pthread.h\u0026gt;\r#include \u0026lt;stdio.h\u0026gt; #define NUM_THREADS 5\rvoid *PrintHello(void *threadId) {\rlong* data = static_cast \u0026lt;long*\u0026gt; threadId;\rprintf(\u0026quot;Hello World! It's me, thread #%ld!\\n\u0026quot;, *data);\rpthread_exit(NULL);\r}\rint main (int argc, char *argv[]) {\rpthread_t threads[NUM_THREADS];\rfor(long tid=0; tid\u0026lt;NUM_THREADS; tid++){\rpthread_create(\u0026amp;threads[tid], NULL, PrintHello, (void *)\u0026amp;tid);\r}\r/* Last thing that main() should do */\rthread_exit(NULL);\r}\r Java Threads  Thread is created by Extending Thread class, Implementing the Runnable interface Java threads are implemented using a thread library on the host System Thread mapping depends on the implementation of JVM  Linux threads  Linux does not support multithreading Various Pthreads implementation are available for user-level The fork system call \u0026ndash; create a new process and a copy of the associated data of the parent process The clone system call \u0026ndash; create a new process and a link that points to the associated data of the parent process A set of flags is used in the clone call for indication of the level of the sharing  None of the flag is set $\\rightarrow$ clone = fork All flags are set $\\rightarrow$ parent and child share everything     Threading Issues  Does fork() duplicate only the calling thread or all threads? Some UNIX system support two versions of fork()  execlp() works the same, replace the entire process, if exec() is called immediately after forking, then duplicating all threads is unnecessary    Thread Cancellation  Asynchronous cancellation (one thread terminates the target thread immediately) 等 main thread 有空 Deferred cancellation (default option) The target thread periodically checks whether it should be terminated, allowing it an opportunity to terminate itself in an orderly fashion (canceled safety)   Signal Handling  Signals (synchronous or asynchronous) are used in UNIX systems to notify a process that an event has occurred A signal handler is used to process signals  Signal is generated by particular event Signal is delivered to a process Signal is handled     Thread Pools  Create a number of threads in a pool where they await work Advantages  Usually slightly faster to service a request with an existing thread than create a new thread Allows the number of threads in the application(s) to be bound the size of the pool   # Of threads: # of CPUs, expected # of requests, amount of physical memory    ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"b5462720240c3a6f0cdbb53dd706387e","permalink":"/courses/operating_system/multithread_programming/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/multithread_programming/","section":"courses","summary":"Threads Threads is a light process, basic unit of CPU utilization All threads belonging to the same process share code section, data section, and OS resources (e.g. open files and","tags":null,"title":"Multithread Programming","type":"docs"},{"authors":null,"categories":null,"content":"Introduction CPU-I/O burst cycle: Process execution consists of a cycle of CPU execution and I/O wait\n Generally, there is a large number of short CPU bursts, and a small number of long CPU bursts A I/O bound program would typically has many very short CPU bursts A CPU-bound program might have a few long CPU bursts CPU scheduler: Select from ready queue to execute (i.e. allocates a CPU for the selected process)  Preemptive vs. Non-preemptive  CPU scheduling decisions may take palce when a process:  Switches from running to waitting state (IO) Switches from running to ready state (Time-sharing) Swtiches from waiting to ready state Terminates   Non-preemptive scheduling (不会打断)  Scheduling under 1 and 4 (no choice in terms of scheduling) The process keeps the CPU until it is terminated or switched to the waitting state   Preemptive scheduling, Scheduling under all cases 使用率很高 Preemptive Issues  Inconsistent state of shared data, require process synchronization, incurs a cost assocated with access to shared data Affect the design of OS kernel Unix solution: waiting either for a system call to complete or for an I/O block to take palce before doing a context switchd (disable interrupt)    Dispatcher Dispatcher module gives control of the CPU to the process selected by scheduler\n switching context jumping to the proper location in the selected program Dispatch latency \u0026ndash; time it takes for the dispatcher to stop one process and start another running  Scheduling Algorithms Scheduling Criteria  CPU utilization theoretically: $0% ~ 100%$, real systems : $40% ~ 90%$ Throughput: number of completed processes per time unit Turnaround time (submission ~ completion) Waiting time (total waiting time in the ready queue) Response time (submission ~ the first response is produced (第一个CPU burst(执行)的时间))  Algorithms First-Come, First-served (FCFS) scheduling  Process (Burst Time) in arriving order: P1(24), P2(3), P3(3) Gantt chart  Waiting time P1 = 0, P2 =24, P3 =27 Average waiting time (AWT) (0+24+27)/3 = 17 Convoy effect: short process behind a long process  Shortest-Job-First (SJF) scheduling  Associate with each process the length of its next CPU burst A process with shortest burst length gets the CPU first SJF provides the minimum average waiting time (optimal) Two schemes  Non-preemptive \u0026ndash; once CPU given to a process, it cannot be preempted until its completion Preemptive \u0026ndash; if a new process arrives with shorter burst length, preemption happens   Non-preemptive SJF example     Process Arrival Time Burst Time     P1 0 7   P2 2 4   P3 4 1   p4 5 4          AWT = [(7-0-7) + (12 - 2 -3) + (8 -4-1) + (16-5-4)] /4 = 4  Response Time: p1 =0, p2 = 6, p3=3, p4 = 7   Preemptive SJF example  AWT = 3 Response time P1 = 0, P2 = 0, P3 = 0, P4 = 2    SJF difficulty: no way to know length of the next CPU burst\n\r- Approximate SJF: the next burst can be predicted as an exponentail average of the measured length of previous CPU bursts (set $\\alpha = 1/2$)\r$$\\tau_{n+1} = \\alpha t_n + (1-\\alpha) \\tau_n = \\frac12 t_n + \\frac14 t_{n_1} + \\frac18 t_{n-2}​$$\rPriority Scheduling  A priority number is associated with each process The CPU is allocated to the highest priority process SJF is a priority scheduling where priority is the predicted next CPU burst time Problem: Starvation (low priority process never execute) Solution: aging(as time progresses increase the priority of process)  Round-Robin(RR) Scheduling  Each process gets a small unit of CPU time(time quantum) After TQ elapsed, process is preempted and added to the end of the ready queue TQ large $\\rightarrow$ FIFO TQ small $\\rightarrow$ (context switch) overhead increases  Multilevel Queue Scheduling  Ready queue is partitioned ino separate queues Each queue has its own scheduling algortihm Scheduling must be done between queues  Fixed priority scheduling: prossibility of starvation     Mutillevel Feedback Queue Schedule  A process can move between the various queues; aging can be implemented Idea: separate processes according to the characteristic of their CPU burst  I/O-bound and interactive processes in higher priority queue $\\rightarrow$ short CPU burst CPU-bound processes in lower priority queue long CPU burst    multilevel feedback queue scheduler is defined by the following parameters:  Number of queues Scheduling algorithm for each queue Method used to determin when to upgrade/demote a process    Evaluation methods  Deterministic Modeling — takes a particular predetermined workload and defines the performance of each algorithm for the workload Queueing Model — mathematical analysis Simulation — random-number generator or trace tapes for workload generation Implementation — the only completely accurate way for algorithm evaluation  Mutli-Processor Scheduling \u0026amp; Multi-Core Processor Scheduling \u0026amp; Real-Time Scheduling  Asymmetric multiprocessing  All system activities are handled by a processor (alleviationg the need for data sharing) the other only execute user code (allocated by the master) far simple than SMP   Symmetric multiprocesiing (SMP):  each processor is self-scheduling all processor in common ready queue, or each has its own private queue of ready processes need synchronization mechanism   Processor affinity A process has an affinity for the processor on which it is currently running  A process populates its recent used data in cache memory of its running processor cache invalidation and repopulation has high cost Soft affinity: possible to migrate between processors hard affinity: not to migrate to other processor   NUMA (non-uniform memory access):  Occurs in systems containing combines CPU and memory boards CPU scheduler and memory-replacement works together A process (assigned affinity to a CPU) can be allocated memory on the board where that CPU resides    Load-balancing  Keep the workload evently distributed across all processors, only necessary on systems where each processor has its own private queue of eligible processes to execute Push migration : move(push) processes from overloaded to idle or less-busy processor Pull migration: idle processors pulls a waiting task from a busy processor Often implemented in parallel    Multi-core Processor Scheduling  Faster and consumer less power memory stall: When access memory, it spends a significant amount of time waiting for the data become available. (e.g. cache miss) Multi-threaded multi-core systems  Two (or more) hardware threads are assigned to each core (Intel Hyper-threading) Takes advantage of memory stall to make progress on another thread while memory retrieve happens    Two ways to multithread a processor:  coarse-grained: switch to another thread when a memory stall occurs. The cost is high as the instruction pipeline must be flushed fine-grained(interleaved): switch between threads at the boundary of an instruction cycle. The architecture design includes logic for thread switching \u0026ndash; cost is low   Scheduling for Multi-threaded multi-core systems  1st level: Choose which software thread to run on each hardware thread(logical processor) 2nd level: How each core decides which hardware thread to run    Real-Time CPU Scheduling Real-time does not mean speed, but keeping deadlines\n Soft real-time requirements, missing the deadline is unwanted , but is not immediately critical (multimedia streaming) Hard real-time requirements, Missing the deadline results in a fundamental failure (nuclear power plant controller)  Read-Time Scheduling Algorithms Evaluation: Ready, Execution, Deadline\nRate-Monotoinc (RM) algorithm  short period, higher priority (fixed-priority RTS scheduling algorithm) Ex: T1 = (4,1) (red), T2 = (5,2) (orange), T3 = (20,5)(green) (Period, Execution) priority: T_1 \u0026gt; T_2 \u0026gt; T_3   Earliest-Deadline-First(EDF) algorithm  Earlier deadline, higher priority (dynamic priority algorithm) Ex: T1 = (2, 0.9), T2 = (5, 2.3)   Operating System Examples  Solaris Scheduler Windows XP Scheduler Linux Scheduler  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"e8ee4ac4e5f1480ec84a895ee2223555","permalink":"/courses/operating_system/process_schedule/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/process_schedule/","section":"courses","summary":"Introduction CPU-I/O burst cycle: Process execution consists of a cycle of CPU execution and I/O wait Generally, there is a large number of short CPU bursts, and a small number","tags":null,"title":"Process Scheduling","type":"docs"},{"authors":null,"categories":null,"content":"Background  Concurrent access to shared data may result in data inconsistency Maintaining data consistency requires mechanism to ensure the orderly execution of cooperating processes Consumer \u0026amp; Producer Problem Race condition: the situation where several processes access and manipulate shared data concurrenlty. The final value of the shared data depends upon which process finishes last, commonly described as critical section problem To prevent race condition, concurrent processes must be synchronized, on a single-process machine, we could disable interrupt or use non-preemptive CPU scheduling  Crtical Section  Purpose: a protocal for processes to cooperate Probelm description:  N process are competing use some shared data Each process has a code segment, called critical selection, in which the shared data is accessed Ensure that when one process is executing in its critical section, no other process is allowed to execute in its critical selection $\\rightarrow$ mutually exclusive   Critical Section Requirements  Mutual Exclusion: if process P in executing in its CS, no other processes can be executing in their CS Progress: if no process is executing in its CS and there exist some processes that wish to enter their CS, these processes cannot be postponed indefinitely Bounded Waiting: A bound must exist on the number of times that other processes are allowed to enter their CS after a process has made a request to enter its CS    Solution Algorithm for two Processes  only 2 processes, $P_0$ and $P_1$ Shared variables  int turn; // initially turn = 0 turn = i $\\rightarrow$ $P_i$ can enter its critical section  Mutual exclustion (yes); Progress (No); Bounded-Wait(Yes)    Peterson\u0026rsquo;s solution for Two processes   shared variables\n int turn, initially turn =0 turn = i $\\rightarrow$ $P_i$ can enter its critical section boolean flag[2] // initially flag[0] = flag[1] = false flag[I] = true $\\rightarrow$ $P_i$ ready to enter its critical section Mutual Selection, progress, bounded waiting proof    Example\ndo {\rflag[i] = TRUE; // Pi 是否想要进去\rturn = j; // 先把key交给对方\rwhile (flag[j] \u0026amp;\u0026amp; turn == j);\r// critical section\rflag[i] = FALSE;\rremainder section\r} while(1);\r   Bakery Algorithm (n processes)   Before enter its CS, each process receives a number\n  Holder of the smallest number enters CS\n  The numbering scheme always generates number in non-decreasing order; i.e. 1,2,3,3,4,5,5,5\n  if processes $P_i$ and $P_j$ receive the same numbe, if $i\u0026lt;j$ then $P_i$ is served first Bounded-waiting because processes enter CS on a First-come, First Served basis\n// process i:\rdo {\rchoosing[i] = TRUE;\rnum[i] = max(num[0], num[1], ..., num[n-1]) + 1;\rchoosing[i] = FALSE;\rfor(j =0; j\u0026lt;n; j++){\rwhile(choosing[j]); // cannot compare when num is being modified\rwhile((num[j]!=0) \u0026amp;\u0026amp; ((num[j], j)) \u0026lt; (num[i],i))); // FCFS\r}\r// critical section\rnum[i] = 0;\r// reminder section\r}while(1);\r   Pthread Lock/Mutex Routines  To use mutex, it must be declared as type pthread_mutex_t and initialized with pthread()_mutex_init() A mutex is destoryed with pthread_mutex_destory() A critical selection can then be protected using pthread_mutex_lock() and pthread_mutex_unlock()  Condition Variables (CV)  CV represent some condition that a thread can:  Wait on, until the condition occurs; or Notify other waiting threads that the condition has occured   Three operations on condition variables:  wait() — Block until another thread calls signal() or broadcast() on the CV signal() — Wake up one thread waiting on the CV broadcast() — Wake up all threads waiting on the CV   All condition variable operation must be performed while a mutex is locked In Pthread, CV type is pthread_cond_t  Use pthread_cond_init() to initalize pthread_cond_wait(\u0026amp;theCV, \u0026amp;somelock) pthread_cond_signal(\u0026amp;theCV) pthread_cond_broadcast(\u0026amp;theCV)   Example:  A thread is designed to take action when x = 0 Another thread is responsible for decrementing the counter All condition variable operation must be performed while a mutex is locked    Procedure  left thread  Lock mutex Wait()  Put the thread into sleep and releases the lock Waked up, but the thread is locked Re-acquire lock and resume execution   Release the lock   right thread  Lock mutex Signal() Release the lock()      ThreadPool Implementation struct threadpool_t {\rpthread_mutex_t lock;\rpthread_cond_t notify;\rpthread_t *treahd;\rthreadpool_task_t *queue;\rint thread_count;\rint queue_size;\rint head;\rint tail;\rint count;\rint shutdown;\rint started;\r};\rtypedef struct {\rvoid (*function) (void*);\rvoid *argument;\r} threadpool_task_t;\r// allocate thread and task queue\rpool-\u0026gt;threads = (pthread_t *) malloc(sizeof(pthread_t) * thread_count);\rpool-\u0026gt;queue = (threadpool_task_t *) malloc(sizeof(threadpool_task_t) * queue_size);\r// threadpool implementation\rstatic void *threadpool_thread(void *threadpool)\r{\rthreadpool_t *pool = (threadpool_t *) threadpool;\rthreadpool_task_t task;\rfor(;;){\r// lock must be taken to wait on conditional varaibl\rpthread_mutex_lock(\u0026amp;(pool-\u0026gt;lock));\rwhile((pool-\u0026gt;count=0) \u0026amp;\u0026amp; (!pool-\u0026gt;shutdown)) {\rpthread_cond_wait(\u0026amp;(pool-\u0026gt;notify), \u0026amp;(pool-\u0026gt;lock));\rtask.function = pool-\u0026gt;queue[pool-\u0026gt;head].function;\rtask.argument = poll-\u0026gt;queue[pool-\u0026gt;head].argument;\rpool-\u0026gt;head +=1;\rpool-\u0026gt;head = (pool-\u0026gt;head == poll-\u0026gt;queue_size) ? 0 : pool-\u0026gt;head;\rpool-\u0026gt;count -=1;\rpthread_mutex_unlock(\u0026amp;(pool-\u0026gt;lock));\r(*(task.function))(task.argument);\r}\r}\r}\r Hardware Support  The CS problem occurs because the modification of a shared variable may be interrupted If disable interrupts when in CS, not feasible in multiprocessor machine, clock interrupts cannot fire in any machine HW support solution: atomic instruction  atomic: as one uninterruptible unit Examples: TestAndSet(var), Swap(a, b)    Atomic TestAndSet() booelean TestAndSet(bool \u0026amp;lock) {\rbool value = lock;\rlock = TRUE; // return the value of \u0026quot;lock\u0026quot; and set \u0026quot;lock\u0026quot; to ture\rreturn value;\r}\r Mutual Exclusion (Yes), Progress(Yes), Bounded-Wait(No)\rAtomic Swap()  Idea: enter CS if lock=false Shared data: boolean lock; //initially lock=FALSE   Semaphores   A tool to generalize the synchronization problem (easier to solve, but no guarantee for correctness)\n  A record of how many units of a particular resource are available\n if #record = 1 $\\rightarrow$ binary semaphore, mutex lock if #record \u0026gt; 1 $\\rightarrow$ counting semaphore    Accessed only through 2 atomic ops: wait \u0026amp; signal\n  Spinlock implementation (浪费CPU资源)\nwait(S) {\rwhile (S\u0026lt;=0);\rS--;\r}\rsignal(S){\rS++;\r}\r   POSIX Semaphore (OS support) Semaphore is part of POSIX standard but it is not belong to Pthread, it can be used with or without thread\n  POSIX Semaphore routines\n sem_init(sem_t *sem, int pshared, unsigned int value) sem_wait(sem_t *sem) sem_post(sem_t *sem) sem_getvalue(sem_t *sem, int *valptr) sem_destory(sem_t *sem)    Example\n  #include\u0026lt;semaphore.h\u0026gt;\rsem_t sem;\rsem_init(\u0026amp;sem);\rsem_wait(\u0026amp;sem);\r// critical section\rsem_post(\u0026amp;sem);\rsem_destory(\u0026amp;sem);\r n-Process Critical Section Problem  shared data: semaphore mutex; // initially mutex = 1 Process Pi  do {\rwait(mutex); // pthread_mutex_lock(\u0026amp;mutex)\rcritical section\rsignal(mutex); // pthread_mutex_unlock(\u0026amp;mutex)\rremainder section\r} while(1);\rProgress? Yes\rBounded waiting? Depends on the implementation of `wait()`\r Non-busy waiting Implementation   Semaphore is data struct with a queue, may be any queuing strategy\ntypedef struct {\rint value; // init to 0\rstruct process *L // queue\r} semaphore\r   wait() and signal()\n use system calls: block() and wakeup() must be executed atomically Ensure atomic wait \u0026amp; signal ops?   Single-process: disable interrupts\n  Multi-processor: 1. HW support 2. SW solution(Peterson\u0026rsquo;s solution, Bakery algorithm)\nvoid wait(semaphore S)\r{\rS.value--;\rif(S.value \u0026lt; 0){\radd this process to S.L\rsleep();\r}\r}\rvoid signal(semaphore S) {\rS.value++;\rif(S.value\u0026lt;=0){\rremove a process P from S.L;\rwakeup(P);\r}\r}\r       Semaphore with Critical Section Classical Synchronization Problems  Purpose: Used for testing newly proposed synchronization scheme Bounded-Buffer (Producer-Consumer) Reader-Writer Problem Dining-Philosopher Problem  Bounded-Buffer Problem A poof of n buffers, each capable of holding one item\n Producer:  Grad an empty buffer place an item into the buffer waits if no empty buffer is available   Consumer  grab a buffer and retracts the item place the buffer back to the free poll waits if all buffers are empty    Readers-Writers Problem  A set of shared data objects A group of processes (reader processes, writer processes, a writer process has exclusive access to a shared object) Different variations involving priority  first RW problem: no reader will be kept waiting unless a writer is updating a shared object (writer会starvation) second RW problem: once a writer is ready, it performs the updates as soon as the shared object is released (writer has higher priority than reader; once a writer is ready, no new reader may start reading)   First Reader-Writer Algorithm  // mutual exclustion for write\rsemaphore wrt = 1;\rsemaphore mutex = 1;\rint readcount = 0;\rWriter() {\rwhile(TRUE) {\rwait(wrt);\r// Writer Code\rsignal(wrt);\r}\r}\rReader() {\rwhile(TRUE) {\rwait(mutex);\rreadcount++;\rif(readcount==1) // 之后的 Reader 不需要拿lock\rwait(wrt);\rsignal(mutex);\r// Reader Code\rwait(mutex);\rreadcount--;\rif(readcount == 0)\rsignal(wrt);\rsignal(mutex);\r}\r}\r Dining-Philosopher Problem  5 person sitting on 5 chairs with 5 chopsticks A person is either thinking or eating  thinking: no interaction with the rest 4 persons eating: need 2 chopsticks at hand a person picks up 1 chopsticks at a time done eating: put down both chopsticks   deadlock problem  one chopstick as one semaphore   starvation problem  Monitors (A high-level language construct) Motivation Although semaphores provide a convenient and effective synchronization mechanism, its correctness is depending on the programmer\n All processes access a shared data object must execute wait() and signal() in the right order and right place This may not be true because honest programming error or uncooperative programmer The representation of a monitor type consists of declarations of variables whose values define the state of an instance of the type and the functions(procedures) that implement operations on the type The monitor type is similar to a class in O.O. language  A procedure within a monitor can access only local variables and the formal parameters The local variables of a monitor can be used only by the local procedures   The monitor ensures that only one process at a time can be active within the monitor  Similar idea is incorporated to many programming language (concurrent pascal, C#, and Java) \r  Monitor introduction High-level synchronization construct that allows the safe sharing of an abstract data type among concurrent processes  Monitor Condition Variables  To allow a process to wait within the monitor, a condition variable must be declared as condtion x, y; Condition variable can only be used with the operations wait() and signal() wait() means that the process invoking this operation is suspended until another process invokes signal() resumes exactly one suspended process. If no process is suspended, the signal operation has no effect (in contrast, signal always change the state of semaphore)     Dining-Philosophers Example monitor dp {\renum {thinking, hungry, eating} state[5]; //current state\rcondition self[5]; // delay eating if can't obtain chopsticks\rvoid pickup(int i) // pickup chopsticks\rvoid putdown (int i) // putdown chopsticks\rvoid test (int i) // try to eat\rvoid init(){\rfor (int i =0; i\u0026lt;5; i++)\rstate[i] = thinking;\r}\r}\rvoid pickup(int i) {\rstate[i] = hungry;\rtest(i);\rif (state[i] != eating)\rself[i].wait(); // wait to eat\r}\rvoid putdown(int i) {\rstate[i] = thinking;\r// check if neighbors are waiting to eat\rtest((i+4) % 5);\rtest((i+1) % 5);\r}\rvoid test(int i) {\rif ((state[(i+4) % 5] != eating) \u0026amp;\u0026amp; (state[(i+1) % 5] != eating) \u0026amp;\u0026amp; (state[i] == hungry)) {\rstate[i] = eating;\rself[i].signal(); // if Pi is suspended, resume it, if Pi is not suspended, no effect\r}\r}\r Synchronized Tools in JAVA   Synchronized Methods(Monitor)\n  Synchronized method uses the method receiver as a lock\n  Two invocations of synchronized methods cannot interleave on the same object\n  When one thread is executing a synchronized method for a object, all other threads that invoke synchronized methods the same object block until the first thread exist the object\npublic class SynchronizedCounter {\rprivate int c= 0;\rpublic synchronized void increment() {c++;}\rpublic synchronized void decrement() {c--;}\rpublic synchronized int value() {return c;}\r}\r     Synchronized Statement (Mutex Lock)\n  Synchronized blocks uses the expression as a lock\n  A synchronized statement can be only be executed once the thread has obtained a lock for the object or the class that has been referred to in the statement\n  useful for improving concurrency with fine-grained\npublic void run(){\rsynchronized(p1)\r{\rint i = 10; // statement without locking requirement\rp1.display(s1);\r}\r}\r     Atomic Transactions  Transactions: a collection of instructions (or instructions) that performs a single logic function Atomic Transactions: Operations happen as a single logical unit of work, in its entirely, or not at all Atomic translation is particular a concern for database system (Strong interest to use DB techniques in OS)  File I/O example  Transaction is a series of read and write operations Terminated by commit (transaction successful) or abort (transaction failed) operation Aborted transaction must be rolled back to undo any changes it performed (it is part of )  Log-Based Recovery  Record to stable storage information about all modifications by a transaction Write-ahead logging: Each log record describes single transaction write operation  Transaction time Data item name Old \u0026amp; new values Special Events: \u0026lt;$T_i$, start\u0026gt;, \u0026lt;$T_i$, commmits\u0026gt;   Log is used to reconstruct the state of the data items modified by the transactions Checkpoints  when faiulre occurs, must consult the log to determine which transactions must be re-done, searching process is time consuming and redone may not be necessary for all transactions use checkpoints to reduce the above overhead, output all log records, modified data, log record  to stable storage    ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"d59e68c245240c3f235ae079ef5326ed","permalink":"/courses/operating_system/process_synchronization/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/process_synchronization/","section":"courses","summary":"Background Concurrent access to shared data may result in data inconsistency Maintaining data consistency requires mechanism to ensure the orderly execution of cooperating processes Consumer \u0026amp; Producer Problem Race condition:","tags":null,"title":"Process Synchronization","type":"docs"},{"authors":null,"categories":null,"content":"Introduction  A set of blocked process each holding some resources and waiting to acquire a resource held by another process in the set Ex1: 2 processes and 2 tape dirves, each process holds a tape drive, each process requests another tape drive Ex2: 2 processes, and semaphores A \u0026amp; B, P1(hold B, wait A): wait(A), signal(B), P2(hold A, wait B): wait(B), signal(A)  Necessary Conditions  Mutual exclustion (only 1 process at a time can use a resource) Hold \u0026amp; Wait: a process holding some resources and is waiting for another resource No preemption: a resource can be only released by a process voluntarily Circular wait: there exist a set {$P_0, P_1, \u0026hellip;, P_n$} of waiting process such that $P_0 \\rightarrow P_1 \\rightarrow P_2, \u0026hellip; , P_0$ All four conditions must hold for possible deadlock   System Model  Resources type $R_1, R_2, \u0026hellip; , R_m$ E.g. CPU, memory pages, I/O devices Each resource type $R_i$ has $W_i$ instances, E.g. a computer has 2 CPUs Each process utilizes a resouce as follows: Request $\\rightarrow$ use $\\rightarrow$ release  Resource-Alocation Graph  3 processes, P1-P3 4 resources, R1-R4 (the black dot represent the number of instance) Request edges: P1$\\rightarrow$ R1: P1 requests R1 Assignment edges: R2$\\rightarrow$ P1: one instance of R2 is allocated to P1 P1 is hold on an instance of R2 and waiting for an instance of R1  if the graph consists a cycle, a deadlock may exist deadlock  no deadlock  If graph contains no cycle $\\rightarrow$ no deadlock If graph contains a cype:  if one instance per resource type $\\rightarrow$ deadlock if multiple instances per resource type $\\rightarrow$ possibility of deadlock    Handing Deadlocks  Ensure the system will never a deadlock state  deadlock preventation: ensure that at least one of the 4 necessary conditions cannot hold deadlock avoidance: dynamically examines the resource-allocation state before allocation   Allow to enter a deadlock state and then recover  deadlock detection dedlock recovery   Ignore the problem and pretend that deadlocks never occur in the system (used by most operating systems, including UNIX)  Deadlock Prevention  Mutual Exclustion(ME): do not require ME on sharable resources  e.g. there is no needto ensure ME on read-only files Some resources are not shareable, however (e.g. printer)   Hold \u0026amp; Wait:  When a process requests a resource, it does not hold any resource Pre-allocate all resources before executing (resource utilization is low, starvation is possible)   No preemption  When a process is waiting on a resource, all its holding resources are preempted (e.g. P1 request R1, which is allocated P2, which in turn is waiting on R2, R1 can be preempted and reallocated to P1) Applied to resources whose states can be easily saved and restored later (e.g. CPU registers \u0026amp; memory) It cannot easily be applied to other resources(e.g. printers \u0026amp; tape drives)   Circular wait  impose a total ordering of all resources types A process requests resources in an increasing order  Let $R = {R_0, R_1, \u0026hellip; , R_N}$ be the set of resource types When request $R_k$, should release all $R_i$, $i \\geq k$      Avoidance Algortihm  safe state: a system is in a safe state if there exists a sequence of allocations to satisfy requests by all processes (this sequence of allocations is called safe sequence)   Single instance of resource type (resource-allocation graph (RAG) algorithm based on circle detection) Multiple instance of resource type banker\u0026rsquo;s algorithm based on safe sequence detection  Resource-Allocation Graph (RAG) Algorithm  Request edge Assignment edge Claim edge: $P_i \\rightarrow R_j$, process $P_i$ may request $R_j$ in the future Clain edge converts to request edge (when a resource is requested by process) Assignment edge converts back to claim edge (when a resource is released by a process) Resources must be claimed a priori in the system Grant a request only if no cycle created Check for safety using a cycle-detection algorithm, $O(n^2)$  R2 cannot be allocated to P2  Banker\u0026rsquo;s algorithm  Safe State with Safe Sequnce use for multiple instance of each resource type use a general safety algorithm to pre-determine if any safe sequence exists after allocation only proceed the allocation if safe sequence exists Procedures:  Assume processes need maximum resources Find a process that can be satisfied by free resources Free the resource usage of the process repeat to step 2 until all processes are satisfied   Example Safe sequence: $P_1, P_3, P_4, P_2, P_0$  if Request (P1) = (1,0,2): P1 allocation $\\rightarrow$ 3, 0, 2 (Safe sequence: $P_1, P_3, P_4, P_0, P_2$) if request (P4) = (3,3,0): P4 allocation $\\rightarrow$ 3,3,2 (no safe sequence can be found)    Deadlock Detection  Single instance of each resource type  convert request/assignment edges into wait-for graph deadlock exists if there is a cycle in the wait-for graph    Multiple-Instance for each Resource type Total instances: A(7), B(2), C(6) (Request 表示已经发生)  The system is in a safe state $\\rightarrow$ \u0026lt;P_0, P_2, P_3, P_1, P_4\u0026gt; (no deadlock) If P2 request = \u0026lt;0, 0, 1\u0026gt; $\\rightarrow$ no safe sequence can be found $\\rightarrow$ the system is deadlocked    Deadlock Recovery  process termination  abort all deadlocked processes abort 1 process at a time until the deadlock cyle is eliminated (which process should we abort first)   Resource preemption  select a victim: which one to preempt rollback: partial rollback or total rollback? starvation: can the same process be preempted always?    ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"a59146e4b935a81b2d64b2e9336521ce","permalink":"/courses/operating_system/deadlocks/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/deadlocks/","section":"courses","summary":"Introduction A set of blocked process each holding some resources and waiting to acquire a resource held by another process in the set Ex1: 2 processes and 2 tape dirves,","tags":null,"title":"DeadLocks","type":"docs"},{"authors":null,"categories":null,"content":"File Concept  A logical storage unit created by OS (v.s. physical storage unit in disk (sector, track))  file attributes (identifier, Name, type, Location, Size, protection, Last-access time, last-updated time) file operations (creating, reading, writing, repositioning, deleting, truncating) file types: .exe .com .obj .cc .mov (Hint for OS to operate file in a resonable way) Process: open-file table OS: system-wide table (process shared)  Open-File Tables \u0026amp; System-wide table  Per-process table  Tracking all files opened by this process Current file pointer for each opened file Access rights and accounting information   System-wide table  Each entry in the per-process table points to this table Process-independent information such as disk location, access dates, file size     Access Methods  Sequential access  Read/write next (block) Reset: repositoning the file pointer to the beginning Skip/rewind n records   Direct(relative) access  Access an element at an arbitrary positin in a sequence File operation include the block # as parameter Often use random access to refer the access pattern from direct access    Index Access Methods  Index: contains pointers to blocks of a file To find a record in a file (search the index file $\\rightarrow$ find the pointer, use the pointer to directly access the record) with a large file $\\rightarrow$ index could become too large     Directory Structure  Partition (formatted or raw)  raw partiton (no file system): UNIX swap space, database Formatted partition with file system is called volume a partition can be a portion of a disk or group of multiple disks (distributed system) some storage devices (e.g.: floopy disk) dose not and cannot have partition   Directories are used by file system to store the information about the files in the partition  Directory Operations (search, create, delete, list, rename, traverse)  Single-Level Directory All files in one directory, filename has to be unique, poor efficiency in locating a file as number of file increases Two-Level Dicectory  a separate dir for each user path = user name + file name single-level dir problems still exist per user   Tree-Structured Directory  Absolute path: starting from the root Relative path: starting from a directory  Acyclic-Graph Directory  use links to share files or directories (symbolic link ln) a file can have multiple absolute paths when dose a file actually get deleted?  deleting the link but not the file deleting the file but leaves the link $\\rightarrow$ dangling pointer     General-Graph Directory  May contain cycles  Reference count dose not work any more (e.g. self-reference file) How can we deal with cycles? (Garbage collection) First pass traverses the entire graph and marks accessible files or directories second pass collect and free everything that is un-marked poor performance on millions of files   Use cycle-detection algorithm when a link is created  File-System Mounting \u0026amp; File sharing File-System Mounting  A file system must be mounted before it can be accessed Mount point: the root path that a FS will be mounted to Mount timing: boot time, automatically at run-time, manually at run time  mount -t type deivce dir (mount -t ext2 /dev/sda0 /mnt) \r  File sharing On Multiple Users  Each user: (userID, groupID)  ID is associated with every ops/process/thread/ the user issues   each file has 3 sets of attributes (owner, group, others) Owner attributes describe the privileges for the owner of the file  same for group/others attributes group/others attributes are set by owner or root    Access-Control List  We can create an access-control list (ACL) for each user  check requested file access against ACL problem: unlimited # of users   3 classes of users $\\rightarrow$ 3 ACL (RWX) for each file  owner (e.g 7 = RWX = 111) group (e.g. 6 = RWX = 110) others (e.g. 4 = RWX = 100)    File Protection  File owner/creator should be able to control (what can be done, by whom, access control list(ACL)) file should be kept from (  physical damage (reliability): RAID improper access (protection): i.e. password    ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"0f99ca4aed86f46c0fccdba0fe2f437a","permalink":"/courses/operating_system/file_system_interface/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/file_system_interface/","section":"courses","summary":"File Concept  A logical storage unit created by OS (v.s. physical storage unit in disk (sector, track))  file attributes (identifier, Name, type, Location, Size, protection, Last-access time, last-updated time) file operations (creating, reading, writing, repositioning, deleting, truncating) file types: .","tags":null,"title":"File System Interface","type":"docs"},{"authors":null,"categories":null,"content":"File-System Structure  I/O transfer between memory and disk are performed in units of blocks  one block is one or more sectors one sector is usually 512 bytes   One OS can support more than 1 FS types (NTFS, FAT32) Two design problems in FS  interface to user programs interface to physical storage (disk)   Layered File System   Data Structure On-Disk Structure  Boot control block (per partition): information needed to boot an OS from that partition  typical the first block of the partition (empty means no OS) UFS (Unix File Sys): boot block, NTFS: partition boot sector   Partition Control Block (per partion): partion details  details: # of blocks, block size, free-block-size, free FCB pointers USF: superblock, NTFS: Master File Table   File control block (per file): details regrading a file  details: permission, size, location of data blocks UFS: inode, NTFS: stored in MFT(relational database)   Directory structure (pef file system): organize files   In-Memory Structure  in-memory partition table: information about each mounted parition in-memory directory structure: information of recently accessed directories system-wide open-file table: contain a copy of each opened file\u0026rsquo;s FCB (file control block) per-process open-file table: pointer (file handler / descriptor) to the corresponding entry in the above table  Example  create  OS allocates a new FCB update directory structure  OS reads in the corresponding directory Updates the dir structure with the new file name and the FCB (After file being closed), OS writes back the directory structure back to disk   the file appears in user\u0026rsquo;s dir command    Virtual File System  VFS provides an object-oriented way of implementing file systems VFS allows the same system call interface to be used for different types of FS VFS calls the appropriate FS routines based on the partition info  Four main object types defined by Linux VFS:  inode (an individual file, file control block) file object (an open file) superblock object (an entire file system) dentry object (an individual directory entry)   VSF defines a set of operations that must be implemented (e.g. for file object)  int open(\u0026hellip;) (open a file) ssize_t read() (read from a file)   Directory implementation  Linear Lists (list of names with pointers to data blocks, easy to program but poor performance) Hash table \u0026ndash; linear list w/hash data structure, constant time for searching, linked list for collosions on a hash entry, hash table usually has fixed number of entires    Allocation Methods  An allocation method refers to how disk blocks are allocated for files Allocation strategy: Contiguous allocation, Linked allocation, Indexed allocation  Contiguous Allocation - Each file occupies a set of contiguous blocks\r- num of disk seeks is minimized\r- The dir entry for each file = (starting num, size)\r- Both sequential \u0026 random access can be implemented efficiently\r- Problems\r- External fragmentation $\\rightarrow$ compaction\r- file cannot grow $\\rightarrow$ extend-based FS\rExtent-Based File System  Many newer file system use a modified contiguous allocation scheme Extent-based file system allocate disk blocks in extents An extent is a contiguous blocks of disks  A file contains one or more extents An extent: (starting block num, length, pointer to next extent)   Random access become more costly Both internal \u0026amp; external fragmentation are possible  Linked Allocation  each file is a linked list of blocks  each block contains a pointer to the next block data portion: block size \u0026ndash; pointer size   file read: following through the list  Problems  only good for sequential-access files random access requires traversing through the link list each access to link listis a disk I/O (link pointer is stored inside the data block) space required for pointer (4/512 = 0.78%) (solution: cluster of blocks) Reliability (one missing link breaks the whole file)    FAT (File Allocation Table) file system - FAT32\r- store all links in a table\r- 32 bits per table entry\r- located in a section of disk at the beginning of each partition\r- FAT(table) is often cached in memory\r- Random access is improved\r- Disk head find the location of any block by reading FAT\rIndex Allocation Example  The directory contains the address of the file index block each file has its own index block index block stores block # for file data  Bring all the pointers together into one location: the index block (one for each file) Implement direct and random access efficiently No external fragmentation Easy to create file (no allocation problem) Disadvantages  space for index blocks how large the index block should be:  linked scheme multilevel index combined scheme (inode in BSD UNIX)      Linked Indexed Scheme Multilevel Scheme (two-level) combined scheme: unix inode  File pointer: 4B (32 bits) Let each data/index block be 4KB   Free space  Free-space list: records all free disk blocks Scheme  Bit vector Linked List (same as linked allocation) Grouping (same as linked index allocation) Counting (same as contiguous allocation)   file system usually manage free space in the same way as a file  Bit vector  Bit vector(bitmap): one bit foreach block simplicity, efficient (HW support bit-manipulation instruction) bitmap must be cached for gooe performance  A 1-TB(4KB block) disk needs 32MB bitmap     ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"adb66d458430c7d04542a7c899d35566","permalink":"/courses/operating_system/file_system_implementatin/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/file_system_implementatin/","section":"courses","summary":"File-System Structure  I/O transfer between memory and disk are performed in units of blocks  one block is one or more sectors one sector is usually 512 bytes   One OS can support more than 1 FS types (NTFS, FAT32) Two design problems in FS  interface to user programs interface to physical storage (disk)   Layered File System   Data Structure On-Disk Structure  Boot control block (per partition): information needed to boot an OS from that partition  typical the first block of the partition (empty means no OS) UFS (Unix File Sys): boot block, NTFS: partition boot sector   Partition Control Block (per partion): partion details  details: # of blocks, block size, free-block-size, free FCB pointers USF: superblock, NTFS: Master File Table   File control block (per file): details regrading a file  details: permission, size, location of data blocks UFS: inode, NTFS: stored in MFT(relational database)   Directory structure (pef file system): organize files   In-Memory Structure  in-memory partition table: information about each mounted parition in-memory directory structure: information of recently accessed directories system-wide open-file table: contain a copy of each opened file\u0026rsquo;s FCB (file control block) per-process open-file table: pointer (file handler / descriptor) to the corresponding entry in the above table  Example  create  OS allocates a new FCB update directory structure  OS reads in the corresponding directory Updates the dir structure with the new file name and the FCB (After file being closed), OS writes back the directory structure back to disk   the file appears in user\u0026rsquo;s dir command    Virtual File System  VFS provides an object-oriented way of implementing file systems VFS allows the same system call interface to be used for different types of FS VFS calls the appropriate FS routines based on the partition info  Four main object types defined by Linux VFS:  inode (an individual file, file control block) file object (an open file) superblock object (an entire file system) dentry object (an individual directory entry)   VSF defines a set of operations that must be implemented (e.","tags":null,"title":"File System Implementation","type":"docs"},{"authors":null,"categories":null,"content":"","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597196259,"objectID":"85f179d6cd0695a77a5da0548a6c04e7","permalink":"/courses/parallel_computing/introduction/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/parallel_computing/introduction/","section":"courses","summary":"","tags":null,"title":"Introduction","type":"docs"},{"authors":null,"categories":null,"content":"Disk Structure  Disk drives are addressed as large 1-dim array of logical blocks (logical block: smallest unit of transfer(sector)) Logical blocks are mapped onto disk sequentially  Sector 0: 1st sector of 1st track on the outermost cyl go from outermost cylinder to innermost one   Disk drive attached to a computer by an I/O bus  Sectors Per Track  Constant linear velocity (CLV)  density of bits per track is uniform more sectors on a track in outer cylinders keeping same data rate increase rotation speed in inner cylinders applications: CD-ROM and DVD-ROM   Constant angular velocity (CAV)  keep same rotation speed larger bit density on inner tracks keep same data rate applications: hard disks    Disk Scheduling  Disk-access time has 3 major components  Seek time: move disk arm to the desired cylinder rotational latency: rotate disk head to the desired sector read time: constant transfer time   Disk bandwidth: number of bytes transferred / (complete of last req - start of first req)  Minimize seek time illustrate with a request queue(0-199) (98, 183, 37, 122, 14, 124, 65, 67)  Algorithm  FCFS (First come first served) SSTF(Shortest-seek-time-first)  SSTF scheduling is a form of SJF scheduling; may cause starvation of some requests total head movement: 236 cylinders common and has a natural appeal, but not optimal   SCAN scheduling  disk head move from one end to the other end A.k.a elvator algorithm total head movement: 236 cylinders perform better for disks with heavy load No staravation problem   C-SCAN scheduling  Disk head move in one direction only A variant of SCAN to provide more uniform wait time More uniform wait time   C-LOOK scheduling  version of C-SCAN Disk head moves only to the last request location   Performance is also influenced by the file-allocation method  Contiguous: less head movement Indexed \u0026amp; linked: greater head movement    Disk Management Disk Formatting  Low-level formatting (or physical formatting): dividing a disk into sectors that disk controller can read and write each sector = header + data area + trailer  header \u0026amp; trailer : sector number and ECC (error-correction code) ECC is calculated based on all bytes in data area data area size: 512B, 1KB, 4KB   OS does the next 2 steps to use the disk  partition the disk into one or more groups of cylinders logical formatting (i.e. creation of a file system)    Boot Block  Bootstrap program  Initialize CPU, registers, device, controllers, memory, and then starts OS First boostrap code stored in ROM complete bootstrap in the boot block of the boot disk (aka system disk)    Windows 2000  Run bootstrap code in ROM Read boot code in MBR (Master Boot Record) Find boot partition from partition table read boot sector/block and continue booting   Bad Blocks  Simple disks like IDE disks Sophisticated disks like SCSI disks Sector sparing (forwarding): remap bad block to a spare one  Could affect disk-scheduling performance A few spare sectors in each cylinder during formatting   Sector slipping: ships sectors all down one spot  Swap-Space Management  swap-space: Virtual memory use disk space (swap-space) as an extension of main memory UNIX: allows use of multiple swap spaces Location  part of a normal file system (e.g. NT) less efficient separate disk partition (raw partition) size is fixed allows access to both types (e.g. LINUX)    Swap Space Allocation  1st version: copy entire process between contiguous disk regions and memory 2nd version: copy pages to swap space  Solaris 1: text segments read from file system, thrown away when pageout, only anonymous memory (stack, heap, etc) store in swap space Solaris 2: swap-space allocation only when pageout rather than vitrual memory creation time   Data structures for Swapping   RAID Structure  RAID = Redundant Arrays of Inexpensive Disks  Provide reliability via redundacy improve performance via parallelism   RAID is arranged into different levels (Striping, mirror, Error-correcting code (ECC) \u0026amp; Parity bit)  RAID 0 \u0026amp; RAID 1  RAID 0: non-redundant striping  improve performance via parallelism I/O bandwidth is proportional to the striping count (Both read and write BW increase by N times)   RAID 1: Mirrored disks  Provide reliability via redundancy  Read BW increases by N times Write BW remains the same       RAID 2: Hamming code  E.g.: Hamming code (7,4)  4 data bits (on 4 disks) + 3 parity bits (on 3 disks) each parity bit is linear code of 3 data bits   Recover from any single disk failure  can detect up to two disk (i.e. bits) error but can only \u0026ldquo;correct\u0026rdquo; one bit error   better space efficient than RAID1 (75% overhead)   RAID 3 \u0026amp; 4: Parity Bit  Disk controller can detect whether a sector has been read correctly a single parity bit is enough to correct error from a single disk failure RAID 3: bit-level striping; RAID 4: Block-level striping Even though space efficiency Cost to compute \u0026amp; store parity bit RAID4 has higher I/O throughput, because controller does not need to reconstruct block from multiple disks   RAID 5: Distributed Parity  Spread data \u0026amp; parity across all disks Prevent over use of a single disk  Read BW increases by N times, because all four disks can serve a read request write BW:  Method 1: (1) read out all unmodified (N-2) data bits (2) re-compute parity bit (3) write both modified bit and parity bit to disks (BW = N/(N-2+2) = 1) remains the same Method 2: (1) only read the parity bit and modified bit. (2) re-compute parity bit by the difference. (3) write both modified bit and parity bit (BW = N/(2+2) = N/4 times faster)     RAID 6: P + Q Dual Parity Redundancy  Like RAID 5, but stores extra redundant information to guard against multiple disk failure Use Ecc code (i.e. Error correction code) instead of single parity bit Parity bits are also striped across disks   Hybird RAID  RAID 0+1: Stripe then replicate RAID 1+0: Replicate then stripe  First level control by a controller. Therefore, RAID 10 has better fault tolerance than RAID01 when multiple disk failes  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"586aec3089a261a12b3f07c2fbb9794c","permalink":"/courses/operating_system/mass_storage_system/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/mass_storage_system/","section":"courses","summary":"Disk Structure  Disk drives are addressed as large 1-dim array of logical blocks (logical block: smallest unit of transfer(sector)) Logical blocks are mapped onto disk sequentially  Sector 0: 1st sector of 1st track on the outermost cyl go from outermost cylinder to innermost one   Disk drive attached to a computer by an I/O bus  Sectors Per Track  Constant linear velocity (CLV)  density of bits per track is uniform more sectors on a track in outer cylinders keeping same data rate increase rotation speed in inner cylinders applications: CD-ROM and DVD-ROM   Constant angular velocity (CAV)  keep same rotation speed larger bit density on inner tracks keep same data rate applications: hard disks    Disk Scheduling  Disk-access time has 3 major components  Seek time: move disk arm to the desired cylinder rotational latency: rotate disk head to the desired sector read time: constant transfer time   Disk bandwidth: number of bytes transferred / (complete of last req - start of first req)  Minimize seek time illustrate with a request queue(0-199) (98, 183, 37, 122, 14, 124, 65, 67)  Algorithm  FCFS (First come first served) SSTF(Shortest-seek-time-first)  SSTF scheduling is a form of SJF scheduling; may cause starvation of some requests total head movement: 236 cylinders common and has a natural appeal, but not optimal   SCAN scheduling  disk head move from one end to the other end A.","tags":null,"title":"Mass Storage System","type":"docs"},{"authors":null,"categories":null,"content":"Overview  The two main jobs of a computer I/O and computation I/O devices: tape, HD, mouse, joystick, screen I/O subsytems: the methods to control all I/O devices Two conflicting trends  Standardization of HW/SW interfaces Board variety of I/O devices   Device drivers: a uniform device-access interface to the I/O subsystem(Simliar to system calls between apps and OS) I/O Hardware  Port: A connection point between I/O diveces and the host(E.g. USB ports) Bus: A set of wires and a well-defined protocal that specifies message sent over the wires (E.g. PCI bus) Controller: A collecton of electronics that can operate a port, a bus, or a device (A controller could have its own processor, memory, etc(E.g. SCSI controller))    Basic I/O method (Port-mapped I/O)  Each I/O port (device) is identified by a unique port address Each I/O port consists of four registers (1~4 Bytes)  Data-in regsiter: read by the host to get input Data-out register: written by the host to send output status register: read by the host to check I/O status Control register: written by the host to control the device   Program interact with an I/O port through special I/O instructions (differnet from memory access)  I/O methods Categorization  Depends on how to address a deive  port-mapped I/O: use different address space from memory, access by special I/O instruction(e.g. IN, OUT) Memory-mapped I/O: Reserve specific memory space for device, Access by standard data-transfer instruction (e.g. MOV) (more efficient for large memory I/O, vulnerable to accidental modification)   Depends on how to interact with a deivce:  Poll(busy-waiting): processor periodically check status register of a device Interrupt: device notify processor of its completion   Depending on who to control the transer:  Programmed I/O: transfer controlled by CPU Direct memory access(DMA) I/O: controlled by DMA controller(A special purpose controller, design for large data transfer)     Kernel I/O Subsystem  I/O scheduling \u0026ndash; improve system performance by ordering the jobs in I/O queue (e.g. disk I/O order scheduling) Buffering \u0026ndash; store data in memory while transferring between I/O deivces  Speed mismatch between devices Devices with different data-transfer sizes Support copy semantics   Caching \u0026ndash; fast momory that holds copies of data (Key to performance) Spooling \u0026ndash; holds output for a device(e.g. printing, cannot accept interleaved files) Error handlig \u0026ndash; when I/O error happens(e.g. SCSI device returns error information) I/O protection(privileged instructions)  Blocking and Nonblocking I/O  Blocking \u0026ndash; process suspended until I/O completed  Easy to use and understand Insufficient for some needs Use for synchronous communication \u0026amp; I/O   Nonblocking  Implemented via multi-threading Returns quickly with count of bytes read or wriiten Use for asynchronous communication \u0026amp; I/O    Transforming I/O requests to Hardware operations Performance and Improving  I/O is a major factor in system performance  It placs heavy demancs on the CPU to execute device driver code The resulting context switches stress the CPU and its hareware caches I/O loads dwon the memory bus during data copy bewteen controllers and physcial memory Interrupt handling is a relatively expensive task Busy-waiting could be more efficient than interrupt-driven if I/O time is small   Improving Performance  Reduce number of context switches Reduce data copying Reduce interrupts by using large transers, smart controllers, polling Use DMA Balance CPU, memory, bus and I/O performance for highest throughput    Application I/O interface  Device drivers: a uniform device-access interface to the I/O subsystem; hide the differences among device controllers from the I/O sub-system of OS  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"687abf32fd582b5ca914e102b97ccb51","permalink":"/courses/operating_system/io_system/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/operating_system/io_system/","section":"courses","summary":"Overview  The two main jobs of a computer I/O and computation I/O devices: tape, HD, mouse, joystick, screen I/O subsytems: the methods to control all I/O devices Two conflicting trends  Standardization of HW/SW interfaces Board variety of I/O devices   Device drivers: a uniform device-access interface to the I/O subsystem(Simliar to system calls between apps and OS) I/O Hardware  Port: A connection point between I/O diveces and the host(E.","tags":null,"title":"I/O system","type":"docs"},{"authors":[],"categories":[],"content":"Code //\r// Created by miao on 2020/7/18.\r//\r#ifndef CPPTRAJ_ACTION_FRACTIONNATIVECONTACTS_H\r#define CPPTRAJ_ACTION_FRACTIONNATIVECONTACTS_H\r#include \u0026lt;map\u0026gt;\r#include \u0026quot;Action.h\u0026quot;\r#include \u0026quot;ImagedAction.h\u0026quot;\rclass Action_FractionNativeContacts : public Action {\rpublic:\rAction_FractionNativeContacts(): debug_(0), first_(false){};\rDispatchObject* Alloc() const override{return (DispatchObject*) new Action_FractionNativeContacts();}\rvoid Help() const override;\rprivate:\rtypedef std::vector\u0026lt;int\u0026gt; Iarray;\rtypedef std::map\u0026lt;std::pair\u0026lt;int, int\u0026gt;, double\u0026gt; DistInfo;\rAction::RetType Init(ArgList\u0026amp;, ActionInit\u0026amp;, int) override;\rAction::RetType Setup(ActionSetup\u0026amp;) override;\rAction::RetType DoAction(int, ActionFrame\u0026amp;) override;\rint SetupContactLists(Topology const\u0026amp;, Frame const\u0026amp; );\rint DetermineNativeContacts(Topology const\u0026amp;, Frame const\u0026amp;);\rbool ValidContact (int a1, int a2, Topology const\u0026amp; parmIn) const{\rreturn abs(parmIn[a1].ResNum() - parmIn[a2].ResNum()) \u0026gt;= resoffset_;\r}\rvoid Print(){}\rDistInfo nativeContact_; ///\u0026lt; hold nativeContact information\rDataSet* qfrac_; ///\u0026lt; hold qfract value\rint resoffset_; ///\u0026lt; only calculate the fraction of native contact spaced this far apart\rdouble distCutoff_; ///\u0026lt; cutoff in the formula, default value is 4.5 A\rdouble lambda_; ///\u0026lt; lambda in the formula, default value is 1.8\rdouble beta_; ///\u0026lt; beta in the formula, default value is 5\runsigned int nframes_; ///\u0026lt; Number of frames\runsigned int nNativeContacts_; ///\u0026lt; Number of native contacts\rint debug_; ///\u0026lt; action debug level\rbool first_; ///\u0026lt; using first frame as reference frame or not\rImagedAction image_; ///\u0026lt; Holding imaging-related info/routines\rAtomMask Mask_; ///\u0026lt; Only calculate the residue in the mask;\rIarray contactIdx1_; ///\u0026lt; Hold atom/residue indicies for Mask\rTopology* CurrentParm_;\rMatrix_3x3 ucell_, recip_;\r};\r#endif //CPPTRAJ_ACTION_FRACTIONNATIVECONTACTS_H\r //\r// Created by miao on 2020/7/18.\r//\r#include \u0026quot;Action_FractionNativeContacts.h\u0026quot;\r#include \u0026quot;CpptrajStdio.h\u0026quot;\rvoid Action_FractionNativeContacts::Help() const {\rmprintf(\u0026quot;\\t[\u0026lt;mask1\u0026gt;][out \u0026lt;filename\u0026gt;][cutoff \u0026lt;cut\u0026gt;][noimage]\\n\u0026quot;\r\u0026quot;\\t[ first | %s ]\\n\u0026quot;\r\u0026quot;\\t[resoffset \u0026lt;n\u0026gt;] [name \u0026lt;dsname\u0026gt;] [beta \u0026lt;beta\u0026gt;] [lambda \u0026lt;lambda\u0026gt;]\\n\u0026quot;\r\u0026quot; Calculate Fraction of native contact (Q) in \u0026lt;mask1\u0026gt; \\n\u0026quot;\r\u0026quot; distance cutoff (4.5 Ang. default), only calculate residue index larger than \\n\u0026quot;\r\u0026quot; offset, default beta is 5 Ang-1, and lambda is 1.8 for all-atom simulation \\n\u0026quot;\r\u0026quot; Equation: Q(X) = 1 / |S| \\\\sum_{i, j \\\\in S} \\\\frac {1}{1+exp(\\\\beta(r_ij(X) - \\\\lambda r_{ij}^0)}\\n\u0026quot;,\rDataSetList::RefArgs);\r}\rAction::RetType Action_FractionNativeContacts::Init(ArgList\u0026amp; actionArgs, ActionInit\u0026amp; init, int debugIn) {\rdebug_ = debugIn;\rdouble dist_ = actionArgs.getKeyDouble(\u0026quot;cutoff\u0026quot;, 4.5);\rresoffset_ = actionArgs.getKeyInt(\u0026quot;resoffset\u0026quot;, 4); // greater or equal than 4\rbeta_ = actionArgs.getKeyDouble(\u0026quot;beta\u0026quot;, 5);\rlambda_ = actionArgs.getKeyDouble(\u0026quot;lambda\u0026quot;, 1.8);\rimage_.InitImaging(!(actionArgs.hasKey(\u0026quot;noimage\u0026quot;)));\rif(resoffset_ \u0026lt; 1) {\rmprinterr(\u0026quot;Error: Residue offset must be \u0026gt;=0 \\n\u0026quot;);\rreturn Action::ERR;\r}\rdistCutoff_ = dist_ * dist_ ; // square cutoff\rDataFile* outfile = init.DFL().AddDataFile(actionArgs.GetStringKey(\u0026quot;out\u0026quot;), actionArgs);\rReferenceFrame REF = init.DSL().GetReferenceFrame(actionArgs);\rif(!first_) {\rif (REF.error()) return Action::ERR;\rif (REF.empty()) {\rmprintf(\u0026quot;Warinng: No reference structure specified. Defaulting to first.\\n\u0026quot;);\rfirst_ = true;\r}\r} else {\rif(!REF.empty()) {\rmprinterr(\u0026quot;Error: Must only specify 'first' or a reference structure, not both.\\n\u0026quot;);\rreturn Action::ERR;\r}\r}\rstd::string name = actionArgs.GetStringKey(\u0026quot;name\u0026quot;);\rif(name.empty())\rname = init.DSL().GenerateDefaultName(\u0026quot;FractionNativeContact\u0026quot;);\rqfrac_ = init.DSL().AddSet(DataSet::DOUBLE, MetaData(name, \u0026quot;QFraction\u0026quot;));\rif(outfile != nullptr) {\routfile -\u0026gt; AddDataSet(qfrac_);\r}\rif(Mask_.SetMaskString(actionArgs.GetMaskNext())) return Action::ERR;\rmprintf(\u0026quot; FractionOfNativeContactS: Mask = '%s'\\n\u0026quot;, Mask_.MaskString());\rif(!image_.UseImage())\rmprintf(\u0026quot; imaging is off.\\n\u0026quot;);\relse\rmprintf(\u0026quot; imaging is on. \\n\u0026quot;);\rif(outfile != nullptr)\rmprintf(\u0026quot;\\tOutput to '%s'\\n\u0026quot;, outfile-\u0026gt;DataFilename().full());\rmprintf(\u0026quot;\\tNative Contacts will be ignored for residue spaced \u0026lt; %i apart. \\n\u0026quot;, resoffset_);\rmprintf(\u0026quot;\\tNative Contacts will be ignored when distance \u0026lt; %g Angstroms\\n\u0026quot;, dist_);\rmprintf(\u0026quot;\\tBeta is set to %g, and Lambda is set to %g\\n\u0026quot;, beta_, lambda_);\rif(!first_) {\rimage_.SetupImaging(REF.CoordsInfo().TrajBox().Type());\rif(image_.ImageType()==NONORTHO)\rREF.Coord().BoxCrd().ToRecip(ucell_, recip_);\rif(DetermineNativeContacts( REF.Parm(), REF.Coord() )) return Action::ERR;\r}\rreturn Action::OK;\r}\rAction::RetType Action_FractionNativeContacts::Setup(ActionSetup \u0026amp; setup) {\rif(SetupContactLists(setup.Top(), Frame()))\rreturn Action::SKIP;\rmprintf(\u0026quot;\\t%i potential contact sites for '%s'\\n\u0026quot;, Mask_.Nselected(), Mask_.MaskString());\rimage_.SetupImaging(setup.CoordInfo().TrajBox().Type());\rif(image_.ImagingEnabled())\rmprintf(\u0026quot;\\tImaging enabled.\\n\u0026quot;);\relse\rmprintf(\u0026quot;\\tImaging disabled.\\n\u0026quot;);\rCurrentParm_ = setup.TopAddress();\rreturn Action::OK;\r}\rAction::RetType Action_FractionNativeContacts::DoAction(int frameNum, ActionFrame \u0026amp; frm) {\rif(image_.ImageType() == NONORTHO) frm.Frm().BoxCrd().ToRecip(ucell_, recip_);\rif(first_) {\rmprintf(\u0026quot;\\tUsing first frame to determine native contacts.\\n\u0026quot;);\rif(DetermineNativeContacts(*CurrentParm_, frm.Frm())) return Action::ERR;\rfirst_ = false;\r}\rnframes_++;\rdouble sumDist = 0;\rfor(int c1 = 0; c1 != Mask_.Nselected(); ++c1){\rfor(int c2 = c1 + 1; c2 != Mask_.Nselected(); ++c2) {\rif(nativeContact_.find({Mask_[c1], Mask_[c2]}) != nativeContact_.end()) {\rdouble Dist2 = DIST2(frm.Frm().XYZ(Mask_[c1]), frm.Frm().XYZ(Mask_[c2]),\rimage_.ImageType(), frm.Frm().BoxCrd(), ucell_, recip_);\rsumDist += 1.0 / (1 + exp(beta_ * (sqrt(Dist2) - lambda_ *\rnativeContact_[{Mask_[c1], Mask_[c2]}]) ) );\r}\r}\r}\rsumDist /= nNativeContacts_;\rqfrac_-\u0026gt;Add(frameNum, \u0026amp;sumDist);\rreturn Action::OK;\r}\rint Action_FractionNativeContacts::SetupContactLists(const Topology \u0026amp;parmIn, const Frame \u0026amp;frameIn) {\rif(parmIn.SetupIntegerMask(Mask_, frameIn)) return 1;\rMask_.MaskInfo();\rif(Mask_.None()) {\rmprinterr(\u0026quot;Warning: Noting selected for '%s' \\n\u0026quot;, Mask_.MaskString()) ;\rreturn 1;\r}\rfor(int atom : Mask_)\rcontactIdx1_.push_back(atom);\rreturn 0;\r}\rint Action_FractionNativeContacts::DetermineNativeContacts(const Topology \u0026amp; parmIn, const Frame \u0026amp; fIn) {\rif(SetupContactLists(parmIn, fIn)) return 1;\rfor(auto c1 = Mask_.begin(); c1 != Mask_.end(); ++c1)\rfor(auto c2 = c1+1; c2!=Mask_.end(); ++c2) {\rif(ValidContact(*c1, *c2, parmIn)) {\rdouble Dist2 = DIST2(fIn.XYZ(*c1), fIn.XYZ(*c2), image_.ImageType(), fIn.BoxCrd(), ucell_, recip_);\rif(Dist2 \u0026lt; distCutoff_) {\rnativeContact_.insert({{*c1, *c2}, sqrt(Dist2)});\r}\r}\r}\rnNativeContacts_ = nativeContact_.size();\rmprintf(\u0026quot;\\tSetup %zu native contacts:\\n\u0026quot;, nNativeContacts_);\rif(debug_ \u0026gt; 0)\rfor (auto \u0026amp; contact : nativeContact_)\r{\rint a1 = contact.first.first;\rint a2 = contact.first.second;\rmprintf(\u0026quot;\\t\\tAtom '%s' to '%s' and dist %g\\n\u0026quot;, parmIn.AtomMaskName(a1).c_str(),\rparmIn.AtomMaskName(a2).c_str(), contact.second);\r}\rreturn 0;\r}\r ","date":1598930700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598930700,"objectID":"703e7fa0ca575c64a08e5919398985b5","permalink":"/post/fraction_nativecontact/","publishdate":"2020-09-01T11:25:00+08:00","relpermalink":"/post/fraction_nativecontact/","section":"post","summary":"Calculate Fraction of Native Contacts","tags":["Trajectory","CPPTRAJ"],"title":"Fraction of native Contact","type":"post"},{"authors":[],"categories":[],"content":"Code //\r// Created by miao on 2020/8/7.\r//\r#ifndef CPPTRAJ_ACTION_HELIXANGLE_H\r#define CPPTRAJ_ACTION_HELIXANGLE_H\r#include \u0026lt;vector\u0026gt;\r#include \u0026quot;Action.h\u0026quot;\rclass Action_HelixAngle : public Action{\rpublic:\rAction_HelixAngle(): HelixAngle_(nullptr), BB_C_(\u0026quot;C\u0026quot;), BB_O_(\u0026quot;O\u0026quot;), BB_CA_(\u0026quot;CA\u0026quot;), BB_N_(\u0026quot;N\u0026quot;),\rcutOff_{1.5}, debug_(0), method_{ORIENTATION}{};\rDispatchObject* Alloc() const override {return (DispatchObject* )new Action_HelixAngle();}\rvoid Help() const override;\rprivate:\rAction::RetType Init(ArgList\u0026amp;, ActionInit\u0026amp;, int) override;\rAction::RetType Setup(ActionSetup\u0026amp;) override;\rAction::RetType DoAction(int, ActionFrame\u0026amp;) override;\rvoid Print() override {};\renum MethodType {ORIENTATION=0, ORIENTATION_HBOND, LOOP_ORIENTATION, CAFIT_ORIENTATION};\rclass ResidueInfo{\rpublic:\rResidueInfo(): C_(-1), CA_(-1), N_(-1), O_(-1), num_(-1){};\rint C() const {return C_;}\rint CA() const {return CA_;}\rint N() const {return N_;}\rint O() const {return O_;}\rint Num() const {return num_;}\rvoid SetC(int i) {C_ = i;}\rvoid SetCA(int i) {CA_ = i;}\rvoid SetN(int i) {N_ = i;}\rvoid SetO(int i) {O_ = i;}\rbool hasCO() const {return C_ !=-1 \u0026amp;\u0026amp; O_ != -1;}\rbool hasNO() const {return N_ !=-1 \u0026amp;\u0026amp; O_ != -1;}\rbool hasCN() const {return C_ !=-1 \u0026amp;\u0026amp; N_ != -1;}\rprivate:\rint C_;\rint CA_;\rint N_;\rint O_;\rint num_;\r};\rtypedef std::vector\u0026lt;ResidueInfo\u0026gt; ResidueInfoVector;\rResidueInfoVector MaskToResArray(Topology const\u0026amp;, AtomMask const\u0026amp;) ;\rVec3 HelixComputeDirection(ActionFrame const\u0026amp;, ResidueInfoVector const\u0026amp;);\rVec3 calculateDirectionWithCutOff(std::vector\u0026lt;Vec3\u0026gt;\u0026amp; vec3Vector) const;\rAtomMask Mask1_; ///\u0026lt; Mask 1 of helix residue\rAtomMask Mask2_; ///\u0026lt; Mask 2 of helix residue\rDataSet* HelixAngle_; ///\u0026lt; Dataset Hold Helix Angle;\rstd::vector\u0026lt;ResidueInfo\u0026gt; Residue1_; ///\u0026lt; Residue index which Mask1 contains\rstd::vector\u0026lt;ResidueInfo\u0026gt; Residue2_; ///\u0026lt; Residue index which Mask2 contains\rNameType BB_C_; ///\u0026lt; Protein N Atom Name ('C')\rNameType BB_O_; ///\u0026lt; Protein N Atom Name ('O')\rNameType BB_CA_; ///\u0026lt; Protein CA Atom Name ('CA')\rNameType BB_N_; ///\u0026lt; Protein N Atom Name ('N')\rdouble cutOff_; ///\u0026lt; Drop outliers that is greater than standard deviation * cutOff\rint debug_; ///\u0026lt; Action Debug level\rMethodType method_; ///\u0026lt; Choose which method to calculate Orientation\r};\r#endif //CPPTRAJ_ACTION_HELIXANGLE_H\r //\r// Created by miao on 2020/8/7.\r//\r#include \u0026quot;Action_HelixAngle.h\u0026quot;\r#include \u0026quot;CpptrajStdio.h\u0026quot;\r#include \u0026quot;VectorMath.h\u0026quot;\r#include \u0026quot;DistRoutines.h\u0026quot;\r#include \u0026quot;DataSet_Vector.h\u0026quot;\rvoid Action_HelixAngle::Help() const {\rmprintf(\u0026quot;\\t[\u0026lt;name\u0026gt;] \u0026lt;mask1\u0026gt; \u0026lt;mask2\u0026gt; [out \u0026lt;filename\u0026gt;] [method \u0026lt;method\u0026gt;]\\n\u0026quot;\r\u0026quot; Calculate the helix angle between mask1 and mask2 \\n\u0026quot;\r\u0026quot; reference to PyMOl anglebetweenhelices script\\n\u0026quot;\r\u0026quot; mask1 and mask2 must contain more than 4 residues\\n\u0026quot;\r\u0026quot; there are four methods could be used: \\n\u0026quot;\r\u0026quot; Method: 0, Average direction C(i) -\u0026gt; O(i) pseudo bonds; \\n\u0026quot;\r\u0026quot; 1, Average direction O(i) -\u0026gt; N(i+4) hydrogen bonds; \\n\u0026quot;\r\u0026quot; 2, Average direction N(i) -\u0026gt; C(i) pseudo bonds; \\n\u0026quot;\r\u0026quot; 3, least square linear fit on CA atoms\\n\u0026quot;);\r}\rAction::RetType Action_HelixAngle::Init(ArgList\u0026amp; actionArgs, ActionInit \u0026amp;init, int debugIn) {\rdebug_ = debugIn;\rDataFile* outfile = init.DFL() .AddDataFile(actionArgs.GetStringKey(\u0026quot;out\u0026quot;), actionArgs);\rmethod_ = static_cast\u0026lt;MethodType\u0026gt;(actionArgs.getKeyInt(\u0026quot;method\u0026quot;, 0));\rstd::string mask1 = actionArgs.GetMaskNext();\rstd::string mask2 = actionArgs.GetMaskNext();\rif (mask1.empty() || mask2.empty()) {\rmprinterr(\u0026quot;Error: helixAngle: Require 2 masks \\n\u0026quot;);\rreturn Action::ERR;\r}\rif(Mask1_.SetMaskString(mask1)) return Action::ERR;\rif(Mask2_.SetMaskString(mask2)) return Action::ERR;\rmprintf(\u0026quot;ANGLE: between helix %s - helix %s \\n\u0026quot;, Mask1_.MaskString(), Mask2_.MaskString());\rHelixAngle_ = init.DSL().AddSet(DataSet::DOUBLE,\rMetaData(actionArgs.GetStringNext(), MetaData::M_ANGLE), \u0026quot;Ang\u0026quot;);\rif(!HelixAngle_) return Action::ERR;\rif(outfile) outfile-\u0026gt;AddDataSet(HelixAngle_);\rreturn Action::OK;\r}\rAction::RetType Action_HelixAngle::Setup(ActionSetup \u0026amp; setup) {\rif(setup.Top().SetupIntegerMask(Mask1_)) return Action::ERR;\rif(setup.Top().SetupIntegerMask(Mask2_)) return Action::ERR;\rResidue1_ = MaskToResArray(setup.Top(), Mask1_);\rResidue2_ = MaskToResArray(setup.Top(), Mask2_);\rif(method_ == ORIENTATION) cutOff_=1.5;\relse if(method_ == ORIENTATION_HBOND) cutOff_=3.5;\relse cutOff_ = 0;\rmprintf(\u0026quot;\\t\u0026quot;);\rMask1_.BriefMaskInfo();\rMask2_.BriefMaskInfo();\rmprintf(\u0026quot;\\t\\n\u0026quot;);\rif (Residue1_.size() \u0026lt; 4) mprinterr(\u0026quot;Error: Mask1: Must contain at least four residues\u0026quot;);\rif (Residue2_.size() \u0026lt; 4) mprinterr(\u0026quot;Error: Mask2: Must contain at least four residues\u0026quot;);\rreturn Action::OK;\r}\rAction::RetType Action_HelixAngle::DoAction(int frameNum, ActionFrame \u0026amp; frm) {\rVec3 dir1 = HelixComputeDirection(frm, Residue1_) ;\rVec3 dir2 = HelixComputeDirection(frm, Residue2_) ;\rif((dir1[0] == 0 \u0026amp;\u0026amp; dir2[1] == 0 \u0026amp;\u0026amp; dir1[2] == 0) || (dir2[0] == 0 \u0026amp;\u0026amp; dir2[1] == 0 \u0026amp;\u0026amp; dir2[2] == 0)) {\rmprintf(\u0026quot;\\t Can't calculate the orientation for %i as contacts is zero \\n\u0026quot;, frameNum);\rreturn Action::SKIP;\r}\rdouble ang = dir1.Angle(dir2);\rang *= Constants::RADDEG;\rif (debug_ \u0026gt; 0) {\rdir1.Print(\u0026quot;dir1\u0026quot;);\rdir2.Print(\u0026quot;dir2\u0026quot;);\rmprintf(\u0026quot;Angle: %g\\n\u0026quot;, ang);\r}\rHelixAngle_-\u0026gt;Add(frameNum, \u0026amp;ang);\rreturn Action::OK;\r}\rstd::vector\u0026lt;Action_HelixAngle::ResidueInfo\u0026gt;\rAction_HelixAngle::MaskToResArray(const Topology \u0026amp; currentParm,\rconst AtomMask \u0026amp; mask)\r{\rint currentResNum = -1;\rstd::vector\u0026lt;ResidueInfo\u0026gt; residueInfoVector;\rfor(const auto\u0026amp; atom: mask){\rint resNum = currentParm[atom].ResNum();\rif (resNum != currentResNum) {\rResidue const\u0026amp; thisRes = currentParm.Res(resNum);\rResidueInfo resInfo;\rif(method_ == ORIENTATION) // only save C and O atom;\r{\rfor(int at = thisRes.FirstAtom(); at != thisRes.LastAtom(); at++) {\rif(currentParm[at].Name() == BB_C_) resInfo.SetC(at);\rif(currentParm[at].Name() == BB_O_) resInfo.SetO(at);\r}\r}\relse if(method_ == ORIENTATION_HBOND) { // save O and N atom\rfor(int at = thisRes.FirstAtom(); at != thisRes.LastAtom(); at++) {\rif(currentParm[at].Name() == BB_O_) resInfo.SetO(at);\rif(currentParm[at].Name() == BB_N_) resInfo.SetN(at);\r}\r}\relse if(method_ == LOOP_ORIENTATION) { // Save C and N atom\rfor(int at = thisRes.FirstAtom(); at != thisRes.LastAtom(); at++) {\rif(currentParm[at].Name() == BB_C_) resInfo.SetC(at);\rif(currentParm[at].Name() == BB_N_) resInfo.SetN(at);\r}\r}\relse if(method_ == CAFIT_ORIENTATION) { // Save CA atom\rfor(int at = thisRes.FirstAtom(); at != thisRes.LastAtom(); at++) {\rif (currentParm[at].Name() == BB_CA_) resInfo.SetCA(at);\r}\r}\rcurrentResNum = resNum;\rresidueInfoVector.push_back(resInfo);\r}\r}\rreturn residueInfoVector;\r}\rVec3 Action_HelixAngle::HelixComputeDirection(const ActionFrame\u0026amp; frm, const ResidueInfoVector\u0026amp; Residue) {\rint residueNum = Residue.size();\rDataSet_Vector CaData;\rif(method_ == CAFIT_ORIENTATION) {\rfor(auto\u0026amp; residueInfo: Residue) {\rif(residueInfo.CA() != -1) {\rCaData.AddVxyz(frm.Frm().XYZ(residueInfo.CA()));\r}\r}\rVec3 fit = CaData.LinearFit();\rif(fit * (*(CaData.end()-1) - *CaData.begin()) \u0026lt; 0)\rfit.Neg();\rreturn fit;\r}\rstd::vector\u0026lt;Vec3\u0026gt; vec3Vector;\rif (method_ == ORIENTATION) {\rfor (auto \u0026amp;residueInfo: Residue) {\rif (residueInfo.hasCO()) {\rVec3 v1(frm.Frm().XYZ(residueInfo.C()));\rVec3 v2(frm.Frm().XYZ(residueInfo.O()));\rvec3Vector.push_back(v1 - v2);\r}\r}\r} else if (method_ == ORIENTATION_HBOND) {\rfor (int i = 0; i \u0026lt; residueNum - 4; i++) {\rif (Residue[i].O() != -1 \u0026amp;\u0026amp; Residue[i + 4].N() != -1) {\rVec3 v1(frm.Frm().XYZ(Residue[i].O()));\rVec3 v2(frm.Frm().XYZ(Residue[i + 4].N()));\rif (DIST2_NoImage(v1, v2) \u0026lt; cutOff_ * cutOff_)\rvec3Vector.push_back(v1 - v2);\r}\r}\r} else if (method_ == LOOP_ORIENTATION) {\rfor (auto \u0026amp;residueInfo: Residue) {\rif (residueInfo.hasCN()) {\rVec3 v1(frm.Frm().XYZ(residueInfo.C()));\rVec3 v2(frm.Frm().XYZ(residueInfo.N()));\rvec3Vector.push_back(v1 - v2);\r}\r}\r}\rreturn calculateDirectionWithCutOff(vec3Vector);\r}\rVec3 Action_HelixAngle::calculateDirectionWithCutOff(std::vector\u0026lt;Vec3\u0026gt;\u0026amp; vec3Vector) const {\rVec3 vec3Sum;\rstd::vector\u0026lt;double\u0026gt; angles;\rint residueNum = vec3Vector.size();\rif(residueNum == 0) return {0,0,0};\rfor (auto \u0026amp;vec3: vec3Vector) {\rvec3Sum += vec3;\r}\rif (method_ == ORIENTATION) {\rfor (auto \u0026amp;vec3: vec3Vector) {\rdouble angle = vec3Sum.NoNormAngle(vec3);\rangles.push_back(angle);\r}\rauto meanStd = MeanAndStd(angles);\rvec3Sum.Zero();\rfor (int i = 0; i \u0026lt; residueNum; ++i) {\rif (std::abs(angles[i] - meanStd.first) \u0026lt; cutOff_ * meanStd.second) {\rvec3Sum += vec3Vector[i];\r} else {\rif(debug_ \u0026gt; 0)\rmprintf(\u0026quot;remove angle %g for it's a outlier. \\n\u0026quot;, angles[i]);\r}\r}\r}\rvec3Sum.Normalize();\rreturn vec3Sum;\r}\r ","date":1598930549,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598930549,"objectID":"9b20db935cd21964db9252e7b72c1be4","permalink":"/post/helixangle/","publishdate":"2020-09-01T11:22:29+08:00","relpermalink":"/post/helixangle/","section":"post","summary":"Calcualte Anlge between two helixes","tags":["Trajectory","CPPTRAJ"],"title":"Angle between two helixes","type":"post"},{"authors":[],"categories":[],"content":"Code //\r// Created by miao on 2020/8/30.\r//\r#ifndef CPPTRAJ_ACTION_ERMSD_H\r#define CPPTRAJ_ACTION_ERMSD_H\r#include \u0026lt;vector\u0026gt;\r#include \u0026quot;Action.h\u0026quot;\r#include \u0026quot;ReferenceAction.h\u0026quot;\r#include \u0026quot;DataSet_MatrixDbl.h\u0026quot;\rclass Action_Ermsd : public Action {\rpublic:\rAction_Ermsd();\rDispatchObject* Alloc() const override {return (DispatchObject*) new Action_Ermsd();}\rvoid Help() const override;\rprivate:\rAction::RetType Init(ArgList\u0026amp;, ActionInit\u0026amp;, int) override;\rAction::RetType Setup(ActionSetup\u0026amp;) override;\rAction::RetType DoAction(int, ActionFrame\u0026amp;) override;\rvoid Print() override{};\rRange resRange_; ///\u0026lt; Residue range to calculate eRMSD\rDataSet* ermsd_; ///\u0026lt; hold ermsd Data\rdouble cutoff_; ///\u0026lt; CUTOFF, default is 2.4 A ,Larger values of cutoff can be useful\r///\u0026lt; when analyzing unstructured/flexible molecules.\rint debug_; ///\u0026lt; debug level\rint nres_; ///\u0026lt; number of residue (base)\rbool first_; ///\u0026lt; if true, use the first structure as reference\rReferenceFrame REF_; ///\u0026lt; Reference frame to calcuate GRefMat\rtypedef std::vector\u0026lt;std::vector\u0026lt;std::vector\u0026lt;double\u0026gt;\u0026gt;\u0026gt; vvv;\rvvv GRefMat_, GActionMat_; ///\u0026lt; G matrix for Reference and Action, has shape nres_ * nres_ * 4\rVec3 F_factors{5, 5, 3}; ///\u0026lt; anisotropic position vector\rclass BaseInfo {\rpublic:\renum BaseType {None = 0, Purine, Pyrimidine};\rBaseInfo(): C2_(-1), C4_(-1), C6_(-1), idx_(-1), type_(None){}\rBaseInfo(int idx1, int idx2, int idx3, int idx4, BaseType ba):\rC2_(idx1),\rC4_(idx2),\rC6_(idx3),\ridx_(idx4),\rtype_(ba){}\rbool checkComplete() const {\rreturn (C2_ != -1) \u0026amp;\u0026amp; (C4_ != -1) \u0026amp;\u0026amp; (C6_ != -1) \u0026amp;\u0026amp; (idx_ != -1);\r}\rbool operator==(const BaseInfo\u0026amp; rhs) const {\rreturn type_ == rhs.type_ \u0026amp;\u0026amp; checkComplete() \u0026amp;\u0026amp; rhs.checkComplete();\r}\rvoid SetC2(int C2){C2_=C2;}\rvoid SetC4(int C4){C4_=C4;}\rvoid SetC6(int C6){C6_=C6;}\rvoid SetResIdx(int idx){idx_=idx;}\rvoid SetType(BaseType ba){type_=ba;}\rint C2() const {return C2_;}\rint C4() const {return C4_;}\rint C6() const {return C6_;}\rint ResIdx() const {return idx_;}\rBaseType type() const {return type_;}\rprivate:\rint C2_;\rint C4_;\rint C6_;\rint idx_{};\rBaseType type_;\r};\rstd::vector\u0026lt;BaseInfo\u0026gt; vecRefBase_; ///\u0026lt; contain baseinfo for reference\rstd::vector\u0026lt;BaseInfo\u0026gt; vecCalcBase_; ///\u0026lt; contain baseinfo for action frame\rint SetupReference(const Topology\u0026amp;, std::vector\u0026lt;BaseInfo\u0026gt;\u0026amp; );\rvoid calcGMat(const Frame\u0026amp;, vvv\u0026amp;);\r};\r#endif\r //\r// Created by miao on 2020/8/30.\r// Reference The role of nucleobase interactions in RNA structure and dynamics, Nucleic Acids Research,\r// 2014, Vol. 42, No. 21, doi: 10.1093/nar/gku972\r//\r#include \u0026lt;cmath\u0026gt;\r#include \u0026quot;Action_Ermsd.h\u0026quot;\r#include \u0026quot;CpptrajStdio.h\u0026quot;\rAction_Ermsd::Action_Ermsd():ermsd_(nullptr),cutoff_(0),debug_(0),nres_(0),first_(false){\r}\rvoid Action_Ermsd::Help() const {\rmprintf(\u0026quot;\\t[\u0026lt;name\u0026gt;] [resrange \u0026lt;range\u0026gt;] [out \u0026lt;filename\u0026gt;]\\n\u0026quot;\r\u0026quot;\\t[cutoff \u0026lt;cutoff\u0026gt;]\\n%s\u0026quot;,\rReferenceAction::Help());\rmprintf(\u0026quot; Calculate nucleic acid root-mean-squared deviation of atoms in \u0026lt;mask\u0026gt;\\n\u0026quot;);\r}\rAction::RetType Action_Ermsd::Init(ArgList \u0026amp; actionArgs, ActionInit \u0026amp;init, int debugIn) {\r// Initialize keyword\rdebug_ = debugIn;\rcutoff_ = actionArgs.getKeyDouble(\u0026quot;cutoff\u0026quot;, 2.4);\rDataFile* outfile = init.DFL() .AddDataFile(actionArgs.GetStringKey(\u0026quot;out\u0026quot;), actionArgs);\rfirst_ = actionArgs.hasKey(\u0026quot;first\u0026quot;);\r// Initialize Range\rresRange_.SetRange(actionArgs.GetStringKey(\u0026quot;resrange\u0026quot;));\rif(!resRange_.Empty())\rresRange_.ShiftBy(-1);\r// Initialize REF\rREF_ = init.DSL().GetReferenceFrame(actionArgs);\rif(!first_) {\rif (REF_.error()) return Action::ERR;\rif (REF_.empty()) {\rmprintf(\u0026quot;Warinng: No reference structure specified. Defaulting to first.\\n\u0026quot;);\rfirst_ = true;\r}\r} else {\rif(!REF_.empty()) {\rmprinterr(\u0026quot;Error: Must only specify 'first' or a reference structure, not both.\\n\u0026quot;);\rreturn Action::ERR;\r}\r}\rermsd_ = init.DSL().AddSet(DataSet::DOUBLE, MetaData(actionArgs.GetStringNext(),MetaData::M_DISTANCE),\r\u0026quot;ermsd\u0026quot;);\rif(!ermsd_) return Action::ERR;\rif(outfile) outfile -\u0026gt; AddDataSet(ermsd_);\r// Output message\rif(outfile)\rmprintf(\u0026quot;\\tOutput to '%s'\\n\u0026quot;,outfile-\u0026gt;DataFilename().full());\rmprintf(\u0026quot;\\tCutoff is set to %g Angstroms\\n\u0026quot;, cutoff_);\rmprintf(\u0026quot;# Citations: Sandro Bottaro, Francesco Di Palma and Giovanni Bussi\\n\u0026quot;\r\u0026quot;# The role of nucleobase interactions in RNA structure and dynamics\\n\u0026quot;\r\u0026quot;# Nucleic Acid Research, 2014, 42, 21, doi: 10.1093/nar/gku972\\n\u0026quot;);\rreturn Action::OK;\r}\rAction::RetType Action_Ermsd::Setup(ActionSetup \u0026amp;setup) {\r// setup Reference Range\r//TODO: if resRange_ is not specified, then choose all the nucleic acid residues\r// as cpptraj dose not have function to decide this, I use the soluteResidues(),\r// this may cause problem. Most of time, please specificy correct resRange_.\rif(resRange_.Empty())\rresRange_ = setup.Top().SoluteResidues();\rif(resRange_.Empty()) {\rmprintf(\u0026quot;Waring: No residues specified for %s \\n\u0026quot;, setup.Top().c_str());\rreturn Action::SKIP;\r}\rresRange_.PrintRange(\u0026quot;\\tBase Range\u0026quot;, 1);\rmprintf(\u0026quot;\\n\u0026quot;);\rif(SetupReference(setup.Top(), vecCalcBase_)) return Action::ERR;\rif(first_) {\rif (SetupReference(setup.Top(), vecRefBase_))\rreturn Action::ERR;\r}\relse {\rif (SetupReference(REF_.Parm(), vecRefBase_))\rreturn Action::ERR;\r}\rif(vecRefBase_.empty() || vecCalcBase_.empty()) {\rmprinterr(\u0026quot;Reference or Current Parm contains no base\\n\u0026quot;);\r}\rif(vecCalcBase_ != vecRefBase_) {\rmprinterr(\u0026quot;Reference and Setup Parm does not have same base type\u0026quot;);\rreturn Action::ERR;\r}\rnres_ = vecRefBase_.size();\rmprintf(\u0026quot;\\n\\tERMSD will be calculated on %d bases\\n\u0026quot;, nres_);\rGRefMat_.resize(nres_, std::vector\u0026lt;std::vector\u0026lt;double\u0026gt;\u0026gt;(nres_, std::vector\u0026lt;double\u0026gt;(4, 0)));\rGActionMat_.resize(nres_, std::vector\u0026lt;std::vector\u0026lt;double\u0026gt;\u0026gt;(nres_, std::vector\u0026lt;double\u0026gt;(4, 0)));\rif(!first_)\rcalcGMat(REF_.Coord(), GRefMat_);\rreturn Action::OK;\r}\rAction::RetType Action_Ermsd::DoAction(int frameNum, ActionFrame \u0026amp;frm) {\rdouble res = 0;\rif(first_) {\rmprintf(\u0026quot;\\tUsing First Frame as a reference structure\\n\u0026quot;);\rcalcGMat(frm.Frm(), GRefMat_);\rfirst_ = false;\rermsd_-\u0026gt;Add(frameNum, \u0026amp;res);\rreturn Action::OK;\r}\rcalcGMat(frm.Frm(), GActionMat_);\rfor(int i = 0; i \u0026lt; nres_; ++i) {\rfor(int j = 0; j \u0026lt; nres_; ++j) {\rfor(int k = 0; k \u0026lt; 4; ++k) {\rres += (GRefMat_[i][j][k] - GActionMat_[i][j][k])* (GRefMat_[i][j][k] - GActionMat_[i][j][k]);\r}\r}\r}\r// debug GActionMat_;\r// for(int i = 0; i \u0026lt; 10; ++i) {\r// for (int j = 0; j \u0026lt; 10; ++j) {\r// std::cout \u0026lt;\u0026lt; GActionMat_[i][j][0] \u0026lt;\u0026lt; \u0026quot; \u0026quot;;\r// std::cout \u0026lt;\u0026lt; GActionMat_[i][j][1] \u0026lt;\u0026lt; \u0026quot; \u0026quot;;\r// std::cout \u0026lt;\u0026lt; GActionMat_[i][j][2] \u0026lt;\u0026lt; \u0026quot; \u0026quot;;\r// std::cout \u0026lt;\u0026lt; GActionMat_[i][j][3] \u0026lt;\u0026lt; \u0026quot; \u0026quot;;\r// }\r// std::cout \u0026lt;\u0026lt; \u0026quot;\\n\u0026quot;;\r// }\rres = sqrt(res / nres_);\rermsd_-\u0026gt;Add(frameNum, \u0026amp;res);\rreturn Action::OK;\r}\rint Action_Ermsd::SetupReference(const Topology \u0026amp; parmIn, std::vector\u0026lt;BaseInfo\u0026gt;\u0026amp; vecBase) {\rfor (auto resnum : resRange_){\rBaseInfo::BaseType basetype_;\rBaseInfo baseinfo;\r//TODO: decide a residue is protein or nucleic acid or ions\rconst auto\u0026amp; res = parmIn.Res(resnum);\rconst auto c = res.SingleCharName();\rswitch(c) {\rcase 'A': case 'G':\rbasetype_ = BaseInfo::BaseType::Purine;\rbreak;\rcase 'C': case 'T': case 'U':\rbasetype_ = BaseInfo::BaseType::Pyrimidine;\rbreak;\rdefault:\rbasetype_=BaseInfo::BaseType::None;\rmprintf(\u0026quot;Warning: not a valid nucleic acid base\u0026quot;);\rbreak;\r}\rif(basetype_) {\rbaseinfo.SetType(basetype_);\rfor (int i = res.FirstAtom(); i != res.LastAtom(); ++i) {\rauto atomType = parmIn[i].Name();\rif (atomType.Match(\u0026quot;C2\u0026quot;)) {\rbaseinfo.SetC2(i);\r} else if (atomType.Match(\u0026quot;C4\u0026quot;)) {\rbaseinfo.SetC4(i);\r} else if (atomType.Match(\u0026quot;C6\u0026quot;)) {\rbaseinfo.SetC6(i);\r}\r}\rbaseinfo.SetResIdx(resnum);\rif(!baseinfo.checkComplete()) {\rmprintf(\u0026quot;BaseInfo is not completed, please make sure the nucleic base is correct\u0026quot;);\rreturn 1;\r}\rvecBase.push_back(baseinfo);\r}\r}\rreturn 0;\r}\rvoid Action_Ermsd::calcGMat(const Frame\u0026amp; fIn, vvv\u0026amp; GMat) {\rfor(int i=0; i \u0026lt;nres_; ++i)\rfor(int j=0; j\u0026lt;nres_; ++j)\rfor(int k=0; k\u0026lt;4; ++k)\rGMat[i][j][k] = 0;\rstd::vector\u0026lt;Vec3\u0026gt; VecCom;\rstd::vector\u0026lt;std::vector\u0026lt;Vec3\u0026gt;\u0026gt; VecDir;\rfor(const auto\u0026amp; base: vecRefBase_) {\rVec3 C2XYZ{fIn.XYZ(base.C2())};\rVec3 C4XYZ{fIn.XYZ(base.C4())};\rVec3 C6XYZ{fIn.XYZ(base.C6())};\rVec3 COM{(C2XYZ + C4XYZ + C6XYZ) / 3.0};\rVec3 XDir{C2XYZ-COM};\rXDir.Normalize();\rVec3 ZDir(0.0);\rif(base.type() == BaseInfo::BaseType::Pyrimidine)\rZDir = (C4XYZ - COM).Cross(XDir);\relse if(base.type() == BaseInfo::BaseType::Purine)\rZDir = (C6XYZ - COM).Cross(XDir);\rZDir.Normalize();\rVec3 YDir{XDir.Cross(ZDir)};\rYDir.Normalize();\rstd::vector\u0026lt;Vec3\u0026gt; Dir = std::vector\u0026lt;Vec3\u0026gt;{XDir, YDir, ZDir};\rVecCom.push_back(COM);\rVecDir.push_back(Dir);\r}\rdouble firstCutOff = cutoff_ * F_factors[0];\rfor(int i = 0; i \u0026lt; nres_; ++i) {\rfor(int j = 0; j \u0026lt; nres_; ++j) {\rif (i == j) continue;\rdouble dist = sqrt((VecCom[i]-VecCom[j]).Magnitude2());\r// check first cutoff\rif(dist \u0026gt; 0.01 \u0026amp;\u0026amp; dist \u0026lt; firstCutOff) {\rVec3 diff = VecCom[j] - VecCom[i];\rVec3 projection{diff*VecDir[i][0], diff*VecDir[i][1], diff*VecDir[i][2]};\rprojection /= F_factors;\rdouble dist2 = sqrt(projection.Magnitude2());\rdouble gamma = Constants::PI / cutoff_;\r// second check cutoff\rdouble sin_gamma_r = sin(gamma * dist2);\rdouble cos_gamma_r = cos(gamma * dist2);\rif(dist2 \u0026lt;= cutoff_) {\rGMat[i][j][0] = projection[0] / dist2 * sin_gamma_r / gamma;\rGMat[i][j][1] = projection[1] / dist2 * sin_gamma_r / gamma;\rGMat[i][j][2] = projection[2] / dist2 * sin_gamma_r / gamma;\rGMat[i][j][3] = (1 + cos_gamma_r) / gamma;\r}\r}\r}\r}\r}\r ","date":1598930270,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598930270,"objectID":"6af96d8e0c63e79d1e52c8d46290dc74","permalink":"/post/ermsd/","publishdate":"2020-09-01T11:17:50+08:00","relpermalink":"/post/ermsd/","section":"post","summary":"Cacluate ERMSD of Nucleic acid base","tags":["Trajectory","CPPTRAJ"],"title":"ERMSD","type":"post"},{"authors":null,"categories":null,"content":"Overview \r\r(2020).\rPymolCheatsheet.\r\r\rI ❤️ Academic 😄\n","date":1597801403,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598336322,"objectID":"7c034b1eb620a0494fa46f403ccc7344","permalink":"/books/book1/chapter1/","publishdate":"2020-08-19T09:43:23+08:00","relpermalink":"/books/book1/chapter1/","section":"books","summary":"从统计力学到生物分子，囊括了Thermodynamics, PMF, Free Energy, Binding, Allostery, Kinetics 等基本概念. 整本书从概率角度出发，联系了热力学, 统计力学, 到真实生物分子的自由能,分子结合,构象变化等重要的生物学过程。","tags":["Statistical"],"title":"Chapter1","type":"page"},{"authors":null,"categories":null,"content":"Overview ","date":1597801403,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597807871,"objectID":"3f10b11e83b2439fe31a51e9cf04e573","permalink":"/books/book1/chapter2/","publishdate":"2020-08-19T09:43:23+08:00","relpermalink":"/books/book1/chapter2/","section":"books","summary":"Overview ","tags":null,"title":"Chapter2","type":"page"},{"authors":[],"categories":[],"content":"NGLVIEW Show PDB Structure  show raw pdb structure with different representation  \u0026lt;/style\u0026gt;\r\u0026lt;script src=\u0026quot;/js/ngl.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;script\u0026gt;\rdocument.addEventListener(\u0026quot;DOMContentLoaded\u0026quot;, function () {\rvar stage = new NGL.Stage(\u0026quot;viewport\u0026quot;, {backgroundColor: \u0026quot;black\u0026quot;});\rstage.loadFile(\u0026quot;rcsb://1cfd\u0026quot;).then(function(component) {\r// component.addRepresentation(\u0026quot;cartoon\u0026quot;, {colorScheme: \u0026quot;sstruc\u0026quot;});\rcomponent.addRepresentation(\u0026quot;cartoon\u0026quot;, {color: \u0026quot;gray\u0026quot;, opacity: 0.7});\rcomponent.addRepresentation(\u0026quot;ball+stick\u0026quot;, {sele: \u0026quot;12-13\u0026quot;});\rcomponent.autoView(\u0026quot;12-13\u0026quot;);\r})\rstage.mouseControls.remove(\u0026quot;scroll\u0026quot;, NGL.MouseActions.zoomScroll)\r\u0026lt;!-- stage.keyControls.add(\u0026quot;r\u0026quot;, NGL.KeyActions.autoView); --\u0026gt;\r});\r\u0026lt;/script\u0026gt;\r\u0026lt;div id=\u0026quot;viewport\u0026quot; style=\u0026quot;width:100%; height:400px; margin: 0 auto; overflow:hidden;\u0026quot;\u0026gt; \u0026lt;/div\u0026gt;\r \n\r\rdocument.addEventListener(\"DOMContentLoaded\", function () {\rvar stage = new NGL.Stage(\"viewport\", {backgroundColor: \"black\"});\rstage.loadFile(\"rcsb://1cfd\").then(function(component) {\r// component.addRepresentation(\"cartoon\", {colorScheme: \"sstruc\"});\rcomponent.addRepresentation(\"cartoon\", {color: \"gray\", opacity: 0.7});\rcomponent.addRepresentation(\"ball+stick\", {sele: \"12-13\"});\rcomponent.autoView(\"12-13\");\r})\rstage.mouseControls.remove(\"scroll\", NGL.MouseActions.zoomScroll)\r});\r\r\r show two structures and align  \u0026lt;script\u0026gt;\rdocument.addEventListener(\u0026quot;DOMContentLoaded\u0026quot;, function () {\rvar stage = new NGL.Stage(\u0026quot;viewport2\u0026quot;, {backgroundColor: \u0026quot;black\u0026quot;});\rstage.mouseControls.remove(\u0026quot;scroll\u0026quot;, NGL.MouseActions.zoomScroll)\rPromise.all([\rstage.loadFile('rcsb://1cll').then(function(o) {\ro.addRepresentation('cartoon', {color: 'lightgreen'})\ro.autoView()\rreturn o;\r}),\rstage.loadFile('rcsb://1cfd').then(function(o) {\ro.addRepresentation('cartoon', {color: 'tomato'})\ro.autoView()\rreturn o;\r})\r]).then(function (ol) {\rvar s1 = ol[0].structure\rvar s2 = ol[1].structure\rNGL.superpose(s1, s2, true, \u0026quot;1-63.CA\u0026quot;, \u0026quot;1-63.CA\u0026quot;)\rol[ 0 ].updateRepresentations({ position: true })\r})\r});\r\u0026lt;/script\u0026gt;\r\u0026lt;div id=\u0026quot;viewport2\u0026quot; style=\u0026quot;width:100%; height:400px; margin: 0 auto; overflow:hidden;\u0026quot;\u0026gt; \u0026lt;/div\u0026gt;\r \rdocument.addEventListener(\"DOMContentLoaded\", function () {\rvar stage = new NGL.Stage(\"viewport2\", {backgroundColor: \"black\"});\rstage.mouseControls.remove(\"scroll\", NGL.MouseActions.zoomScroll)\rPromise.all([\rstage.loadFile('rcsb://1cll').then(function(o) {\ro.addRepresentation('cartoon', {color: 'lightgreen'})\ro.autoView()\rreturn o;\r}),\rstage.loadFile('rcsb://1cfd').then(function(o) {\ro.addRepresentation('cartoon', {color: 'tomato'})\ro.autoView()\rreturn o;\r})\r]).then(function (ol) {\rvar s1 = ol[0].structure\rvar s2 = ol[1].structure\rNGL.superpose(s1, s2, true, \"1-63.CA\", \"1-63.CA\")\rol[ 0 ].updateRepresentations({ position: true })\r})\r});\r\r\r add a pdb INPUT for showing the structure and double click for a full screen  \u0026lt;script\u0026gt;\rvar stage;\rdocument.addEventListener(\u0026quot;DOMContentLoaded\u0026quot;, function () {\rstage = new NGL.Stage(\u0026quot;viewport3\u0026quot;);\rstage.mouseControls.remove(\u0026quot;scroll\u0026quot;, NGL.MouseActions.zoomScroll)\rstage.viewer.container.addEventListener(\u0026quot;dblclick\u0026quot;, function () {\rstage.toggleFullscreen();\r});\rfunction handleResize () {\rstage.handleResize();\r}\rdocument.getElementById(\u0026quot;pdbidInput\u0026quot;).addEventListener(\u0026quot;keydown\u0026quot;, function (e) {\rif (e.keyCode === 13) {\rstage.removeAllComponents();\rvar url = \u0026quot;rcsb://\u0026quot; + e.target.value + \u0026quot;.mmtf\u0026quot;;\rstage.loadFile(url, {defaultRepresentation: true});\re.target.value = \u0026quot;\u0026quot;;\re.target.blur();\r}\r});\r});\r\u0026lt;/script\u0026gt;\r\u0026lt;div id=\u0026quot;viewport3\u0026quot; style=\u0026quot;width:100%; height:400px;\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;\r\u0026lt;div style=\u0026quot;text-align:center; position:relative; bottom:3em;\u0026quot;\u0026gt;\r\u0026lt;div style=\u0026quot;display:inline-block;\u0026quot;\u0026gt;\r\u0026lt;input id=\u0026quot;pdbidInput\u0026quot; style=\u0026quot;opacity:0.7; width:4em;\u0026quot;/\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r \rvar stage;\rdocument.addEventListener(\"DOMContentLoaded\", function () {\rstage = new NGL.Stage(\"viewport3\");\rstage.mouseControls.remove(\"scroll\", NGL.MouseActions.zoomScroll)\rstage.viewer.container.addEventListener(\"dblclick\", function () {\rstage.toggleFullscreen();\r});\rfunction handleResize () {\rstage.handleResize();\r}\rdocument.getElementById(\"pdbidInput\").addEventListener(\"keydown\", function (e) {\rif (e.keyCode === 13) {\rstage.removeAllComponents();\rvar url = \"rcsb://\" + e.target.value + \".mmtf\";\rstage.loadFile(url, {defaultRepresentation: true});\re.target.value = \"\";\re.target.blur();\r}\r});\r});\r\r\r\r\rDraw custom shape  show arrow beteen two atoms and zoom  \u0026lt;script\u0026gt;\rdocument.addEventListener(\u0026quot;DOMContentLoaded\u0026quot;, function () {\rvar stage = new NGL.Stage(\u0026quot;viewport4\u0026quot;, {backgroundColor: \u0026quot;white\u0026quot;});\rstage.mouseControls.remove(\u0026quot;scroll\u0026quot;, NGL.MouseActions.zoomScroll)\rstage.loadFile(\u0026quot;rcsb://1crn.mmtf\u0026quot;).then(function (o){\ro.addRepresentation(\u0026quot;cartoon\u0026quot;, {opacity: 0.3, depthWrite: false, flatShaded: false, side: \u0026quot;double\u0026quot;})\rconst ap1 = o.structure.getAtomProxy();\rconst ap2 = o.structure.getAtomProxy();\rvar idx = o.structure.getAtomIndices(new NGL.Selection(\u0026quot;(10.CA) or (11.CA)\u0026quot;))\rap1.index = idx[0];\rap2.index = idx[1];\rvar shape = new NGL.Shape('shape', { dashedCylinder: true });\rshape.addArrow(ap1.positionToVector3(), ap2.positionToVector3(), [ 0, 1, 1 ], 0.3)\rvar shapeComp = stage.addComponentFromObject( shape );\ro.addRepresentation('ball+stick', {sele:\u0026quot;10, 11\u0026quot;})\rshapeComp.addRepresentation( \u0026quot;buffer\u0026quot; );\rvar center = o.getCenter(\u0026quot;10, 11\u0026quot;)\rvar zoom = o.getZoom(\u0026quot;10, 11\u0026quot;)\rstage.animationControls.zoomMove(center, zoom, 0);\r});\r});\r\u0026lt;/script\u0026gt;\r\u0026lt;div id=\u0026quot;viewport4\u0026quot; style=\u0026quot;width:100%; height:400px; margin: 0 auto; overflow:hidden;\u0026quot;\u0026gt; \u0026lt;/div\u0026gt;\r \rdocument.addEventListener(\"DOMContentLoaded\", function () {\rvar stage = new NGL.Stage(\"viewport4\", {backgroundColor: \"white\"});\rstage.mouseControls.remove(\"scroll\", NGL.MouseActions.zoomScroll)\rstage.loadFile(\"rcsb://1crn.mmtf\").then(function (o){\ro.addRepresentation(\"cartoon\", {opacity: 0.3, depthWrite: false, flatShaded: false, side: \"double\"})\rconst ap1 = o.structure.getAtomProxy();\rconst ap2 = o.structure.getAtomProxy();\rvar idx = o.structure.getAtomIndices(new NGL.Selection(\"(10.CA) or (11.CA)\"))\rap1.index = idx[0];\rap2.index = idx[1];\rvar shape = new NGL.Shape('shape', { dashedCylinder: true });\rshape.addArrow(ap1.positionToVector3(), ap2.positionToVector3(), [ 0, 1, 1 ], 0.3)\rvar shapeComp = stage.addComponentFromObject( shape );\ro.addRepresentation('ball+stick', {sele:\"10, 11\"})\rshapeComp.addRepresentation( \"buffer\" );\rvar center = o.getCenter(\"10, 11\")\rvar zoom = o.getZoom(\"10, 11\")\rstage.animationControls.zoomMove(center, zoom, 0);\r});\r});\r\r\rdocument.addEventListener(\"DOMContentLoaded\", function () {\rvar stage = new NGL.Stage(\"viewport5\", {backgroundColor: \"white\"});\rstage.loadFile('rcsb://2MI7.pdb').then(function (o) {\rNGL.autoLoad('rcsb://2MI7.pdb').then(function (frames) {\ro.addTrajectory(frames, {\rinitialFrame: 0,\rdeltaTime: 200\r})\ro.addRepresentation('licorice', {scale: 0.5})\ro.addRepresentation('spacefill', {sele: 'not :B'})\ro.addRepresentation('cartoon')\ro.addRepresentation('backbone')\rstage.autoView()})\r})\r})\r\r --  ","date":1597454709,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597807871,"objectID":"9f56de68877d3082c1faa807aad2f547","permalink":"/post/nglview/","publishdate":"2020-08-15T09:25:09+08:00","relpermalink":"/post/nglview/","section":"post","summary":"NGLVIEW basic example for my website with a few example","tags":["visualize"],"title":"NGLVIEW","type":"post"},{"authors":[],"categories":[],"content":"Introduction  Select around:  select active, byres all with 5 of ligands select ligand_water, ((ligands)around 3.2) and (resn HOH)   Change radius  alter active_water, vdw=0.5 rebuild   color and cartoon setting  set cartoon_color, slate set transparency 0.4 set cartoon_fancy_helices, 1 set cartoon_highlight_color, grey50 bg_color, white set antialias, 1   load netcdf  load rna.prmtop load_traj min2.rst, rna, 0, nc load rna.prmtop, traj load_traj prod_2us.nc, traj, 0, nc, start=8215, stop=8215   Create from an object  create name, (selection), [, source_state [, target_state]]   remove solvent and ions  remove solvent remove inorganic   Hydrogen bond  distance test, interface, nucleic, mode=2   align with specified residue  align 1cfd_unbound///168/, 1cll_bound///168/   Draw RNA dssr block  set ambient, 0.6 dssr_block block_file=face+edge block_color=C yellow   Rename an object  set_name rna rna01   Print resName of an object  iterate bca. all , print (resn) Iterate * PyMOLWiki Python API save to a name myspace = {\u0026quot;resname\u0026quot;: []}\rcmd.iterate(\u0026quot;bca. all\u0026quot;, \u0026quot;resname.append(resn)\u0026quot;, space=myspace)\r    Get XYZ of a atom  cmd.get_coords(\u0026quot;/1cll///20/CA\u0026quot;)   Calcualte helix angle between  angle_between_helices 1cll///71*76/, 1cll///86*92/, visualize=1   Calcualte helix every frame for i in range(30):\rpymol.cmd.set(\u0026quot;state\u0026quot;, i+1)\rangle.append(anglebetweenhelices.angle_between_helices(\u0026quot;1dmo///71*76/\u0026quot;, \u0026quot;1dmo///86*92/\u0026quot;))\r   ","date":1597376208,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598336322,"objectID":"97842820afd7c92045126bea44b07c83","permalink":"/post/pymolcheatsheet/","publishdate":"2020-08-14T11:36:48+08:00","relpermalink":"/post/pymolcheatsheet/","section":"post","summary":"PYMOL Cheatsheet for visulize the structure of biomolecule","tags":["visualize"],"title":"PymolCheatsheet","type":"post"},{"authors":[],"categories":[],"content":"Reference Books \r\r Idea of OpenMP Distribured and shared-memory computers Fork jon programming model, program starts as a single thread of execution, and a team of threads is forked at the begining of parallel region and joined at the end Feature Set  Creating Teams of Threads  Specifiy the parallel region by inserting a parallel directive before the code that is to be executed   Sharing Work among Threads  Work sharing and loops while construct cannot be shared, work should be independent Giving distinct pieces of work to individual threads   OpenMP Memory Model  Shared-memory model, by default, data is shared among the threads and is visble to all of them. flush operation makes sure that the thread calling it has same values for shared data objects.   Thread Synchronization  By default, OpenMP gets threads to wait at the end of a work-sharing construct or parallel region until all threads in the team executing it have finished their portion of work. (Barrier) Synchronization Point  explicit and implicit barriers start and end of critical regions points where locks are required or released anywhere the progarmmer has inserted a flush directive      Performance Considerations  Amdahl\u0026rsquo;s law $$S=\\frac{1}{\\left(f_{\\text {par}} / P+\\left(1-f_{\\text {par}}\\right)\\right)}$$  $f_{\\text par}$ parallel fraction of the code, idel case is 1 $p$ number of processors If only $80%$ percent of code runs is parallel  maximal speedup on 16 processors is 4 maximal speedup on 32 processors is 4.4     Other Considerations  overheads by forking and joining threads, thread synchronization and memeory accessed    ","date":1597201195,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598336322,"objectID":"6ac606b3abd53c7600c20493271907ee","permalink":"/post/openmp/openmp_introduction_1/","publishdate":"2020-08-12T10:59:55+08:00","relpermalink":"/post/openmp/openmp_introduction_1/","section":"post","summary":"Basic about OpenMP Programming interface","tags":["openmp","parallel"],"title":"OPENMP Overview","type":"post"},{"authors":null,"categories":["Learn"],"content":"问题 给定一个数据流，数据流长度N很大，且N直到处理完所有数据之前都不可知，请问如何在只遍历一遍数据（O(N)）的情况下，能够随机选取出m个不重复的数据。\n解法  如果接收的数据量小于m，则依次放入蓄水池。 当接收到第i个数据时，i \u0026gt;= m，在[0, i]范围内取以随机数d，若d的落在[0, m-1]范围内，则用接收到的第i个数据替换蓄水池中的第d个数据。 重复步骤2。  解释  这里需要证明处理完这整个数据流之后，所以数被选择的概率都能有 m / N i\u0026lt;=m 时， 第 i 个数据进入 reservior 的概率是 1 i\u0026gt;m 时， 从 [1, i] 选择随机数 d, 如果 d \u0026lt;= m , 则使用第 i 个数据替换 第 d 个数据， 第 i 个数据进入 reservoir概率是 m / i i\u0026lt;=m时， 第(m+1) 次会替换掉池中数据的概率 m/(m+1), 替换第 i 个数据概率 1/m, 则 m+1 次替换掉 i个数据概率 为 1/(m+1), 不被替换概率为 m/(m+1), 第 (m+2) 次处理不替换第 i 个数据概率 (m+1) / (m+2)， 依次计算可得第 i 个数据不被替换的概率为 m / N i \u0026gt; m 时， 接收 i+1 个数据可能替换第 i 个数据， 第 i 个数据不被替换概率 i / N i\u0026lt;=m 时 ，第 i 个数据留在 reservoir 概率是 $1*m/N$, i\u0026gt;m 时， 第 i 个数据留在 reservoir 概率为 $m /i * i/ N = m / N$  练习 reservior 深度为 1\n https://leetcode-cn.com/problems/random-pick-index/ https://leetcode-cn.com/problems/linked-list-random-node  \r","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597807871,"objectID":"84bb90c7d6b3320e43ecabcec4928463","permalink":"/post/reservior_sampling/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/post/reservior_sampling/","section":"post","summary":"蓄水池采样","tags":["Sampling","Algorithm"],"title":"蓄水池采样","type":"post"},{"authors":null,"categories":["Learn"],"content":"在科研论文中，经常需要输入LaTeX公式，这是一件很令人头疼的事，因为公式输入很复杂且容易出错。MathPix这个软件简化了这个过程，可以直接通过图片识别出 LaTeX 公式。以前这个软件是免费的, 后来开始收费且价格不菲. 不过推出了 API 调用方式，一个月3000条免费，后面超出的价格也不贵。\n为了更好的使用这个API，一键操作，仿照以前的 MathPix界面，用 pyqt 写了一个简略的界面，不过也可以实现基本功能。\nIDE 用的是 PyCharm, 不像 Qt 有 QtCreater, python 版本的 pyqt 没找到官方的 IDE, 因此用的是PyCharm. 在 PyCharm 上加了两个 External Tools:\n qt designer, 设计 UI 界面  pyUIC5, 将 UI file 转为 .py file   Qt Designer 主要用来设计UI界面，设计好的界面如下  图片显示界面， 可以从剪切板里导入图片，或者是从应用外拖进图片 Convert, 触发调用API命令，并将结果显示 Qt Web, 一个网页显示界面，在 convert， 生成 LaTeX 代码之后，通过调用 MathJax Api， 显示生成后的 LaTeX 公式， 方便与原始图片进行比较 PasteBoard 从剪切板导入图片 生成后的 LaTeX 代码，通过 copy 将文字导入到剪切板， 三个copy 对应三种不同返回格式  用到的对象如下\n这里有一点需要注意的是，label 标签对应的类不是 QLabel, 因为如果 QLabel 要支持拖拽功能，需要在 QLabel 下重新定义一个子类，并让子类重写dragEnterEvent 和 dropEvent 两个函数，这样，需要对 Qt Designer 的 QLabel 进行提升，这里我将提升的类命名为 ImageArea.\n代码  app_run.py 主程序，程序代码的入口，定义了整个对象，以及对象之间如何交互 customClass.py 定义了 ImageArea 类， 让 QLabel 支持拖拽 dealWithApi.py MathPix 的调用接口 qtDesigner.py 通过 pyUIC 将 ui 文件转换为 py 文件 showLaTeXImage.py 将生成的LaTeX 公式文本，用 MathJax 显示  演示 For the source code, see at https://github.com/zhanghaomiao/image_to_latex_app\n","date":1584835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597196259,"objectID":"a7ff458ea9dc3f2b11bf83ad6e848e00","permalink":"/post/image_latex_app/","publishdate":"2020-03-22T00:00:00Z","relpermalink":"/post/image_latex_app/","section":"post","summary":"给定识别图像公式的api, 返回需要的数学公式","tags":["computer","qt","python"],"title":"用 PyQT 创建一个识别公式的应用","type":"post"},{"authors":null,"categories":["Learn"],"content":"Prepare  Perplexity Measurement of how well a probability model predicts a sample.  Definition $$H(P) = -\\sum_x p(x) \\log_2 p(x)$$ $$\\text{Perplexity} = 2^{H(p)}$$ Understanding  $H(p)$ be the average of bits to decode the information $\\text{Perplexity}$ is the total amount of all possible information   Usage In the NLP, the best language model is one that predicts an unseen test set gives the highest P $$ PP(w) = p (w_1w_2\\dots w_N )^{-1/N}$$ Minmizing perplexity is the same as maximizing probability   Gaussian Distriubtion and T Distribution  T distribution is used when the samples is small T distribution is heavy tail     T-SNE method SNE method (Stochastic Neighbor Embedding)   High Dimension Space $$p_{j | i}=\\frac{\\exp \\left(-\\left|x_{i}-x_{j}\\right|^{2} / 2 \\sigma_{i}^{2}\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left|x_{i}-x_{k}\\right|^{2} / 2 \\sigma_{i}^{2}\\right)}$$\n  Low dimension space\n$$q_{j | i}=\\frac{\\exp \\left(-\\left|y_{i}-y_{j}\\right|^{2}\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left|y_{i}-y_{k}\\right|^{2}\\right)}$$\n  Cost function $$C=\\sum_{i} K L\\left(P_{i} | Q_{i}\\right)=\\sum_{i} \\sum_{j} p_{j | i} \\log \\frac{p_{j | i}}{q_{j | i}}$$\n  How to choose $\\sigma$ for different points\nDifferent region have different density, so the $\\sigma$ determines how many effective neighbors needs to be considered. As the $\\sigma$ increase, the entropy of the distribution is increased. And the entroy has an expontential form with perplexity. Using the perplexity to determine the $\\sigma$ with every point has a fixed perplexity.\n   Problem  The cost function is not symmetric, it focus on retaining the local structure of the data Dealing with outlier Crowding problem : The area of the two-dimensional map that is available to accommodate distant datapoints will not be large enough compared with the area available to accommodate nearby datapoints    T-SNE advantages  Symmetric SNE   Pairwise similarities in the low dimensional map $q_{ij}$ $$q_{i j}=\\frac{\\exp \\left(-\\left|y_{i}-y_{j}\\right|^{2}\\right)}{\\sum_{k \\neq l} \\exp \\left(-\\left|y_{k}-y_{l}\\right|^{2}\\right)}$$ High dimensional $p_{ij}$ $$P_{ij} = \\frac {p_{j|i} + p_{i|j}}{2n}$$   Crowding problem The gradient could be negative, so the low dimensional space could be expanded, and the heavy tail T-Distribution is preferred.\n  Easy to compute the gradient\n$$\\frac{\\delta C}{\\delta y_{i}}=4 \\sum_{j}\\left(p_{i j}-q_{i j}\\right)\\left(y_{i}-y_{j}\\right)\\left(1+\\left|y_{i}-y_{j}\\right|^{2}\\right)^{-1}$$\n  Algorithm  Data : data Set $\\mathcal{X} = {x_1, x_2, \\dots, x_n}$ cost function parameters: perplexity $Perp$ optimzation paraemters: number of iterations $T$, learning rate $\\eta$, momentum $\\alpha(t)$ Output: low-dimensional data representation $\\mathcal{Y}^{T} = {y_1, y_2, \\dots, y_n}$ Steps  compute pairwise affinities $p_{j|i}$ with perplexity $Perp$ Set $p_{ij} = (p_{j|i} + p_{i|j}) / 2n$, sample initial solution $\\mathcal{Y}^{(0)} = {y_1, y_2, \\dots, y_n}$ from $\\mathcal{N}(0, 10^{-4}I)$ for $t = 1$ to $T$ do  compute low-dimensional affinities $q_ij$ compute gradient $\\partial C / \\partial \\mathcal{Y}$ set $$\\mathcal{Y}^{(t)}=\\mathcal{Y}^{(t-1)}+\\eta \\frac{\\delta C}{\\delta \\gamma}+\\alpha(t)\\left(\\mathcal{Y}^{(t-1)}-\\mathcal{Y}^{(t-2)}\\right)$$      Code with python import numpy as np\rimport pylab\rdef Hbeta(D=np.array([]), beta=1.0):\r\u0026quot;\u0026quot;\u0026quot;\rCompute the perplexity and the P-row for a specific value of the\rprecision of a Gaussian distribution.\r\u0026quot;\u0026quot;\u0026quot;\r# Compute P-row and corresponding perplexity\rP = np.exp(-D.copy() * beta)\rsumP = sum(P)\rH = np.log(sumP) + beta * np.sum(D * P) / sumP\rP = P / sumP\rreturn H, P\rdef x2p(X=np.array([]), tol=1e-5, perplexity=30.0):\r\u0026quot;\u0026quot;\u0026quot;\rPerforms a binary search to get P-values in such a way that each\rconditional Gaussian has the same perplexity.\r\u0026quot;\u0026quot;\u0026quot;\r# Initialize some variables\rprint(\u0026quot;Computing pairwise distances...\u0026quot;)\r(n, d) = X.shape\rsum_X = np.sum(np.square(X), 1)\rD = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\rP = np.zeros((n, n))\rbeta = np.ones((n, 1))\rlogU = np.log(perplexity)\r# Loop over all datapoints\rfor i in range(n):\r# Print progress\rif i % 500 == 0:\rprint(\u0026quot;Computing P-values for point %d of %d...\u0026quot; % (i, n))\r# Compute the Gaussian kernel and entropy for the current precision\rbetamin = -np.inf\rbetamax = np.inf\rDi = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\r(H, thisP) = Hbeta(Di, beta[i])\r# Evaluate whether the perplexity is within tolerance\rHdiff = H - logU\rtries = 0\rwhile np.abs(Hdiff) \u0026gt; tol and tries \u0026lt; 50:\r# If not, increase or decrease precision\rif Hdiff \u0026gt; 0:\rbetamin = beta[i].copy()\rif betamax == np.inf or betamax == -np.inf:\rbeta[i] = beta[i] * 2.\relse:\rbeta[i] = (beta[i] + betamax) / 2.\relse:\rbetamax = beta[i].copy()\rif betamin == np.inf or betamin == -np.inf:\rbeta[i] = beta[i] / 2.\relse:\rbeta[i] = (beta[i] + betamin) / 2.\r# Recompute the values\r(H, thisP) = Hbeta(Di, beta[i])\rHdiff = H - logU\rtries += 1\r# Set the final row of P\rP[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP\r# Return final P-matrix\rprint(\u0026quot;Mean value of sigma: %f\u0026quot; % np.mean(np.sqrt(1 / beta)))\rreturn P\rdef pca(X=np.array([]), no_dims=50):\r\u0026quot;\u0026quot;\u0026quot;\rRuns PCA on the NxD array X in order to reduce its dimensionality to\rno_dims dimensions.\r\u0026quot;\u0026quot;\u0026quot;\rprint(\u0026quot;Preprocessing the data using PCA...\u0026quot;)\r(n, d) = X.shape\rX = X - np.tile(np.mean(X, 0), (n, 1))\r(l, M) = np.linalg.eig(np.dot(X.T, X))\rY = np.dot(X, M[:, 0:no_dims])\rreturn Y\rdef tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0):\r\u0026quot;\u0026quot;\u0026quot;\rRuns t-SNE on the dataset in the NxD array X to reduce its\rdimensionality to no_dims dimensions. The syntaxis of the function is\r`Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\r\u0026quot;\u0026quot;\u0026quot;\r# Check inputs\rif isinstance(no_dims, float):\rprint(\u0026quot;Error: array X should have type float.\u0026quot;)\rreturn -1\rif round(no_dims) != no_dims:\rprint(\u0026quot;Error: number of dimensions should be an integer.\u0026quot;)\rreturn -1\r# Initialize variables\rX = pca(X, initial_dims).real\r(n, d) = X.shape\rmax_iter = 1000\rinitial_momentum = 0.5\rfinal_momentum = 0.8\reta = 500\rmin_gain = 0.01\rY = np.random.randn(n, no_dims)\rdY = np.zeros((n, no_dims))\riY = np.zeros((n, no_dims))\rgains = np.ones((n, no_dims))\r# Compute P-values\rP = x2p(X, 1e-5, perplexity)\rP = P + np.transpose(P)\rP = P / np.sum(P)\rP = P * 4.\t# early exaggeration\rP = np.maximum(P, 1e-12)\r# Run iterations\rfor iter in range(max_iter):\r# Compute pairwise affinities\rsum_Y = np.sum(np.square(Y), 1)\rnum = -2. * np.dot(Y, Y.T)\rnum = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))\rnum[range(n), range(n)] = 0.\rQ = num / np.sum(num)\rQ = np.maximum(Q, 1e-12)\r# Compute gradient\rPQ = P - Q\rfor i in range(n):\rdY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)\r# Perform the update\rif iter \u0026lt; 20:\rmomentum = initial_momentum\relse:\rmomentum = final_momentum\rgains = (gains + 0.2) * ((dY \u0026gt; 0.) != (iY \u0026gt; 0.)) + \\\r(gains * 0.8) * ((dY \u0026gt; 0.) == (iY \u0026gt; 0.))\rgains[gains \u0026lt; min_gain] = min_gain\riY = momentum * iY - eta * (gains * dY)\rY = Y + iY\rY = Y - np.tile(np.mean(Y, 0), (n, 1))\r# Compute current value of cost function\rif (iter + 1) % 10 == 0:\rC = np.sum(P * np.log(P / Q))\rprint(\u0026quot;Iteration %d: error is %f\u0026quot; % (iter + 1, C))\r# Stop lying about P-values\rif iter == 100:\rP = P / 4.\r# Return solution\rreturn Y\rif __name__ == \u0026quot;__main__\u0026quot;:\rprint(\u0026quot;Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\u0026quot;)\rprint(\u0026quot;Running example on 2,500 MNIST digits...\u0026quot;)\rX = np.loadtxt(\u0026quot;mnist2500_X.txt\u0026quot;)\rprint(X.shape)\rlabels = np.loadtxt(\u0026quot;mnist2500_labels.txt\u0026quot;)\rY = tsne(X, 2, 50, 20.0)\rpylab.scatter(Y[:, 0], Y[:, 1], 20, labels)\rpylab.show()\r Outcome for mnist dataset Reference  \rPerplexity Intuition (and its derivation) - Towards Data Science \rGitHub - lvdmaaten/bhtsne: Barnes-Hut t-SNE \rt-SNE – Laurens van der Maaten  ","date":1570147200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597807871,"objectID":"eaef9af9347ce38745ced87304fde4e9","permalink":"/post/ml/t-sne/","publishdate":"2019-10-04T00:00:00Z","relpermalink":"/post/ml/t-sne/","section":"post","summary":"一种降维方法，将高维数据映射到低维空间，用于可视化。","tags":["machine learning"],"title":"T-SNE","type":"post"},{"authors":null,"categories":["Learn"],"content":"讲一下随机森林，GBDT，XGBoost   随机森林：\n 集成算法， 有放回的随机选择特征， 构建决策树 不同基学习器是独立的 特征随机和和样本随机 bagging 方法， 最后做投票    优点：\n 集成学习，方差偏差都比较低， 泛化性能比较好 高维数据集处理能力很好，并确定最重要的变量 可以处理确缺失数据 可以并行 不需要归一化    缺点：\n 回归问题表现不好，不能连续输出 无法了解模型内部 忽略属性之间的相关性    参数调配\n 网格搜索， GridSearchCV 贪心算法， 固定其他参数， 把这个参数选好 随机搜索    GBDT (Gradient Boosting Decsion Tree)\n 每一次计算减少上次出现的残差， 为了消除残差，训练一个新的模型， 训练好的弱分类器累加到现有模型中 回归树，不是分类树。    XGBoost\n GBDT 是机器学习算法， XGBoost 是该算法的实现 GBDT 算法基于经验损失函数的负梯度构造新的决策树， 在构建完成之后进行剪枝， 而 XGBoost 在决策树构造阶段加入了正则项 GBDT 在模型只使用了一阶导数信息， XGBoost 对损失函数进行二阶泰勒展开。 根据百分位列举几个可能成为分隔点的候选者， 从中找最佳的分割点 传统 GBDT 采用 CART 作为基分类器， XGBoost 采用了与随机森林类似的策略， 支持对数据采样    梯度提升和梯度下降的区别 利用损失函数相对于模型的负梯度信息来对模型进行更新， 梯度下降过程中， 模型以参数化形式表示，而梯度提升， 模型不是以参数化形式表示，直接定义在函数空间中， 大大提高了可以使用的模型种类。\n随机森林如何处理缺失值  数值型变量：给缺失值一些估计值，例如众数和中位数 描述型变量：缺失部分用出现次数最多的数代替 利用中位数和出现次数做最多的数代替，引入权重， 需要替换的数据先和其他数据做相似度测量, 补全缺失的点是相似的点会具有较高的权重 (所有数据放进随机森林运行一遍，记录每一组数据在决策树中分类路径，判断哪组数据和缺失数据路径最相似， 引入相似度矩阵，记录数据之间的相似度)  AdaBoost 和 XGBoost 区别 详细解释 AUC   TP: 实际这正类， 预测为正类\n  TN: 实际为负类，预测为负类\n  FN: 实际为正类， 预测为负类 漏报\n  FP: 实际为负类，预测为正类 误报\n  TPR = TP / (TP + FN) // 正实例被预测正确的比例 (RECALL) precision 指的是分类正确的正样本个数占分类器判断为正样本的个数比例\n  FPR = FP / (FP + TN) // 负实例被误报的比例\n  选定不同的阈值， 判断是否为正例， 不同的阈值对应不同的指标， 把这些指标连接起来，即为 ROC 曲线\n  解释： 从所有为1的样本从随机选择一个样本， 从所有为0的样本中选择一个样本， 根据分类器对两个样本进行预测，样本1预测为1 的概率为 p1, 样本2预测的概率为 p2, p1 \u0026gt; p2 的概率为AUC\n  随机森林如何评估特征的重要性， Xgboost 如何寻找最优特征   基于基尼指数 看每个特征在随机森林中的每棵树做了多少共享， 取平均值，利用Gini指数, 算利用这个特征分离后的基尼指数之差，取上平均值即可判定\n  基于带外数据 未被抽取样本的集合，使用袋外数据计算第 T_i 颗决策树的校验误差 e1，随机改变 OOB 中的第 J 列， 保持其他列不变， 对第j列进行随机上下置换， 得到误差e2, 利用 e1 - e2 来刻画特征 $j$ 的重要性. 这里可以选择不同的树来加权平均。\n  Xgboost xgboost 在训练过程中可以给出各个特征的评分， 最大的增益会被选出作为依据， 从而记忆了每个特征在模型训练时的重要性 xgboost 属于 boosting 集成方法， 样本不放回，每轮计算样本不重复。\n  欧氏距离和其他距离的区别  欧氏距离样本不同属性之间的差别等同看待， 这有时候不满足条件, 当其他特征比较大是，很多特征接近于 0， 没有考虑维度之间的相关性 曼哈顿距离： 异常值的分类结果影响比欧式距离药效， 各维度的贡献基本一样。  Logistic Regression 为什么需要对特征进行离散化  LR属于广义线性模型， 表达能力受限， 离散化为 N 个后， 每个变量有单独的权重， 为模型引入了非线性， 提升了模型的表达能力 速度快，稀疏向量内积乘法运算速度较快， 计算结果更容易存储和扩展 更具有鲁棒性，数据偏离后也不会造成较大影响 离散后的特征可以进行交叉， 提升表达能力 特征离散化后，会更加稳定  LR 与 SVM 区别和联系  相同点  都是分类算法， 本质上都是找超平面 监督学习算法 判别式模型， 不关心数据怎么生成， 只关心数据的差别， 然后利用数据的差别来进行分类   不同点  SVM 只考虑了 support vectors, 也就是和分类最相关的少数点去学习分类器。而逻辑回归通过非线性映射减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重； LR 的损失函数是交叉熵， SVM 的损失函数是 hinge loss, SVM 对样本数据的依赖减少， 提高了训练效率 LR 是参数模型， SVM 是非参数化模型， 参数模型是指数据服从某一分布， 该分布由一些参数确定。非参数模型不需要对数据的总体分布做假设，只知道总体是一个随机变量，不需要知道分布的具体形式。 LR 可以得到每一个类的概率， SVM 无法得到 LR 不依赖与样本之间的距离， SVM 基于样本之间的距离 解决非线性问题时， SVM 可以采用核函数机制， LR 通常不采样核函数计算 SVM 损失函数自带正则， 而 LR 需要在损失函数之外添加正则项    什么是熵， 联合熵， 条件熵， 相对熵， 互信息  熵 衡量系统的有序成都， 熵越大，系统越混乱， 信息熵表示离散事件的出现概率， 一个系统越是有序，其信息熵则越低。 联合熵 两个变量的联合分布，H(x,y) 条件熵 在随机变量 X 发生的前提下， 随机变量 Y 发生so带来的熵的定义则为 条件熵 H(Y|X) H(Y|X) = H(X,Y) - H(X) 相对熵， KL-Divergence D(q||p) 互信息 两个随机变量X，Y的联合分布和各自独立分布乘积的相对熵  牛顿法和梯度梯度下降法  牛顿法是二阶收敛， 梯度法是一阶收敛， 两种都只是局部算法， 梯度法仅考虑了方向， 牛顿法不仅考虑了方向，还兼顾了步长 牛顿法需要没次计算 Hessian 矩阵， 计算比较复杂 拟牛顿法使用正定矩阵来近似 Hessian 矩阵， 从而简化了运算的复杂度。  Kmeans 算法复杂度  O(tKmn): repeat t times, has K centroids, m data, n features; O((m + k)n) : m data, n feautres, K centroids.  协方差和相关性区别 相关性是协方差的标准化形式， 协方差本身很难作比较， 而相关性可以把数值scale 到 -1， 1 之间， 因此可以比较其相关性\nNavie Bayes 因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的.\n详细说些 EM 算法 在概率模型中寻找参数的极大似然估计算法, 最大化后验概率，概率模型依赖于无法观测的隐变量\n E步: 假设参数已知，计算隐变量的后验概率。 M步：带入隐变量的后验概率，最大化似然估计，计算参数值 M 步得到的参数值用于 E 步计算中  似然函数意义： 通过观测的数据推测参数值, 假设参数已经知道，使得观测的数据概率最大。\nKNN 中 K 如何选择 选择较小的 K 值， 较小领域中的训练实例来预测，学习的近似误差变小， 学习的估计误差增大， 即模型容易发生过拟合。 选择较大的 K 值， 容易发生欠拟合， 增大意味着模型变得简单 KNN 为有监督学习的算法， 一般用交叉验证的方法选择最优的K值\n最大似然估计和贝叶斯估计的区别 最大似然是对点估计， 贝叶斯推断是对分布的估计 最大似然估计求出的是最有可能的参数值， 而贝叶斯推断求解的是参数的分布 另外，贝叶斯推断加入了先验概率， 通过先验和似然来求解后验分布，最大似然直接使用似然函数\n什么是最大熵模型 ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"ac015744b0ca8dfe6670a6e51a6ce62b","permalink":"/post/ml/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/post/ml/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/","section":"post","summary":"做一些汇总，方便面试","tags":["machine learning","math"],"title":"机器学习面试总结","type":"post"},{"authors":null,"categories":["Learn"],"content":"定义  标量 $f$ 对矩阵 $\\mathbf{X}$ 的导数， 定义为 $$\\frac{\\partial f}{\\partial \\mathbf{X}} = \\left[\\frac{\\partial f}{\\partial X_{ij}}\\right]$$ 在求导时不宜拆开矩阵， 需要找到一个整体的算法 一元微积分中的导数与微分的关系有 $$df = f\u0026rsquo;(x) dx$$ 多元的导数与微分的关系 $$df = \\sum_{i=1}^{n}\\frac{\\partial f}{\\partial x_i} dx_i = \\frac{\\partial f}{\\partial \\mathbf{x}}^{t} d\\mathbf{x}$$ 矩阵导数与微分建立联系： $$d f=\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{\\partial f}{\\partial X_{i j}} d X_{i j}=\\operatorname{tr}\\left(\\frac{\\partial f^{T}}{\\partial X} d X\\right)$$ $tr$ 表示方阵对角元素之和  运算法则  加减法 $d(\\mathbf{X}\\pm \\mathbf{Y}) = d\\mathbf{X} \\pm d\\mathbf{Y}$ 乘法 $d(\\mathbf{XY}) = (d\\mathbf{X})\\mathbf{Y} + \\mathbf{X}d\\mathbf{Y}$ 转置 $d(\\mathbf{X}^T) = (d\\mathbf{X})^T$ \r参考 matrix cookbook  计算 建立矩阵与微分联系时， 在求出左侧的微分 $df$, 需要写出右侧形式的导数 $$df = tr\\left(\\frac{\\partial f}{\\partial \\mathbf{X}}^T d\\mathbf{X}\\right)$$\n 标量上迹 $a = tr(a)$ 转置 $tr(A^T) = tr(A)$ 加减法 $tr(A\\pm B) = tr(A) \\pm tr(B)$ 矩阵乘法交换 $tr(AB) = tr(BA)$ 矩阵乘法/逐元乘法交换 $\\text{tr}(A^T(B \\odot C))=\\text{tr}((A\\odot B)^TC)$, 其中 A, B, C 尺寸相同  Example   Linear Regression $l = || \\mathbf{Xw - y}||^2$, solve $\\mathbf{w}, \\mathbf{y}$ is a $m\\times 1$ vector, $\\mathbf X$ has the shape $m\\times n$, and $w$'s shape is $n\\times 1$, and the $l$ is a scalar\nIn this example, we need to solve a scalar\u0026rsquo;s derivate with respect to a vector. As the defintion, we can\u0026rsquo;t calcuate the derivate of a vector or a matrix directly. Instead, we need to calcuate the differential of a matrix. $$\\begin{aligned}l \u0026amp;= \\mathbf{(Xw - y)}^T(\\mathbf{Xw - y}) \\\ndl \u0026amp;= (X d \\boldsymbol{w})^{T}(X \\boldsymbol{w}-\\boldsymbol{y})+(X \\boldsymbol{w}-\\boldsymbol{y})^{T}(X d \\boldsymbol{w})\\\n\\end{aligned}$$ As the first and second form is a dot product between two vector, so we can get $$dl = 2(X \\boldsymbol{w}-\\boldsymbol{y})^{T} \\boldsymbol{X} d \\boldsymbol{w}$$ The differential form of $dl$ $$dl = \\frac{\\partial l}{\\partial \\mathbf{w}}^T d\\mathbf{w}$$ The formula transforms to $$\\frac{\\partial l}{\\partial \\boldsymbol{w}}=2 X^{T}(X \\boldsymbol{w}-\\boldsymbol{y})$$ Set the partial form to $0$, we could get $$\\mathbf{X^TXw} = \\mathbf{X^Ty}$$ and the $w$ will be $$\\mathbf{w = (X^TX)^{-1}X^Ty}$$\n  Reference  \r矩阵求导术（上）知乎 \rmatrix cookbook  ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"8dfdc45b98414e8d4c2eb7cc78d17846","permalink":"/post/ml/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/post/ml/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/","section":"post","summary":"计算学习中涉及到很多矩阵求导运算， 这里给出简单的矩阵求导","tags":["machine learning","math"],"title":"矩阵求导","type":"post"},{"authors":null,"categories":["Learn"],"content":"Combine multiple base-learners to imporve applicability across different domains or generalization performance in a specific task\nEnsemble Strategy   将多个学习器结合可以带来三个方面的好处\n 增强泛化性能 计算有可能陷入局部最小， 降低局部最小的风险 扩大假设空间 \r\rThree advantages\r\r\r    Average\n Simple averaging $$H(X) = \\frac1T \\sum_{i=1}^{T} h_i(x)$$ Weighted averaging $$H(x) = \\sum_{i=1}^{T} \\omega_i h_i(x)$$    Voting\n learner $h_i$ predicte a label from the collection of class ${c_1, c_2, \\dots, c_N}$. The learner $h_i$ generate a vector $(h_i^1(x); h_i^2(x); \\dots; h_i^N(x))$ from the sample $x$. Marority voting  $$H(\\boldsymbol{x})=\\left{\\begin{array}{ll}{c_{j},} \u0026amp; {\\text { if } \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})\u0026gt;0.5 \\sum_{k=1}^{N} \\sum_{i=1}^{T} h_{i}^{k}(\\boldsymbol{x})} \\ {\\text { reject, }} \u0026amp; {\\text { otherwise. }}\\end{array}\\right.$$\n可以拒绝， 即保证要提供可靠的结果\n Plurality voting (预测为票数最多的标记) $$H(\\boldsymbol{x})=c_{\\arg \\max_j \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})}$$ Weighted Voting (加权平均) $$H(\\boldsymbol{x})=c_{\\arg \\max_j \\sum_{i=1}^{T} \\omega_i h_{i}^{j}(\\boldsymbol{x})}$$    学习法(stacking) Stacking 从初始数据集训练出初级学习器， 然后生成一个新数据集用于训练次级学习器。 在新数据集中， 初级学习器的输出被当做样例输入特征， 而初始样本的标记被当做样例标记 \r\rStacking Method\r\r\r 若直接使用初级学习器的训练集产生次级训练集， 则过拟合的风险较大 K-Fold： 初始训练集被划为$k$ 个大小相似的集合 $D_1, D_2, \\dots D_k$, 令 $\\overline{D}_j = D \\backslash D_j$  给定 T 个 base learner, 初级学习器 $h_t^j$ 通过在 $\\overline{D_j}$ 上使用 第$t$ 个学习算法 对 $D_j$ 中每个样本 $x_i$, 令$z_{it} = h_t^j(x_i)$, 则由 $x_i$ 产生的次级训练样例的示例部分为 $z_i = (z_{i1}; z_{i2}; \\dots z_{iT})$, 标记部分为 $y_i$ 交叉验证过程结束后， 从T个初级学习器产生的次级训练集为 $D'=(Z_i, y_i)_{i=1}^m$ 将 $D'$ 用于训练次级学习器      Bagging  Bagging is a voting method, but base-learners are made different deliberately  Train them using slightly different training sets Generate $L$ slightly different samples from a given sample is done by bootstrap: given $X$ of size N, we draw N points randomly from $X$ with replacement to get $X^{(j)}$ Train a base-learner for ecah $X^{(j)}$    Boosting  In bagging, generate \u0026ldquo;uncorrelated\u0026rdquo; base-learners is left to chance and unstability of the learning method. (让下面的 learner train 前几个learner错误的地方) 考虑 binary classification: $d^{(j)}(x) \\in {1, -1}$ Cominber three weak learners to generate a strong learner  A week learner has error probability less than 1/2 A strong learner has arbitrarily small error probability   Traning Algorithm  Given a large traning set, randomly divide in into three Use $X^{(1)}$ to train the first learner $d^{(1)}$ and feed $X^{(2)}$ to $d^{(1)}$ Use all points misclassified by $d^{d(1)}$ and $X^{(2)}$ to train $d^{(2)}$. Then feed $X^{(3)}$ to $d^{(1)}$ and $d^{(2)}$ Use the points on which $d^{(1)}$ and $d^{(2)}$ disgree to train $d^{(3)}$   Tesing Algorithm  Feed a point it to $d^{(1)}$ and $d^{(2)}$ first. If outpus agree, use them as final predction Otherwise the output of $d^{(3)}$ is taken   Example \r\rBoosting example\r\r\r Disadvantages Requires a large traning set to afford the three-way split  AdaBoost   Use the same training set over and over again, but how to make points \u0026ldquo;larger\u0026rdquo;.\n  Modify the probabilities of drawing the instances as a function of error\n  Notation:\n $\\text{Pr}^{(i,j)}$: probability that an example $x^{(i)}, y^{(i)}$ is drawn to train the $j$th base-learner $d^{(j)}$ $\\varepsilon^{(j)} = \\sum_i \\text{Pr}^{(i,j)} 1(y^{(i)} \\neq d^{(j)}x^{(i)})$ error rate of $d^{(j)}$ on its training set.    Algorithm\n Initalize $\\text{Pr}^{(i,j)} = \\frac1N$ for all i start from $j = 1$:  Randomly draw $N$ examples for $X$ with probabilities $\\text{Pr}^{(i,j)}$ and use them to train $d^{(j)}$ Stop adding new base-learners if $\\varepsilon^{(j)} \\geq \\frac12$ Define $\\alpha_j=\\frac12 \\log \\left( \\frac{1-\\varepsilon^{(j)}}{\\varepsilon^{(j)}}\\right)\u0026gt;0$ and set $\\text{Pr}^{(i,j+1)} = \\text{Pr}^{(i,j)} \\exp(-\\alpha_j y^{(i)} d^{(j)} (x^{(i)}))$ for all $i$ Normalize $\\Pr^{(i, j+1)}$      Testing\n Given $x$, calcualte $\\hat{y}^{(j)}$ for all $j$ Make final prediction $\\hat{y}$ by voting $\\hat{y} = \\sum_j \\alpha_j d^{(j)}(x)$    Example   Why AdaBoost Works\n Empirical study: AdaBoost reduces overfitting as $L$ grows, even when there is no training error  AdaBoost increases margin (Same as SVM)  A large margin imporves generalizability Define margin of prediction of an example $(x^{(i)}, y^{(i)}) \\in X$ as: $$margin(x^{(i)}, y^{(i)}) = y^{(i)}f(x^{(i)}) = \\sum_{j:y^{(i)} = d^{(j)}(x^{(i)})} \\alpha_j - \\sum_{j:y^{(i)} \\neq d^{(j)}(x^{(i)})} \\alpha_j$$       ","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597807871,"objectID":"8c3d3fa9292523b6995eada9f26922af","permalink":"/post/ml/ensemble/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/post/ml/ensemble/","section":"post","summary":"集成方法","tags":["machine learning"],"title":"Ensemble Method","type":"post"},{"authors":["Haomiao Zhang","Changjun Chen*"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598336322,"objectID":"4debd1877fd206f6217ba833715b1130","permalink":"/publication/mixing-remd/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/mixing-remd/","section":"publication","summary":"A mixing-REMD method to fast sample the configurations of protein and obtain the free energy landscape. It combines the REMD and Metadynamis/ABMD which makes the sampling method more powerful.","tags":null,"title":"Combining the Biased and Unbiased Sampling Strategy into One Convenient Free Energy Calculation Method","type":"publication"},{"authors":["Haomiao Zhang","Changjun Chen*"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598336322,"objectID":"b1d1898c0ac996b6e6667151ce68cd51","permalink":"/publication/fsatool/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/fsatool/","section":"publication","summary":"Implementing the Mixing-REMD method by our developing FSATOOL program. It contains the sampling module and Markov State Model(MSM) which it's easy to use.","tags":null,"title":"FSATOOL: A useful tool to do the conformational sampling and trajectory analysis work for biomolecules","type":"publication"},{"authors":null,"categories":null,"content":"Computing devices  Hosts (= end systems) (PC workstations, servers, Phones) running network apps Communication Links (fiber, copper, radio, satellite) physical link  Transmission rate = bandwidth   routers: forward packets (chunks of data) protocols: control sending, receiving of messages (TCP, IP, HTTP, FTP, PPP) Internet: network of networks, loosely hierarchical, public Internet(有限的) versus private Intranet Internet standards (定标准的组织)  RFC request for comments IETF: Internet Engineering Task Force   communication infrastructure enables distributed applications (Web, email, games) communication services provided to applications(connectionless, connection-oriented) Protocol  all communication activity in Internet governed by protocols format and order of messages sent and received among network entities as well as actions taken on the transmission/receipt of a message   network edge : applications and hosts network core: routers, network of networks access networks(连接 edge 和 core, 连接 physical media, communication links)  The network edge  end systems(hosts) run application programs client/server model: client host requests, receives service from always-on server peer-peer model : minimal(no) use of dedicated server TCP services (Transmission Control protocol)  reliable, in-order byte-stream data transfer(loss: 掉的做法是 acknowledgements and retransmissions) Flow control, sender won\u0026rsquo;t overwhelm receiver 流量控制 (sender 可以送多快是由 receiver 决定的) congestion control, senders slow down sending rate when networks congested (connection太多时， 网络发生拥挤， 会放慢速度, 逐渐增大封包量， 如果封包掉了，认为网络拥挤) E.g. HTTP(web), FTP(File transfer), SMTP(email)   UDP (User Datagram Protocol) 不需要建连线  unreliable data transfer no flow control no congestion control E.g. streaming media(掉了不会影响), teleconferencing, DNS, Internet telephony    Network access and physical media  How to connect end systems to edge router?  residential access nets institutional access networks (school, company) mobile access networks   keep in mind:  bandwidth (bits per second) of access network shared or dedicated    Residential access  dialup via modem  up to 56Kbps direct access to router (often less) can\u0026rsquo;t surf and phone at same time   ADSL (asymmetric digital subscriber line)  up to 1 Mpbs upstream up to 8 Mpbs downstream FDM 50KHz - 1MHz for downstream, 4KHz-50kHZ for upstream, 0kHZ-4kHZ for ordinary telephone   HFC: hybrid fiber coaxial cable, asymmetric, up to 30 Mbps downstream, 2Mpbs upstream  network of cable and fiber attaches homes to ISP router, shared access to router among home, issues(congestion, dimensioning) deployment: available via cable components, e.g. MediaOne    Company access  company/university local area network (LAN) connects end system to edge router Ethernet: shared or dedicated link connects end system and router(10Mbps, 100Mpbs, Gigabit Ethernet) deployment: institutions, home  Wireless access networks  shared wireless access network connects end system to router, via base station wireless LANs: 802.11b (WiFI): 11 Mbps, 802.11g: 54Mbps wider-area wireless access, provided by telecom operator, 3G/4G, WAP/GPRS  Physical Media  Bit: propagates between transmitter/receiver pairs Physical link: what lies between transmitter \u0026amp; receiver  Guided media, by solid media  Twisted Pairs(tow insulated copper wires) Coaxial cable, two concentric copper conductors, bidirectional(baseband:single channel on cable, broadband: multiple channels on cable) Fiber optic cable: glass fiber carrying light pulses, each pulse a bit, high-speed operation, low error rate: repeaters spaced far apart, immune to electromagnetic noise   unguided media, no physical line (effected by reflection, obstruction of objects, interference)  terrestrial microwave (45 Mpbs channels) LAN (e.g. Wifi) wide-area (e.g. cellular) satellite(50 Mpbs channel, 270 millisecond end-end delay)      Network core  连接成网状 mesh of interconnected routers how is data transferred through net?  Circuit Switching: dedicated circular per call: telephone net packet-switching: data sent through net in discrete \u0026ldquo;chunks\u0026rdquo;    Circuit Switching  end to end resources reserved(保留一定的频宽) 保留 link bandwidth, switch capacity, divided into pieces dedicated resources: no sharing (别的core无法使用) circuit-like (guaranteed) performance call setup required(建连线)  two splits (TDM)   Packet Switching each end-end data stream divided into packets, user A,B packets share network resources, each packet uses full link bandwidth, resources used as needed\n resource contention (竞争)  aggregate resource demand can exceed amount available congestion: pockets queue, wait for link use store and forward, packets move one hop at a time(先排队， 后发送), transmit over link, wait turn at next link sequence of A \u0026amp; B packets done not have fixed pattern $\\rightarrow$ statistical multiplexing, in TDM each host get some slot in revolving TDM frame   move packets through routers form source to destination (path selection algorithms) datagram network:  destination address in packet determines next hop routes may change during session analogy, driving, asking directions   virtual circuit network  each packet carries tag (virtual circuit ID), tag determines next hop fixed path determined at call setup time, remains fixed through call routers maintain per-call state (每一个标签需要记住)    Packet Switching VS. Circuit Switching Packet switching allows more user to use network Example: 1Mbit link, each user 100kbps when \u0026ldquo;active\u0026rdquo;, active 10% of per time\n Circuit-Switching : 10 users Packet-Switching: with 35 users, probability \u0026gt; 10 active less than 0.004 Packet Switching: greater for bursty data, resource sharing, simpler, no call setup Excessive congestion: packet delay and loss, protocols needed for reliable data transfer, congestion control How to provide circuit-like behavior? bandwidth guarantees needed for audio/video apps, still an unsolved problem  Internet Structure: network of network  roughly hierarchical Tier 1 ISP(internet service provider) (e.g. UUNet, Sprint, AT\u0026amp;T) national/international coverage (treat each other as equals) regional ISP, connect to one or more tire-1 ISPs IXP (Internet Exchange point): meeting point where multiple ISPs can peer together   Delay, Loss in Packet-Switched Networks  Packets queue in router buffers  packet arrive rate to link exceeds output link capacity(packets queue, wait for turn)   types  Processing delay, the time required to examine the packet\u0026rsquo;s header and determine where to direct to packet queuing delay, the packet waits to be transmitted onto the link, depends on congestion level of router transmission delay, time to send bits into link L/R (L: packet length, R: link bandwidth), 多快的速度可以从(router)送出去 propagation delay, d/s = (s: propagation speed in medium, d = length of physical link)   Transmission and Propagation Delay  nodal delay $$d_{nodal} = d_{proc} + d_{queue} + d_{trans} + d_{prop}$$  Queueing delay  L: packet length (bits) a: average packet arrival rate R: link bandwidth(bps) transfer intensity = (L*a) / R (L*a) / R ~ 0 : average queueing delay small (L*a) / R $\\rightarrow$ 1: delays become large (L*a) /R \u0026gt; 1: more work arriving than can be serviced, average delay infinite, or packet loss  traceroute program: provides delay measurement from source to router along end-end internet path towards destination  Packet Loss  Queue(also known as buffer) preceding link in buffers has finite capacity when packet arrives to full queue, packet is dropped lost packet may be retransmitted by pervious node, by source and system, or not retransmitted at all  Protocol Layers  Is there any hope of organizing structure of network Layers: each layer implements a service  application: supporting network applications (FTP, SMTP, HTTP) transfer: host-host data transfer (TCP, UDP) network: routing of datagrams from source to destination. IP, routing protocols Link: data transfer between neighboring network elements (PPP, Ethernet) physical: bits \u0026ldquo;on the wire\u0026rdquo;   why layering:  explicit structure allows identification, relationship of complex system\u0026rsquo;s pieces modularization eases maintenance, updating of system, change of implementation of layer\u0026rsquo;s service transparent to rest of system.    Encapsulation ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"3d5587f7517c22667af727893ea96060","permalink":"/courses/computer_network/introduction/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/computer_network/introduction/","section":"courses","summary":"Computing devices Hosts (= end systems) (PC workstations, servers, Phones) running network apps Communication Links (fiber, copper, radio, satellite) physical link Transmission rate = bandwidth routers: forward packets (chunks of","tags":null,"title":"Introduction","type":"docs"},{"authors":null,"categories":null,"content":"Table of Contents\r 模块创建显示接口 函数/子程序作为参数传递 子程序传递多维数组 关键字参数和可选参数 过程接口和接口块 Generic 模块中用于过程的通用接口 内置模块 Fortran 指针 Mixing programming with C/C++  Prerequisite 数据类型兼容 按引用传递参数 ISO_C_BINDING 模块 Working with C++   Reference  \r模块创建显示接口 在模块中编译和调用程序时, 过程接口的所有细节对编译器都是有效的. 当调用程序时, 编译器可以自动检测过程调用中的参数个数, 类型, 是否参数是数组, 已经每个参数的 INTENT 属性. 在模块内, 编译过程和用USE关联访问过程具有一个显示接口 (explicit interface). Fortran 编译器清楚的知道过程每个参数的所有细节. 不在模块内的过程称为隐式接口 (implicit interface). Fortran 编译器调用程序时, 不知道这些过程的信息.\n函数/子程序作为参数传递 在调用和被调用函数中，函数被声明为外部量时(external), 用户自定义函数才可以当做调用参数传递．　当参数表中的某个名字被声明为外部变量，相当于告诉编译器在参数表中传递的是独立的已编译的函数.\n  函数作为参数传递\nprogram main\rimplicit none\rinterface ! must specify the interface\rsubroutine runfunc(fun, array)\rreal*8 :: array(:, :)\rreal*8, external :: fun\rend subroutine\rend interface\rreal*8 :: array(3,3)\rreal*8,external :: fun1\rarray = 2d0\rcall runfunc(fun1, array)\rend program\rsubroutine runfunc(fun, array)\rimplicit none\rinterface !must specify the interface\rreal*8 function fun(array)\rreal*8 :: array(:, :)\rend function\rend interface\rreal*8 :: array(:, :)\rprint*, fun(array)\rend subroutine\rreal*8 function fun1(array)\rimplicit none\rreal*8 :: array(:, :)\rinteger :: i, j\rfun1 = 0d0\rdo i = 1, size(array, 1)\rdo j = 1, size(array, 2)\rfun1 = fun1 + array(i,j)\renddo\renddo\rend function\r   子程序作为参数传递\nprogram main\rimplicit none\rinteger :: n = 5\rexternal :: add, prod\rreal*8 :: array(5) = (/1,2,3,4,5/)\rcall passsub(add, array, n)\rcall passsub(prod, array, n)\rend program\rsubroutine passsub(sub, array, n)\rinteger :: n\rexternal :: sub\rreal*8 :: array(n)\rcall sub(array, n)\rend subroutine\rsubroutine add(array, n)\rinteger :: n\rreal*8 :: array(n)\rinteger :: i\rreal*8 :: sum = 0\rdo i = 1, n\rsum = sum+ array(i)\renddo\rprint*, sum\rend subroutine\rsubroutine prod(array, n)\rinteger :: n\rreal*8 :: array(n)\rinteger :: i\rreal*8 :: cuml = 1\rdo i = 1, n\rcuml = cuml * array(i)\renddo\rprint*, cuml\rend subroutine\r   子程序传递多维数组  显示结构的形参数组 (explicit-shape dummy arrays)  subroutine process1(data1, data2, n, m)\rinteger, intent(in) :: n, m\rreal,intent(in),dimension(n,m) :: data1\rreal,intent(out), dimension(n,m) :: data2\rdata2 = 3 * data1\rend subroutine\r  不定结构形参数组 (assumed-shape dummy arrays)  数组中的每个下标都用冒号代替, 只有子程序或者函数具有显示接口时, 才能使用这种数组. 通常采用将子程序放入模块中, 然后在程序中使用该模块. 编译器可以从接口信息判断每个数组的大小和结构.\nmodule test_module\rcontains\rsubroutine process2(data1, data2)\rreal, intent(in), dimension(:,:) :: data1\rreal, intent(out), dimension(:, :) :: data2\rdata2 = 3 * data1\rend subroutine\rend module\r 关键字参数和可选参数   关键字参数 如果过程接口是显示的, 可以改变参数表中调用参数的顺序, 可以用关键字参数(keyword arguments) 提供更强的灵活性\nmodule procs\rcontains\rreal, function calc(first, second, third)\rimplicit none\rreal, intent(in) :: first, second, third\rcalc = (fisrt - second) / third\rend function calc\rend module\r 在调用时, 即可以用\n  program test_keywords\rimplicit none\rwrite(*, *) calc(3., 1., 2.)\rwrite(*, *) calc(first=3., second=1., third=2.)\rwrite(*, *) calc(3., third=2., second=1.)\r   可选参数\n通过在形参申明中加入 optional 属性, 指定可选参数 integer, intent(in), optioncal :: upper_limit 可选参数是否出现, 可以用 fortran 中的 present 来判断\n  module process\rcontains\rsubroutine extremes(a,n, maxval, pos_maxval)\rinteger, intent(in) :: n\rreal, intent(in), dimension(n) :: a\rreal, intent(out), optional :: maxval\rinteger, intent(out), optional :: pos_maxval\rinteger :: i\rreal :: real_max\rinteger :: pos_max\rreal_max = a(1)\rpos_max = 1\rdo i = 2, n\rmax: if (a(i) \u0026gt; real_max) then\rreal_max = a(i)\rpos_max = i\rend if max\renddo\rif (present(maxval)) then\rmaxval = real_max\rendif\rif (present(pos_maxval)) then\rpos_maxval = pos_max\rendif\rend subroutine\rend module\r 过程接口和接口块 想要使用不定结构形参数组或者可选参数这类 Fortran 高级特性, 程序必须要有显示接口. 创建显示接口最简单的方法是将过程放在模块中, 但是将过程放在模块中, 在一些场合不可能. 如过程和函数是用早期的 fortran 语言编写, 或者外部函数库是用 C 或者其他语言编写, 这样将过程放在一个模块中并不可能.\n当不能把过程放在模块中, fortran 允许在调用程序中定义一个接口块, 接口快指定了过程所有的接口特征, 编译器根据接口快中的信息执行一致性检查. 接口的一般形式\ninterface\rinterface_body_1\rinteface_body_2\r...\rend interface\r 使用接口时, 将它放在调用调用单元的最前面\nprogram interface_example\rimplicit none\rinterface\rsubroutine sort(a,n)\rimplicit none\rreal, dimension(:), intent(inout) :: a\rinteger, intent(in) :: n\rend sobroutine sort\rend interface\rreal, dimension(6) :: array = (/1., 5., 3., 2., 6., 4.,/)\rinteger :: nvals = 6\rcall sort(n = nvals, a = array)\rend program\r   当为大型旧版程序或者函数库创建接口时, 可以将它们放在调用一个模块中, 可以使用 Use 访问\nmodule interface_def ! no contains statement\rinterface\rsubroutine sort(array, n)\r...\rend surtoutine\rsubroutine sort2(array, n)\r...\rend subroutine\rend interface\rend module\r   接口是独立的作用域, 接口块中的形参变量必须单独声明, 即使这些变量已经在相关的作用域中已经被声明过了. Fortran 2003 含有 import 语句, 如果 import 语句出现在接口定义中, 那么import 语句中指明的变量会被导入, 不需要在接口中再次申明. 如果 import 语句中没有带变量, 所有变量都会被导入.\n Named entities from the host scoping unit are not accessible in an interface body that is not a module procedure interface body. The IMPORT statement makes those entities accessible in such interface body by host association.\n   Generic 使用通用接口块定义过程，过程之间通过不同类型的输入参数区分，可以处理不同类型的数据. 给　interface 加入一个通用名，那么在接口快定义的每个过程接口可以看作一个通过过程．通用过程中所有过程要么都是子程序，要么都是函数.\n例如对于不同类型的数据进行排序，创建一个通用子程序 sort 实现排序，可以使用通用接口块\ninterface sort\rsubroutine sorti(array, nvals)\r...\rend subroutine sorti\rsubroutine sortr(array, nvals)\r...\rend subroutine sortr\r...\rsubroutine\rend interface\r 模块中用于过程的通用接口 如果子程序都在一个模块中，并且已经有显示接口，如果再加入过程接口，这样是非法的，不能用上述方式添加通用接口．fortran 包含了一个可用在通用接口模块中的特殊module procedure 语句．\n如果４个排序子程序定义在一个模块中，　子程序sort 通用接口是\ninterface sort\rmodule procedure sorti\rmodule procedure sortr\rmodule procedure sortd\r...\rend interface sort\r 例如，找出一个数组的最大值和最大值位置，不管数组的类型\nmodule generic_maxval\rimplicit none\rinterface maxval\rmodule procedure maxval_i\rmodule procedure maxval_r\r...\rend interface\rcontains\rsubroutine maxval_i (array, nvals, value_max, pos_maxval)\rimplicit none\rinteger, intent(in) :: nvals\rinteger, intent(in), dimension(nvals) :: array\rinteger, intent(out) :: value_max\rinteger, intent(out), optional :: pos_maxval\rinteger :: i\rinteger :: pos_max\rvalue_max = array(i)\rpos_max = 1\rdo i = 2, nvals\rif (array(i) \u0026gt; value_max) then\rvalue_max = array(i)\rpos_max = i\rend if\renddo\rif (present(pos_maxval)) then\rpos_maxval = pos_max\rend if\rend sobroutine maxval_i\rsubroutine maxval_r(array, nvals, value_max, pos_maxval)\r...\rend subroutine\r...\rend module generic_maxval\r 内置模块 ortran 2003 定义了内置模块(intrinsic module), 这种模块是由 fortran 编译器创造者预定义和编写的．fortran　2003 中，有三种内置模块\n ISO_FORTRAN_ENV 定义了描述特定计算机中存储器特征的向量(标准的整型包含多少bit, 标准字符包含多少bit),以及该机器所定义的I/O 单元 ISO_C_BINDING 包含了FORTRAN 编译其和特定处理器的c 语言互操作是必要数据 IEEE 处理器运行浮点数的特征  Fortran 指针  Fortran 申明变量为指针时, 需要使用 Pointer 属性, 并申明指针的类型 在 Fortran 变量的类型定义语句中, 加入Target属性, 将其声明为目标变量  Program test_ptr\rimplicit none\rreal, pointer::p\rreal, target :: t1 = 10, t2=-17\rp =\u0026gt; t1\rprint*, p, t1, t2\rp =\u0026gt; t2\rprint*, p, t1, t2\rend program\r  指针有三种状态, undefined, associated, disassociated, 通过使用 NuLLIFY 语句将指针和所有目标变量断开 指针数组. 指向数组的指针必须声明其指向数组的类型和维数, 不需要申明每一维的宽度  \rMixing programming with C/C++ Prerequisite  在C中, 所有子程序都是函数, void 类型函数不返回值 在 fortran 中, 函数会传递一个返回值, 子程序不传递返回值 Fortran 调用 C 函数时  被调用的C函数返回一个值, 会从Fortran 中将其作为函数来调用 被调用的C函数不返回值, 将其作为子程序来调用   C 函数调用 Fortran 子程序时  如果被调用Fortran 子程序是一个函数, 会从 C 中将其作为一个返回兼容数据类型的函数来调用 如果被调用的 Fortran 子程序是一个子例程, 会从 C 中将其作为一个返回 int 或 void 值的函数来调用. 如果Fortran 子例程使用交替返回 (返回值为 return 语句中的表达式), 会返回一个值. 如果 return 语句中没有出现表达式, 在Subroutine 语句中申明了交替返回, 则返回 0   C 区分大小写, Fortran 忽略大小写  在 C 子程序中, 使C函数名全部小写 用 -U 选项编译 Fortran 程序, 会告知编译器保留函数/子程序名称现有大小写   Fortran 在编译时,在子程序名和末尾加入下划线, C 编译时函数名称和用户指定名称一致. (Fortran 对于块过程名有两个前导下划线, 减少与用户指定子例程名的冲突)  在 C 函数中, 通过在函数名末尾添加下划线更改名称 使用 BIND(C) 属性声明表明外部函数 是C 语言函数 使用 -ext_names 选项编译对于无下划线外部名称引用   Fortran 传递字符型参数时, 会传递一个附加参数, 指定字符串的长度, 这个参数在 C 中为 long int 量, 按值进行传递 C 数组从 0 开始, Fortran 数组默认以 1 开始 C 数组按行主顺序存储, Fortran 按列主顺序存储  数据类型兼容  C 数据类型 int, long int, long 在 32 位环境下是等价的 (4 字节). 在 64 位环境中, long 和指针为8 字节 数组和结构的元素及字段需要兼容 不能按值传递数组, 字符串和结构 按值传递时, 可以使用 %VAL(arg), 或者Fortran95 程序具有显含的接口块, 使用 VALUE 属性声明了伪参数 数据大小与对齐, 参考 https://docs.oracle.com/cd/E19205-01/819-5262/6n7bvdr18/  按引用传递参数  简单数据类型 string structure array 按值传递参数  代码 example, 参考 https://github.com/zhanghaomiao/C_FORTRAN_MIX\nISO_C_BINDING 模块 Fortran 2003 提供了内置模块,处理C 和 fortran 数据类型的转化\n c_associated(c_ptr1[, c_ptr_2]): determines the status of the C pointer c_ptr_1 or if c_ptr_1 is associated with the target c_ptr_2  subroutine association_test(a,b)\ruse iso_c_binding, only: c_associated, c_loc, c_ptr\rimplicit none\rreal, pointer :: a\rtype(c_ptr) :: b\rif(c_associated(b, c_loc(a))) \u0026amp;\rstop 'b and a do not point to same target'\rend subroutine association_test\r  c_f_pointer(cptr, fptr[,shape]) Assign the target, the C pointer, cptr to Frotran pointer. c_f_procpointer(cptr, fptr) assigns the target of the C function pointer cptr to the Fortran procedure pointer fptr c_funloc(x) determine the C address of the argument, return type is c_funptr c_loc(x) determine the C address of the argument, return type is c_ptr c_sizeof(x) calculate the number of bytes of storage the expression x occupies  use iso_c_binding\rreal(c_float) :: r, s(5)\rprint *, (c_sizeof(s)/c_sizeof(r) == 5)\rend\r 数据类型的对应关系,参考 Fortran wiki\nWorking with C++  在使用C++程序时, 需要加入 extern \u0026lsquo;C\u0026rsquo; 关键字 在使用到 C++ 标准库的数据结构时, 链接时需要加入标准C++库 (-lstdc++) Example \r  Reference  http://fortranwiki.org/fortran/show/iso_c_binding https://gcc.gnu.org/onlinedocs/gfortran/Interoperability-with-C.html#Interoperability-with-C https://stackoverflow.com/questions/22813423/c-f-pointer-does-not-work http://www.yolinux.com/TUTORIALS/LinuxTutorialMixingFortranAndC.html https://docs.oracle.com/cd/E19205-01/819-5262/6n7bvdr18/  ","date":1555347585,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583915506,"objectID":"562b0146bd6ce3bae22cb7045971f067","permalink":"/post/fortran-programming/","publishdate":"2019-04-15T16:59:45Z","relpermalink":"/post/fortran-programming/","section":"post","summary":"Fortran 易混淆语法","tags":["Fortran","C"],"title":"fortran programming","type":"post"},{"authors":null,"categories":["Learn"],"content":"  要求\n 所有的程序编译时需要使用 -g 参数， 这样gdb程序可以识别 编译时不要使用优化，使用优化时一些变量的值无法跟踪， 即优化等级设置为 O0    命令\n b 后加行数， 设置断点 li(start, [end]) 显示在start 和 end 之间的代码， end 为可选参数， 如果没有 end, 则显示以start 为中心前后5句代码 r 重新开始调试程序, 在遇到断点处终止 c 开始调试程序，在遇到断点处终止, 与 step into my code 功能类似 s 可后接参数 N ， 表示step n 次， 与 step into 功能类似 n 可后接参数 N ， 表示next n 次， 与 step over 功能类似 fin 从子程序跳出， 与 step out 功能类似 p 打印变量值  对于一维数组， 使用 p temp(n) 会显示temp第n个元素 对于二维数组， 使用 p temp(i:j) 会显示第 i 和 第 j 行的元素   where 显示当前程序所处位置 info b 显示所设置的断点 del breakpoints 删除断点    ","date":1552348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552348800,"objectID":"1e2c1f5b902d2a905bbf1de4a067881f","permalink":"/post/gdb%E8%B0%83%E8%AF%95%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/","publishdate":"2019-03-12T00:00:00Z","relpermalink":"/post/gdb%E8%B0%83%E8%AF%95%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/","section":"post","summary":"主要介绍有关GDB一些基本命令","tags":["computer"],"title":"GDB 调试","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic \rAcademic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click \rPDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? \rAsk\n\rDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598336322,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583858865,"objectID":"9a6cb9348361050ffbcc0117246adb56","permalink":"/tag/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/tag/","section":"","summary":"Hello!","tags":null,"title":"Landing Page","type":"widget_page"}]