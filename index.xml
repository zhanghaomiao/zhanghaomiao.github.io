<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Haomiao</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Haomiao</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 19 Aug 2020 09:43:23 +0800</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Haomiao</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Foundations of Convolutional Neural Networks</title>
      <link>/courses/deep_learning/cnn/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0100</pubDate>
      <guid>/courses/deep_learning/cnn/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;一般计算机视觉问题包含以下几类:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;图像分类(Image classification)&lt;/li&gt;
&lt;li&gt;目标检测(Object Detection)&lt;/li&gt;
&lt;li&gt;风格转换(Neural Style Transfer)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;面临计算机视觉问题时，如何处理图像数据变得很重要，对于一般图片 $1000 \times 1000 \times 3$, 神经网络的输入层将有 $300,000,000$ 数据, 那么网络权重 $W$ 则很大，这会导致两个后果:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据量很少，但是神经网络很复杂，容易出现过拟合&lt;/li&gt;
&lt;li&gt;所需要的内存和计算量很大&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;卷积运算&#34;&gt;卷积运算&lt;/h2&gt;
&lt;p&gt;神经网络从浅层到深层，可以检测出图片的边缘特征，局部特征，最后一层则识别整体的面部轮廓， 这些都是通过卷积神经网络实现的&lt;/p&gt;
&lt;h3 id=&#34;边缘检测&#34;&gt;边缘检测&lt;/h3&gt;
&lt;p&gt;垂直检测(Vertical Edges) and 水平检测(Horizontal Edges), 如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/edge_example.png&#34; width=&#34;350px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;边缘检测可以通过相应的 filter 实现，如下图所示&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/Vertical-Edge-Detection.jpg&#34; width=&#34;350px&#34; /&gt;
原始图片尺寸为 6x6, 通过中间的矩阵之后，尺寸为 4x4. 按照如图所示的方式进行计算，卷积运算的求解是在原始矩阵中选择与 filter 相同大小的区域，一一对应相乘后求和，将所得的结果构成一个矩阵&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;下面是一个示例
&lt;img src=&#34;/img/cnn/Convolutional-operation-example.jpg&#34; width=&#34;350px&#34; /&gt;
将矩阵看做一个图像，最右边中间的亮的区域对应于最左边图像中间的黑白分界线&lt;br&gt;
下面一张图，表示的更加明显
&lt;img src=&#34;/img/cnn/Convolutional-operation.jpg&#34; width=&#34;350px&#34; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;vertical filter and horizontal filter
&lt;img src=&#34;/img/cnn/Vertical-and-Horizontal-Filter.png&#34; width=&#34;350px&#34; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sobel filter and Scharr filter, 增加了中间行的权重， 可以提高结果的稳定性
&lt;img src=&#34;/img/cnn/Sobel-Filter-and-Scharr-Filter.png&#34; width=&#34;350px&#34; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;filter 中的值可以设置为参数，通过模型训练得到，神经网络通过反向传播学习到一些低级特性，使得可以对图像的边缘进行检测，不仅仅局限于水平和垂直方向&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;padding&#34;&gt;Padding&lt;/h2&gt;
&lt;p&gt;假设输入图像大小为 $n\times n$, filter 大小为 $f\times f$, 则输出图片大小为 $(n-f+1) \times (n-f+1)$&lt;/p&gt;
&lt;p&gt;因此，存在两个问题&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;卷积运算后，图像的尺寸会减小&lt;/li&gt;
&lt;li&gt;原始图片的角落，边缘地区的输出采样很少，因此很多边缘地区的信息被丢失&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一般可以使用 padding 方式，对图像进行填充, 通常填充的值为0
&lt;img src=&#34;/img/cnn/Padding.jpg&#34; width=&#34;350px&#34; /&gt;
如果对每个方向的padding 数为 p, 则padding后的图像大小 $(n+2p) \times (n+2p)$, 通过 $f\times f$的filter 之后， 大小则为 $(n+2p-f+1)\times (n+2p-f+1)$&lt;/p&gt;
&lt;p&gt;进行卷积操作时，有两种选择&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;valid 卷积： 没有 padding, 直接卷积， $(n-f+1) \times (n-f+1)$&lt;/li&gt;
&lt;li&gt;same 卷积，进行 padding, 使得卷积后结果大小与输入大小一致，此时有 $p = \frac{f-1}{2}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f 通常为奇数， 原因包括 same 卷积 中可以得到整数，并且 filter 有一个可以表示其所在位置的中心点&lt;/p&gt;
&lt;h2 id=&#34;stride&#34;&gt;Stride&lt;/h2&gt;
&lt;p&gt;通过设置步长来压缩一些信息， 步长表示 filter 在原始图像的水平方向和垂直方向上每次移动的距离。步长设置为2的卷积过程表示为
&lt;img src=&#34;/img/cnn/Stride.jpg&#34; width=&#34;350px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;设步长为 $s$, 填充长度为 $p$, 输入图片大小为 $n\times n$, filter 大小为 $f\times f$, 则卷积后的图片大小为 $$\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor \times\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor$$
向下取整表示原始矩阵的蓝框完全包含在图像内部时，才进行计算.&lt;/p&gt;
&lt;p&gt;目前为止我们学习的“卷积”实际上被称为互相关（cross-correlation），而非数学意义上的卷积。真正的卷积操作在做元素乘积求和之前，要将滤波器沿水平和垂直轴翻转（相当于旋转 180 度）&lt;/p&gt;
&lt;h2 id=&#34;高维卷积&#34;&gt;高维卷积&lt;/h2&gt;
&lt;p&gt;如果对三通道的RGB进行卷积运算，对应的 filter 也是三通道的，这有点类似我们的眼镜，是 see through 这三个维度，
对每个通道 (R, G, B) 与对应的 filter 进行卷积运算求和，然后将三个通道的值相加，如下图所示，是对 27 个乘积和作为一个输出&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/Convolutions-on-RGB-image.png&#34; width=&#34;400px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;如果想进行更多的边缘检测，可以增加 filter 的数量，如下图所示&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/More-Filters.jpg&#34; width=&#34;400px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;设输入图片的尺寸 $n \times n \times n_c$ , filter 的尺寸为 $f\times f \times n_c$, 则卷积运算后的图片尺寸为 $(n-f+1) \times (n-f+1) \times n_c&#39;$,   $n_c&#39;$ 为 filter 的数目&lt;/p&gt;
&lt;h2 id=&#34;单层卷积神经网络&#34;&gt;单层卷积神经网络&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/One-Layer-of-a-Convolutional-Network.jpg&#34; width=&#34;400px&#34; /&gt;
单层卷积神经网络多了激活函数和偏移量，filter 的数值对应 $W^{[l]}$, 卷积运算对应 $W^{[l]}$ 与 $A^{[l-1]}$的乘积
$$\begin{array}{c}Z^{[l]}=W^{[l]} A^{[l-1]}+b \\ A^{[l]}=g^{[l]}\left(Z^{[l]}\right)\end{array}$$&lt;/p&gt;
&lt;p&gt;对于 $3 \times 3$的 filter, 总共有 28 各参数，不予图片尺寸相关，因此 CNN 的参数相较于标准神经网络要小很多&lt;/p&gt;
&lt;h3 id=&#34;符合表示&#34;&gt;符合表示&lt;/h3&gt;
&lt;p&gt;设 $l$ 层为卷积层：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$f^{[l]}$：&lt;strong&gt;滤波器的高（或宽）&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p^{[l]}$：&lt;strong&gt;填充长度&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$s^{[l]}$：&lt;strong&gt;步长&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$n^{[l]}_c$：&lt;strong&gt;滤波器组的数量&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;输入维度&lt;/strong&gt;：$n^{[l-1]}_H \times n^{[l-1]}_W \times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_H$表示输入图片的高，$n^{[l-1]}_W$表示输入图片的宽。之前的示例中输入图片的高和宽都相同，但是实际中也可能不同，因此加上下标予以区分。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;输出维度&lt;/strong&gt;：$n^{[l]}_H \times n^{[l]}_W \times n^{[l]}_c$ 。其中&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$n^{[l]}_H = \biggl\lfloor \frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \biggr\rfloor$$&lt;/p&gt;
&lt;p&gt;$$n^{[l]}_W = \biggl\lfloor \frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \biggr\rfloor$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;每个滤波器组的维度&lt;/strong&gt;：$f^{[l]} \times f^{[l]} \times n^{[l-1]}_c$ 。其中$n^{[l-1]}_c$ 为输入图片通道数（也称深度）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;权重维度&lt;/strong&gt;：$f^{[l]} \times f^{[l]} \times n^{[l-1]}_c \times n^{[l]}_c$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;偏置维度&lt;/strong&gt;：$1 \times 1 \times 1 \times n^{[l]}_c$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于深度学习的相关文献并未对卷积标示法达成一致，因此不同的资料关于高度、宽度和通道数的顺序可能不同。有些作者会将通道数放在首位，需要根据标示自行分辨。&lt;/p&gt;
&lt;h2 id=&#34;简单神经网络&#34;&gt;简单神经网络&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/Simple-Convolutional-Network-Example.jpg&#34; width=&#34;400px&#34; /&gt;
典型的神经网络包含三层， Convolution layer, Pooling layer and Fully Connected Layer.&lt;/p&gt;
&lt;h2 id=&#34;pooling-layer&#34;&gt;Pooling Layer&lt;/h2&gt;
&lt;p&gt;作用是减小模型大小，提高计算速度， 以及减小噪声，提高提取特征的稳健性。  采用最多的一种 Pooling 方式是 Max Pooling, 如下图所示
&lt;img src=&#34;/img/cnn/Max-Pooling.png&#34; width=&#34;400px&#34; /&gt;
另外一种是 Average Pooling, 求区域的平均值
&lt;img src=&#34;/img/cnn/Average-Pooling.png&#34; width=&#34;400px&#34; /&gt;
Pooling 设有一组超参数，即 filter 大小，步长 s, 选用 MaxPooling 或者 AveragePooling 但并没有参数需要学习，&lt;/p&gt;
&lt;p&gt;设 Pooling 之前输入的维度为 $n_H \times n_W \times n_c$
则输出为&lt;/p&gt;
&lt;p&gt;$$\left\lfloor\frac{n_{H}-f}{s}+1\right\rfloor \times\left\lfloor\frac{n_{W}-f}{s}+1\right\rfloor \times n_{c}$$&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/CNN-Example.jpg&#34; width=&#34;700px&#34; /&gt;
各个层的参数表示为&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Activation shape&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Activation Size&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;#parameters&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Input:&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(32, 32, 3)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3072&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CONV1(f=5, s=1)&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(28, 28, 6)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4704&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;158&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;POOL1&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(14, 14, 6)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1176&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CONV2(f=5, s=1)&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(10, 10, 16)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1600&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;416&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;POOL2&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(5, 5, 16)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;FC3&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(120, 1)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;120&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;FC4&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(84, 1)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10164&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Softmax&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(10, 1)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;850&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;为什么使用卷积&#34;&gt;为什么使用卷积&lt;/h2&gt;
&lt;p&gt;卷积过程有效的减小了CNN 参数数量，主要通过两种方式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;参数共享（Parameter sharing）&lt;/strong&gt;：特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;稀疏连接（Sparsity of connections）&lt;/strong&gt;：在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值
池化过程则在卷积后很好地聚合了特征，通过降维来减少运算量。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Application Layer</title>
      <link>/courses/computer_network/application_layer/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/computer_network/application_layer/</guid>
      <description>&lt;p&gt;Creating a network application, run on different end systems and communicate over a network, no software written for devices in network core, network core devices do not function at application layer (e.g. Web server software communicate with browser software)&lt;/p&gt;
&lt;h2 id=&#34;principle-of-network-application&#34;&gt;Principle of network application&lt;/h2&gt;
&lt;h3 id=&#34;application-architectures&#34;&gt;Application architectures&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Client-server
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c9726afec632.png&#34; width=&#34;300px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;server: always-on host, permanent IP address, server forms for scaling&lt;/li&gt;
&lt;li&gt;clients: communicate with server, may be intermittently connected, may have dynamic IP address, do not communicate directly with each other&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;peer-to-peer
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c972763ee74f.png&#34; width=&#34;300px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;no always on server&lt;/li&gt;
&lt;li&gt;arbitrary end systems directly communicate&lt;/li&gt;
&lt;li&gt;peers are intermittently connected and change IP address, highly scalable, but difficult to manage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hybrid of client-server and P2P
&lt;ul&gt;
&lt;li&gt;Napster, file transfer P2P, file search centralized(peer register content at central server, peers query same central server to locate content)&lt;/li&gt;
&lt;li&gt;Instant message: chatting between two users is P2P, presence detection/location centralized (User registers its IP address with central server when it comes online,User contacts central server to find IP addresses of buddies)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;processes-communicating&#34;&gt;Processes Communicating&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;process: program running within a host&lt;/li&gt;
&lt;li&gt;with same host, two processes communicate using inter-process communication (defined by OS)&lt;/li&gt;
&lt;li&gt;processes in different hosts communicated by exchanging messages (Client process: process that initiates communication, Server process: process that waits to be contacted)&lt;/li&gt;
&lt;li&gt;figure
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c972a03ee1fb.png&#34; width=&#34;450px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;process sends/receives messages to/from its socket&lt;/li&gt;
&lt;li&gt;socket analogous to door, sending process relies on transport infrastructure on other side of door which brings message to socket at receiving process&lt;/li&gt;
&lt;li&gt;API (1) choice of transport protocol (2) ability to fix a few parameters&lt;/li&gt;
&lt;li&gt;addressing process(判断哪一个process需要沟通)
&lt;ul&gt;
&lt;li&gt;For a process to receive messages, it must have an identifier&lt;/li&gt;
&lt;li&gt;Every host has a unique 32-bit IP address&lt;/li&gt;
&lt;li&gt;Identifier includes both the IP address and port numbers(16-bit) associated with the process on the host (HTTP server: 80, Mail server: 25)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;app-layer-protocol-defines&#34;&gt;App-layer protocol defines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Types of messages exchanged (e.g. request &amp;amp; response)&lt;/li&gt;
&lt;li&gt;Syntax of message types (what fields in message &amp;amp; how field are delineated)&lt;/li&gt;
&lt;li&gt;Semantics of the field (i.e. meaning of information in fields)&lt;/li&gt;
&lt;li&gt;Rules for when and how processes send &amp;amp; respond to messages&lt;/li&gt;
&lt;li&gt;Public-domain protocols:
&lt;ul&gt;
&lt;li&gt;defined in RFCs&lt;/li&gt;
&lt;li&gt;allows for interoperability(相容)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-transport-service-does-provide&#34;&gt;What transport service does provide&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data loss
&lt;ul&gt;
&lt;li&gt;some apps can tolerate some loss&lt;/li&gt;
&lt;li&gt;other apps require 100% reliable data transfer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Timing
&lt;ul&gt;
&lt;li&gt;some apps require low delay to be &amp;ldquo;effective&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bandwidth
&lt;ul&gt;
&lt;li&gt;some apps require minimum amount of bandwidth to be (games) effective(multimedia)&lt;/li&gt;
&lt;li&gt;other apps(&amp;ldquo;elastic apps&amp;rdquo;) make use of whatever bandwidth they get
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c972dff3be06.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TCP service: connection-oriented, reliable transport, flow control, congestion control (does not providing: timing, minimum bandwidth guarantees)&lt;/li&gt;
&lt;li&gt;UDP service: unreliable data transfer between sending and receiving process, dons not provide connection setup, reliability, flow control, congestion control, timing or bandwidth guarantee
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c9731b3899d9.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;web-and-http&#34;&gt;Web and HTTP&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Web
&lt;ul&gt;
&lt;li&gt;web page consists of objects&lt;/li&gt;
&lt;li&gt;An object is a file such as HTML, file ,a JPEG image&lt;/li&gt;
&lt;li&gt;A web page consists of a bast HTML-file and several referenced object&lt;/li&gt;
&lt;li&gt;The base HTML file references the other objects in the page with the object&amp;rsquo;s URLs (Uniform Resource Locators)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;http-overview&#34;&gt;HTTP Overview&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;HTTP: hypertext transfer protocol&lt;/li&gt;
&lt;li&gt;web&amp;rsquo;s application layer protocol&lt;/li&gt;
&lt;li&gt;client/server model (HTTP1.0, HTTP1.1)&lt;/li&gt;
&lt;li&gt;procedure
&lt;ol&gt;
&lt;li&gt;client initiates TCP connection (creates socket) to server&lt;/li&gt;
&lt;li&gt;server accepts TCP connection from client&lt;/li&gt;
&lt;li&gt;HTTP message exchanged between browser and web server&lt;/li&gt;
&lt;li&gt;TCP connection closed&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;HTTP is stateless, server maintains no information about post client requests&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;http-connections&#34;&gt;HTTP connections&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Non-persistent HTTP
&lt;ul&gt;
&lt;li&gt;At most one object is sent over a TCP connection&lt;/li&gt;
&lt;li&gt;HTTP/1.0 uses non-persistent HTTP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Persistent HTTP
&lt;ul&gt;
&lt;li&gt;Multiple objects can be sent over singe TCP connection between client and server&lt;/li&gt;
&lt;li&gt;A new connection need not be set up for the transfer of each Web object&lt;/li&gt;
&lt;li&gt;HTTP/1.1 use persistent connections in default mode&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;response-time-modeling-non-persistent-http&#34;&gt;Response time modeling (Non-persistent HTTP)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;RTT: time to send a small packet to travel from client to server and back&lt;/li&gt;
&lt;li&gt;Response Time: one RTT to initiate TCP connection, one RTT for HTTP request and first few bytes of HTTP response to return, file transmission time&lt;/li&gt;
&lt;li&gt;file transmission time = 2RTT + transmit time
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c9736941a772.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;two-versions-of-persistent-connections&#34;&gt;Two versions of persistent connections&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Persistent without pipelining
&lt;ul&gt;
&lt;li&gt;client issues new request only when previous response has been received&lt;/li&gt;
&lt;li&gt;One RTT for each referenced object&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Persistent with pipelining
&lt;ul&gt;
&lt;li&gt;default in HTTP/1.1&lt;/li&gt;
&lt;li&gt;client sends requests as soon as it encounters a referenced object&lt;/li&gt;
&lt;li&gt;as little as one RTT for all the referenced objects&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;http-request-message&#34;&gt;HTTP request message&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;two types of HTTP messages: request, response&lt;/li&gt;
&lt;li&gt;HTTP request message (ASCII format)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-HTTP&#34;&gt;GET /somedir/page.html HTTP/1.1
Host: www.someschool.edu
Connection: close
User-agent: Mozilla/5.0
Accept-language: fr
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c9739d016ad7.png&#34; width=&#34;500px&#34;/&gt;
&lt;h4 id=&#34;method-types&#34;&gt;Method Types&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;HTTP/1.0
&lt;ul&gt;
&lt;li&gt;GET: return the object&lt;/li&gt;
&lt;li&gt;POST: send information to be stored on the server&lt;/li&gt;
&lt;li&gt;Head: return only information about the object, such as how old it is, but not the object itself&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;HTTP/1.1
&lt;ul&gt;
&lt;li&gt;GET, POST, HEAD&lt;/li&gt;
&lt;li&gt;PUt: uploads a new copy of existing object in entity body to path specified in URL field&lt;/li&gt;
&lt;li&gt;DELETE: delete object specified in the URL field&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;http-response-message&#34;&gt;HTTP response message&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A status line&lt;/strong&gt;,  which indicates the success of failure of the request&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Header line&lt;/strong&gt;, A description of the information in the response, this is the metadata or meta information&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;actual information&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;response status code
&lt;ul&gt;
&lt;li&gt;200 OK&lt;/li&gt;
&lt;li&gt;300 Moved permanently&lt;/li&gt;
&lt;li&gt;400 Bad request&lt;/li&gt;
&lt;li&gt;404 Not found&lt;/li&gt;
&lt;li&gt;505 HTTP version not supported
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c973bc5afdcd.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;user-server-interaction-cookies&#34;&gt;User-Server Interaction: Cookies&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;it is often desirable for a Web site to identify users&lt;/li&gt;
&lt;li&gt;Four components of cookie technology:
&lt;ul&gt;
&lt;li&gt;cookie header line in the HTTP response message&lt;/li&gt;
&lt;li&gt;cookie header line in HTTP request message&lt;/li&gt;
&lt;li&gt;cookie file kept on user&amp;rsquo;s host and managed by user&amp;rsquo;s browser&lt;/li&gt;
&lt;li&gt;back-end database at web site
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c973e402af4c.png&#34; width=&#34;450px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;advantages
&lt;ul&gt;
&lt;li&gt;authorization&lt;/li&gt;
&lt;li&gt;shopping carts&lt;/li&gt;
&lt;li&gt;recommendations&lt;/li&gt;
&lt;li&gt;user session state (Web  e-mail)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cookies and privacy:
&lt;ul&gt;
&lt;li&gt;Permit sites to learn a lot about you&lt;/li&gt;
&lt;li&gt;you may supply name and e-mail to sites&lt;/li&gt;
&lt;li&gt;search engines use redirection &amp;amp; cookies to learn yet more&lt;/li&gt;
&lt;li&gt;advertising companies obtain info across sites&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;web-caches-proxy-server&#34;&gt;Web caches (proxy server)&lt;/h3&gt;
&lt;p&gt;Satisfy client request without involving origin server
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c973f6c5ec0d.png&#34; width=&#34;400px&#34;/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User sets browser: web accesses via cache&lt;/li&gt;
&lt;li&gt;browser sends all HTTP requests to cache
&lt;ul&gt;
&lt;li&gt;object in cache: cache returns object&lt;/li&gt;
&lt;li&gt;else cache requests object from origin server, then returns object to client&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cache acts as both client and server&lt;/li&gt;
&lt;li&gt;Typically cache is installed by ISP&lt;/li&gt;
&lt;li&gt;why web caching
&lt;ul&gt;
&lt;li&gt;Reduce response for client request&lt;/li&gt;
&lt;li&gt;Reduce traffic&lt;/li&gt;
&lt;li&gt;Internet dense with caches enables poor content providers to effectively deliver content&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conditional-get-client-side-caching&#34;&gt;Conditional GET: client-side caching&lt;/h3&gt;
&lt;p&gt;Don&amp;rsquo;t send object if client has up-to-date cached version&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;client: specify date of cached copy in HTTP (&lt;code&gt;If-Modified-Since &amp;lt;data&amp;gt;&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;server: response contains no object if cached copy is up-to-date &lt;code&gt;HTTP/1.1 304 Not Modified&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ftp-file-transfer-protocol&#34;&gt;FTP (File transfer protocol)&lt;/h2&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c9744679515d.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;transfer file to/from remote host&lt;/li&gt;
&lt;li&gt;client/server model
&lt;ul&gt;
&lt;li&gt;client side: the side that initiates transfer&lt;/li&gt;
&lt;li&gt;server side: remote host&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ftp : RFC 959&lt;/li&gt;
&lt;li&gt;ftp server port 21&lt;/li&gt;
&lt;li&gt;Procedure
&lt;ol&gt;
&lt;li&gt;FTP client contacts FTP server at port 21, specifying TCP as transport protocol&lt;/li&gt;
&lt;li&gt;Client obtains authorization over control connection&lt;/li&gt;
&lt;li&gt;Client browses remote directory by sending commands over control connection&lt;/li&gt;
&lt;li&gt;When server receives a command for a file transfer, the server opens a  TCP data connection to client&lt;/li&gt;
&lt;li&gt;After transferring one file, server closes connection&lt;/li&gt;
&lt;li&gt;Servers open a second TCP data connection to transfer another file&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;FTP server maintains &amp;ldquo;state&amp;rdquo;: current directory&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ftp-commands-responses&#34;&gt;FTP Commands, responses&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;commands
&lt;ul&gt;
&lt;li&gt;USER, username&lt;/li&gt;
&lt;li&gt;PASS, password&lt;/li&gt;
&lt;li&gt;LIST, return list of file in current directory&lt;/li&gt;
&lt;li&gt;RETR filename &amp;ndash; retrieves (gets) file&lt;/li&gt;
&lt;li&gt;STOR filename &amp;ndash; stores file onto remote host&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;response
&lt;ul&gt;
&lt;li&gt;331 Username OK, password required&lt;/li&gt;
&lt;li&gt;125 data connection already open, transfer starting&lt;/li&gt;
&lt;li&gt;425 can&amp;rsquo;t open data connection&lt;/li&gt;
&lt;li&gt;452 Error writing file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;electronic-mail-smtp-pop3-imap&#34;&gt;Electronic Mail (SMTP, POP3, IMAP)&lt;/h2&gt;
&lt;p&gt;Three major components of a mail system user agents, mail servers ,simple mail transfer protocol: SMTP
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c974714a2626.png&#34; width=&#34;450px&#34;/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User Agent:
&lt;ul&gt;
&lt;li&gt;known as &amp;ldquo;mail reader&amp;rdquo;&lt;/li&gt;
&lt;li&gt;composing, editing, reading mail messages&lt;/li&gt;
&lt;li&gt;outgoing, incoming message stored on server&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mail server:
&lt;ul&gt;
&lt;li&gt;mailbox: contains incoming messages for user&lt;/li&gt;
&lt;li&gt;message queue: of outgoing (to be sent) mail messages&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;protocol&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;smtp-protocol&#34;&gt;SMTP protocol&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;client: sending mail receiver&lt;/li&gt;
&lt;li&gt;server: receiving mail server&lt;/li&gt;
&lt;li&gt;use TCP to reliably transfer email message from client to server, port25&lt;/li&gt;
&lt;li&gt;direct transfer: sending server to receiving server&lt;/li&gt;
&lt;li&gt;three phases of transfer
&lt;ol&gt;
&lt;li&gt;handshaking&lt;/li&gt;
&lt;li&gt;transfer of messages&lt;/li&gt;
&lt;li&gt;closure&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;command/response interaction
&lt;ul&gt;
&lt;li&gt;command: ASCII text&lt;/li&gt;
&lt;li&gt;response: status code and phrase&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SMTP use persistent connections&lt;/li&gt;
&lt;li&gt;SMTP requires message to be in 7-bit ASCII&lt;/li&gt;
&lt;li&gt;SMTP server uses &lt;code&gt;CRLF.CRLT&lt;/code&gt; to determine end of message （&lt;code&gt;CRLF&lt;/code&gt; 表示换行）
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c9748876c1b9.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;comparison with HTTP
&lt;ul&gt;
&lt;li&gt;HTTP pull protocol (client&amp;rsquo;s point of view), SMTP(push protocol)&lt;/li&gt;
&lt;li&gt;both have ASCII command/response interaction, status codes&lt;/li&gt;
&lt;li&gt;HTTP dons not require message to be in 7-bit ASCII&lt;/li&gt;
&lt;li&gt;HTTP: one object in  one response message&lt;/li&gt;
&lt;li&gt;SMTP: multiple objects can be set in one message&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;message-format&#34;&gt;Message format&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;From, To, Subject&lt;/li&gt;
&lt;li&gt;MIME(Multipurpose Internet Mail extensions)&lt;/li&gt;
&lt;li&gt;additional lines in message header declare MIME content type&lt;/li&gt;
&lt;li&gt;MIME types: (TEXT, Image, Audio, Video, Application, Multipart, Message)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mail-access-protocols&#34;&gt;Mail access protocols&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SMTP: delivery/storage to receiver&amp;rsquo;s server&lt;/li&gt;
&lt;li&gt;Mail access protocol: retrieval from server
&lt;ul&gt;
&lt;li&gt;POP3: Post Office Protocol, version 3 (authorization (agent -&amp;gt; server) and download)&lt;/li&gt;
&lt;li&gt;IMAP: Internet Mail Access Protocol (RFC 2060)
&lt;ul&gt;
&lt;li&gt;more features (more complex)&lt;/li&gt;
&lt;li&gt;manipulation of stores messages on server&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;HTTP: Hotmail, Gmail etc
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c9764e842cbf.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;pop3-protocol&#34;&gt;POP3 protocol&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;client opens a TCP connection to the mail server on port 110&lt;/li&gt;
&lt;li&gt;authorization phase (client: declare username, password, server response: +OK)&lt;/li&gt;
&lt;li&gt;transaction phase: client
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;list&lt;/code&gt;: list message number&lt;/li&gt;
&lt;li&gt;&lt;code&gt;retr&lt;/code&gt;: retrieve message by number&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dele&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;quit&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;update phase: mail server deletes the message marked for deletion&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;download and delete&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;download and keep&lt;/strong&gt;: copies of messages on different clients&lt;/li&gt;
&lt;li&gt;POP3 is stateless across sessions&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;imap&#34;&gt;IMAP&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;keep all messages in one place: the server&lt;/li&gt;
&lt;li&gt;Allows user to organize messages in folders&lt;/li&gt;
&lt;li&gt;IMAP keeps user state across sessions (names of folders and mappings between message IDs and folder name)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dns-domain-name-system&#34;&gt;DNS (domain name system)&lt;/h2&gt;
&lt;p&gt;MAP between IP addresses and name&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A distributed database implemented in hierarchy of many name servers&lt;/li&gt;
&lt;li&gt;no server has all name-to-IP address mappings&lt;/li&gt;
&lt;li&gt;Ap application-layer protocol that allows host, routers, name servers to communicate to resolve names(address/name translation)
&lt;ul&gt;
&lt;li&gt;DNS provides a core Internet function, implemented as application-layer protocol&lt;/li&gt;
&lt;li&gt;DNS is an example of the Internet design philosophy of placing complexity at network&amp;rsquo;s edge&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Services
&lt;ul&gt;
&lt;li&gt;Mapping&lt;/li&gt;
&lt;li&gt;Host aliasing (Canonical and alias name)&lt;/li&gt;
&lt;li&gt;Mail server aliasing&lt;/li&gt;
&lt;li&gt;Load distribution, Replicated Web servers: set of IP addresses for one canonical name (负载平衡)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Structures
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c9769b66f470.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Client wants IP for &lt;em&gt;&lt;a href=&#34;http://www.amazon.com&#34;&gt;www.amazon.com&lt;/a&gt;&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;client queries a root server to find com DNS server&lt;/li&gt;
&lt;li&gt;client queries com DNS server to get amazon.com DNS server&lt;/li&gt;
&lt;li&gt;Client queries amazon.com DNS server to get IP address for &lt;a href=&#34;http://www.amazon.com&#34;&gt;www.amazon.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Four types of name services
&lt;ol&gt;
&lt;li&gt;root name servers&lt;/li&gt;
&lt;li&gt;top level name servers&lt;/li&gt;
&lt;li&gt;authoritative name servers
&lt;ul&gt;
&lt;li&gt;for a host: stores that host&amp;rsquo;s IP address, name&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;local name servers, each ISP, company has local(default) name server, host DNS query first goes to local name server&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Top-level domain(TLD) servers: responsible for com, org, net, edu, etc, and all top-level country domains uk, fr, ca, jp
&lt;ul&gt;
&lt;li&gt;The company &lt;strong&gt;Network solutions&lt;/strong&gt; maintains servers for com TLD&lt;/li&gt;
&lt;li&gt;The company &lt;strong&gt;Educause&lt;/strong&gt; maintains servers for edu TLD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Authoritative DNS servers: organization&amp;rsquo;s DNS servers, providing authoritative hostname to IP mappings for organization&amp;rsquo;s servers, can be maintained by organization or service provider&lt;/li&gt;
&lt;li&gt;local name server: does not strictly belong to hierarchy, each ISP has one&lt;/li&gt;
&lt;li&gt;Example (iterative query)
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c976d5a0de29.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;recursive query (puts burden of name resolution on contacted name server)
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c976e1ace932.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;caching and updating
&lt;ul&gt;
&lt;li&gt;once name server learns mapping, it caches mapping, caches entries timeout(disappear) after some time&lt;/li&gt;
&lt;li&gt;the contents of each DNS servers were configured &lt;strong&gt;statically&lt;/strong&gt; from a configuration file created by a system manager&lt;/li&gt;
&lt;li&gt;An update option has been added to the DNS protocol to allow data to be added or deleted from the database vid DNS messages&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DNS records
distributed database storing resource records (RR), RR format: (name, value, type, ttl)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dns-protocol-messages&#34;&gt;DNS protocol, messages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DNS protocol query and reply messages, both with same message format
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c97709813988.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;peer-to-peer-applications&#34;&gt;Peer-to-Peer Applications&lt;/h2&gt;
&lt;p&gt;File distribution problem (client-server model)
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c977afd15e0a.png&#34; width=&#34;400px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c977b81b3242.png&#34; width=&#34;400px&#34;/&gt;&lt;/p&gt;
&lt;h3 id=&#34;bit-torrent&#34;&gt;Bit Torrent&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c977bf370a09.png&#34; width=&#34;400px&#34;/&gt;
&lt;h3 id=&#34;structure&#34;&gt;Structure&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;P2P centralized directory
&lt;ul&gt;
&lt;li&gt;when peer connects, it informs central server (IP address, content)&lt;/li&gt;
&lt;li&gt;Single point of failure&lt;/li&gt;
&lt;li&gt;performance bottleneck&lt;/li&gt;
&lt;li&gt;copyright infringement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;socket-programming&#34;&gt;Socket Programming&lt;/h2&gt;
&lt;p&gt;build client/server application communication using sockets&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;client/server paradigm&lt;/li&gt;
&lt;li&gt;types of transport service via socket API (unreliable datagram, reliable, byte stream-oriented)&lt;/li&gt;
&lt;li&gt;socket: a host-local, application-created, OS-controlled interface(&amp;ldquo;door&amp;rdquo;) into which application can both send and receive message to/from another application process&lt;/li&gt;
&lt;li&gt;Socket Programming using Java, cross platform without recompiling, easy programming with high-level API&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;socket-programming-using-tcp&#34;&gt;Socket-programming using TCP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Socket: a door between application process and end-to-end transport protocol (UCP or TCP)&lt;/li&gt;
&lt;li&gt;TCP service: reliable transfer of bytes from one process to another&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Client must contact server
&lt;ul&gt;
&lt;li&gt;server process must first be running&lt;/li&gt;
&lt;li&gt;server must have created socket (door) that welcomes client&amp;rsquo;s contact&lt;/li&gt;
&lt;li&gt;client contacts server by
&lt;ul&gt;
&lt;li&gt;creating client-local TCP socket&lt;/li&gt;
&lt;li&gt;specifying IP address, port number of server process&lt;/li&gt;
&lt;li&gt;when client creates socket: client TCP established connection to server TCP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;when contacted by client, server TCP &lt;strong&gt;creates new socket&lt;/strong&gt; for server process to communicate with client
&lt;ul&gt;
&lt;li&gt;allows server to talk with multiple clients&lt;/li&gt;
&lt;li&gt;source port numbers used to distinguish clients&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;info note&#34;&gt;&lt;p&gt;
Stream
&lt;ul&gt;
&lt;li&gt;A stream is a sequence of characters that flow into or out of a process&lt;/li&gt;
&lt;li&gt;An input stream is attached to some input source for the process (e.g. keyboard or socket)&lt;/li&gt;
&lt;li&gt;An output stream is attached  to an output source (e.g. monitor or socket)&lt;/li&gt;
&lt;/ul&gt;
 &lt;p&gt;&lt;/div&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/26/5c999a9e7389e.png&#34; width=&#34;500px&#34;/&gt;
&lt;h3 id=&#34;tcp-client&#34;&gt;TCP client&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A &amp;ldquo;Socket&amp;rdquo; object, making connection requests&lt;/li&gt;
&lt;li&gt;Parameters
&lt;ul&gt;
&lt;li&gt;Remote ip address&lt;/li&gt;
&lt;li&gt;Remote tcp port&lt;/li&gt;
&lt;li&gt;Local ip address (optional)&lt;/li&gt;
&lt;li&gt;Local port (optional)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
import java.net.*;
import java.io.*;

public class Client{
    private Socket socket = null;
    private DataOutputStream out = null;
    private BufferedReader input;

    public Client(String address, int port)
    {
        try {
            socket = new Socket(address, port);
            System.out.println(&amp;quot;Connected&amp;quot;);
            input = new BufferedReader(new InputStreamReader(System.in));
            out = new DataOutputStream(socket.getOutputStream());
        }
        catch (UnknownHostException u){
            System.out.println(u);}
        catch (IOException i){
            System.out.println(i);}

        String line = &amp;quot;&amp;quot;;
        while(!line.equals(&amp;quot;Over&amp;quot;))
        {
            try{
                line = input.readLine();
                out.writeUTF(line);
            }
            catch(IOException i)
            {
                System.out.println(i);
            }
        }
        try{
            input.close();
            out.close();
            socket.close();
        }
        catch(IOException i)
        {
            System.out.println(i);
        }
    }
    public static void main(String[] args) {
        Client client = new Client(&amp;quot;127.0.0.1&amp;quot;, 5000);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import java.net.*;
import java.io.*;

public class Server{
    private Socket socket = null;
    private ServerSocket server = null;
    private DataInputStream in = null;

    public  Server (int port){
        try{
            server = new ServerSocket(port);
            System.out.println(&amp;quot;Server started&amp;quot;);
            socket = server.accept();
            System.out.println(&amp;quot;client accepted&amp;quot;);

            in = new DataInputStream(socket.getInputStream());

            String line =  &amp;quot;&amp;quot;;
            while (!line.equals(&amp;quot;Over&amp;quot;))
            {
                try{
                    line = in.readUTF();
                    System.out.println(line);
                }
                catch (IOException i)
                {
                    System.out.println(i);
                }
            }
            System.out.println(&amp;quot;Closing connected&amp;quot;);
            socket.close();
            in.close();
        }
        catch (IOException i){
            System.out.println(i);
        }
    }
    public static void main(String[] args){
        Server server = new Server(5000);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;java-socket-programming-with-thread&#34;&gt;Java socket Programming With thread&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import java.io.*;
import java.text.*;
import java.util.*;
import java.net.*;

public class Server {
    public static void main(String[] args) throws IOException
    {
        ServerSocket ss = new ServerSocket(5056);
        while (true)
        {
            Socket s = null;
            try{
                s = ss.accept();
                System.out.println(&amp;quot;A new client is connected&amp;quot;);
                DataInputStream dis = new DataInputStream(s.getInputStream());
                DataOutputStream dos = new DataOutputStream(s.getOutputStream());
                System.out.println(&amp;quot;Assign new thread for this client&amp;quot;);
                Thread t = new ClientHandler(s, dis, dos);
                t.start();
            }
            catch (Exception e){
                s.close();
                e.printStackTrace();
            }
        }
    }
}

class ClientHandler extends Thread{
    private DateFormat fordate = new SimpleDateFormat(&amp;quot;yyyy/MM/dd&amp;quot;);
    private DateFormat fortime = new SimpleDateFormat(&amp;quot;hh:mm:ss&amp;quot;);
    final private DataInputStream dis;
    final private DataOutputStream dos;
    final private Socket s;

    ClientHandler(Socket s, DataInputStream dis, DataOutputStream dos){
        this.dis = dis;
        this.dos = dos;
        this.s = s;
    }

    @Override
    public void run(){
        String received;
        String toreturn;
        while(true){
            try{
                dos.writeUTF(&amp;quot;what do you want? [Date|TIme]..\n&amp;quot; + &amp;quot;Type exit to terminate connection&amp;quot;);
                received = dis.readUTF();
                if(received.equals(&amp;quot;Exit&amp;quot;))
                {
                    System.out.println(&amp;quot;Client&amp;quot; + this.s + &amp;quot;Sends exist&amp;quot;);
                    System.out.println(&amp;quot;Closing this connection&amp;quot;);
                    this.s.close();
                    System.out.println(&amp;quot;Connection closed&amp;quot;);
                    break;
                }
                Date date = new Date();
                switch (received) {
                    case &amp;quot;Date&amp;quot;:
                        toreturn = fordate.format(date);
                        dos.writeUTF(toreturn);
                        break;
                    case &amp;quot;Time&amp;quot;:
                        toreturn = fortime.format(date);
                        dos.writeUTF(toreturn);
                        break;
                    default:
                        dos.writeUTF(&amp;quot;Inavalid input&amp;quot;);
                        break;
                }
            }
            catch (IOException e)
            {
                e.printStackTrace();
            }
        }
        try {
            this.dis.close();
            this.dos.close();
        } catch(IOException e)
        {
            e.printStackTrace();
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import java.io.*;
import java.net.*;
import java.util.Scanner;

public class Client {
    public static void main(String[] args)
    {
        try {
            Scanner scn = new Scanner(System.in);
            InetAddress ip = InetAddress.getByName(&amp;quot;localhost&amp;quot;);
            Socket s = new Socket(ip, 5056);
            DataInputStream dis = new DataInputStream(s.getInputStream());
            DataOutputStream dos = new DataOutputStream(s.getOutputStream());

            while (true) {
                System.out.println(dis.readUTF());
                String tosend = scn.nextLine();
                dos.writeUTF(tosend);

                if (tosend.equals(&amp;quot;Exit&amp;quot;)) {
                    System.out.println(&amp;quot;Closing the connection&amp;quot;);
                    s.close();
                    System.out.println(&amp;quot;Connection closed&amp;quot;);
                    break;
                }
                String received = dis.readUTF();
                System.out.println(received);
            }
            scn.close();
            dis.close();
            dos.close();
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;socket-programming-with-udp&#34;&gt;Socket programming with UDP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;no handshaking&lt;/li&gt;
&lt;li&gt;sender explicitly attaches IP address and port of destination to each packet&lt;/li&gt;
&lt;li&gt;server must extract IP address, port of sender from received packet&lt;/li&gt;
&lt;li&gt;transmitted data may be received out of water or lost
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9dc3a0bc9fa.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Exception Handling during initializing sockets, establishing connection, data transmission&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>OS structures</title>
      <link>/courses/operating_system/os_structures/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/os_structures/</guid>
      <description>&lt;h2 id=&#34;os-services&#34;&gt;OS services&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;OS services: Communication,  Error detection,  User Interface,  Resource allocation,  Program Execution&lt;/li&gt;
&lt;li&gt;User Interface
&lt;ul&gt;
&lt;li&gt;CLI (Command Line Interface)
Shell: Command-line interpreter (CSHELL, BASH)&lt;/li&gt;
&lt;li&gt;GUI (Graphic User Interface)
Icons,  Mouse&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Communication Models
&lt;ul&gt;
&lt;li&gt;Message passing
例如， process A 与 process B 交换信息： 把 processA 的 信息 copy 到 os 的 memory,  再从 os 把 process A 的信息 copy 到 process B。(Protection 机制, 需要使用 system call)， 缺点是较慢&lt;/li&gt;
&lt;li&gt;Shared memory
有一块公用的 memory, 两个 process 可以直接使用，需要先通过 system call 分配好。 (Multi-thread programming)， 缺点是有可能有 dead-lock problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-calls--api&#34;&gt;System Calls &amp;amp; API&lt;/h2&gt;
&lt;p&gt;Request OS services:  Process control, File management, Device management, Information maintenance, Communication&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;System calls (简单， bug-free)
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OS interface&lt;/strong&gt; to running program&lt;/li&gt;
&lt;li&gt;An explicit request to the kernel made via a &lt;strong&gt;software interrupt&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Available as &lt;strong&gt;assembly-language&lt;/strong&gt; instructions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;API: Application Program Interface (方便使用者, simplicity, portability, efficiency)
&lt;ul&gt;
&lt;li&gt;Users mostly program against API instead of system call&lt;/li&gt;
&lt;li&gt;Commonly implemented by language libraries, e.g. C library&lt;/li&gt;
&lt;li&gt;An api call could involve zero or multiple system call
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;malloc()&lt;/code&gt; and &lt;code&gt;free()&lt;/code&gt; use system call &lt;code&gt;brk()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;abs()&lt;/code&gt; don&amp;rsquo;t need to involve system call&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;不会直接产生 Interrupt, 由 system call 产生 interrupt&lt;/li&gt;
&lt;li&gt;Three most common APIs:
&lt;ul&gt;
&lt;li&gt;Win32API for windows&lt;/li&gt;
&lt;li&gt;POSIX API for POSIX-based systems (Portable Operating System Interface for Unix)  所有的 API 都一样， 但是实作(Library)不一样&lt;/li&gt;
&lt;li&gt;Java API for the java virtual machine&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;System calls: passing parameters
&lt;ul&gt;
&lt;li&gt;Pass parameters in registers&lt;/li&gt;
&lt;li&gt;Store the parameters in a table in memory, and the table address is passed as a parameter in a register&lt;/li&gt;
&lt;li&gt;Push the parameters onto the stack by the program, and pop off the stack by operating system&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-structure&#34;&gt;System Structure&lt;/h2&gt;
&lt;p&gt;User goals and system goals is different
User goals: easy to use and learn, as well as reliable, safe and fast
System goals: easy to &lt;em&gt;design&lt;/em&gt;, &lt;em&gt;implement&lt;/em&gt; and &lt;em&gt;maintain&lt;/em&gt; as well as reliable, error-free, and efficient&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple OS Architecture
&lt;ul&gt;
&lt;li&gt;unsafe, difficult to enhance
&lt;img style=&#34;margin: auto;&#34; src=https://i.loli.net/2019/03/15/5c8b08039972e.png width=&#34;40%&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Layered OS Architecture
&lt;ul&gt;
&lt;li&gt;lower levels independent of upper levels&lt;/li&gt;
&lt;li&gt;Easy to debugging and maintenance&lt;/li&gt;
&lt;li&gt;less efficient, difficult to define layers
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b0e784d3f5.png&#34; width=&#34;40%&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Microkernel OS (有新的系统朝着这个方向做）
&lt;ul&gt;
&lt;li&gt;Moves as much from the kernel info &amp;ldquo;user&amp;rdquo; space&lt;/li&gt;
&lt;li&gt;Communications is provided by message passing, 避免 synchronization&lt;/li&gt;
&lt;li&gt;Easier for extending and porting
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b0a3e2836d.png&#34; width=&#34;40%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Modular OS Architecture
&lt;ul&gt;
&lt;li&gt;Modern OS implement kernel modules&lt;/li&gt;
&lt;li&gt;Object-oriented Approach&lt;/li&gt;
&lt;li&gt;Each core component is separate&lt;/li&gt;
&lt;li&gt;Each to talk each other over known interfaces&lt;/li&gt;
&lt;li&gt;Each is loadable as needed within the kernel
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b0b7fbd37c.png&#34; width=&#34;40%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Virtual Machine
Hard: Critical Instruction，指的是一些指令在 User space 和 Kernel space 执行的结果不一样， 因此需要知道一些指令是否是 critical instruction.
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b1c7b544c4.png&#34; width=&#34;40%&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;虚拟化指的是如何加 virtual-machine implementation layer。&lt;/li&gt;
&lt;li&gt;工作流程: 虚拟机的Kernel 在运行的时候是假设是在 kernel space, 因此才可以执行privileged instruction. 但从架构来说，虚拟机的 kernel 应该是存在于 user space 中。 Privileged instruction 在 虚拟机的 Kernel space 执行时，系统会产生 一个 signal (exception)， interrupt 会回到底层的OS， 因此系统会得知虚拟机想要执行privileged instruction， 底层的OS 会重复执行这个命令。&lt;/li&gt;
&lt;li&gt;缺点:  虚拟机执行效率比较低。&lt;/li&gt;
&lt;li&gt;现在很多 CPU 都支持虚拟机， 除了 User mode, kernel mode, 还有 VM mode。&lt;/li&gt;
&lt;li&gt;Usage: Protection, compatibility problems (系统兼容)， research and development， honeypot (A virtual honeypot is software that emulates a vulnerable system or network to attract intruders and study their behavior), cloud computing (不需要直接用虚拟机提供， container)&lt;/li&gt;
&lt;li&gt;Full Virtualization (VMware, 需要有 hardware support, 执行效率才会快)
&lt;ul&gt;
&lt;li&gt;Run in user mode as an application on top of OS&lt;/li&gt;
&lt;li&gt;Virtual machine believe they are running on bare hardware but in fact are running inside a user-level application&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Para-virtualization: Xen (存在一个 global zone)
&lt;ul&gt;
&lt;li&gt;Presents guest with system similar but not identical to the guest&amp;rsquo;s preferred systems (Guest must be modified)&lt;/li&gt;
&lt;li&gt;Hardware rather than OS and its devices are virtualized&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Java Virtual Machine
&lt;ul&gt;
&lt;li&gt;Code translation, compile 执行完成后, 会有自己的 binary code. 再进行一次translation, 在host os执行。&lt;/li&gt;
&lt;li&gt;Just-In-Time(JIT) compliers, 会记录执行的命令，然后 reuse.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Process</title>
      <link>/courses/operating_system/process/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/process/</guid>
      <description>&lt;h2 id=&#34;concept&#34;&gt;Concept&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Program: passive entity: binary stored in disk&lt;/li&gt;
&lt;li&gt;Process: active entity: a program in execution in memory&lt;/li&gt;
&lt;li&gt;A process includes:
&lt;ul&gt;
&lt;li&gt;Code segment&lt;/li&gt;
&lt;li&gt;Data section &amp;ndash;global variables&lt;/li&gt;
&lt;li&gt;Stack &amp;ndash; temporary local variables and functions&lt;/li&gt;
&lt;li&gt;Heap &amp;ndash; dynamic allocated variables or classes&lt;/li&gt;
&lt;li&gt;Current activity (program counter, register contents) 用来管理 process&lt;/li&gt;
&lt;li&gt;A set of associated resources (e.g. open file handlers)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Process in memory
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b3959e8cd0.png&#34; width=&#34;40%&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Threads
&lt;ul&gt;
&lt;li&gt;lightweight process&lt;/li&gt;
&lt;li&gt;All threads belonging to the same processes share &lt;strong&gt;code section, data section and OS resources (e.g. open files and signals)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Each thread has it own &lt;strong&gt;thread ID, program counter, register set and stack&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Process States
&lt;ul&gt;
&lt;li&gt;New: process is being created&lt;/li&gt;
&lt;li&gt;Ready: process is in the memory waiting to be assigned to a processor, 放到 waiting &lt;strong&gt;queue&lt;/strong&gt; 中&lt;/li&gt;
&lt;li&gt;Running: instructions are being executed by CPU&lt;/li&gt;
&lt;li&gt;Waiting: the process is waiting for events to occur (e.g. IO)&lt;/li&gt;
&lt;li&gt;Terminated: the process has finished execution
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b3b57c8609.png&#34; width=&#34;40%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;process-switch&#34;&gt;Process switch&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Process control Block
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b3e1bb0ea9.png&#34; width=&#34;20%&#34;/&gt;&lt;/li&gt;
&lt;li&gt;context switch
Context-switch time is purely overhead
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b3f9767fd1.png&#34; width=&#34;50%&#34;/&gt;
&lt;div class=&#34;note warning&#34;&gt;&lt;p&gt;Switch time depends on memory speed, number of registers, existence of special instructions (a single instruction to save/load all registers), hardware support (multiple sets of registers) &lt;/p&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;process-scheduling&#34;&gt;Process Scheduling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Concept
&lt;ul&gt;
&lt;li&gt;Multiprogramming: CPU runs process at all times to maximize CPU utilization&lt;/li&gt;
&lt;li&gt;Time sharing: switch CPU frequently such that users can interact with each program while it is running&lt;/li&gt;
&lt;li&gt;Processes will have to wait until the CPU is free and  can be re-scheduled&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scheduling Queues
&lt;ul&gt;
&lt;li&gt;Job queue (New State) &amp;ndash; set of all processes in the system&lt;/li&gt;
&lt;li&gt;ready queue (Ready state) &amp;ndash; set of all processes residing in main memory, ready and waiting to execute&lt;/li&gt;
&lt;li&gt;device queue (Wait state) &amp;ndash; set of processes waiting for an I/O device&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Diagram
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b452e66392.png&#34; width=&#34;40%&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Schedulers
&lt;ul&gt;
&lt;li&gt;Short- term scheduler (CPU scheduler) &amp;ndash; selects which process should be executed and allocated CPU (Ready state $\longrightarrow$ Run state) 很频繁&lt;/li&gt;
&lt;li&gt;Long-term scheduler (job scheduler) &amp;ndash; selects which processes should be loaded into memory and brought into ready queue (New state $\longrightarrow$ Ready state)&lt;/li&gt;
&lt;li&gt;Middle-term scheduler &amp;ndash; selects which processes should be swapped in/out memory (Ready state $\longrightarrow$ Wait state) 与 virtual memory 相结合
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b46dfcfc3b.png&#34; width=&#34;50%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Long-Term Scheduler (现在memory 很大， 现在变为 middle-term scheduler)
&lt;ul&gt;
&lt;li&gt;control degree of multiprogramming (Degree 很少时，cpu 会 idle， Degree 很多时， 会竞争CPU 资源)&lt;/li&gt;
&lt;li&gt;Execute less frequently (e.g. invoked only when a process leaves the system or once several minutes)&lt;/li&gt;
&lt;li&gt;Select a &lt;strong&gt;good mix of CPU-bound &amp;amp; I/O- bound&lt;/strong&gt; processes to increase system overall performance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Short-Term Scheduler
&lt;ul&gt;
&lt;li&gt;Execute quite frequently (e.g. once per 100ms)&lt;/li&gt;
&lt;li&gt;Must be efficient (averaging wait time)
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b49ed102a6.png&#34; width=&#34;50%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Medium-Term Scheduler
&lt;ul&gt;
&lt;li&gt;Swap out: removing processes from memory to reduce the degree of multiprogramming&lt;/li&gt;
&lt;li&gt;Swap in: reintroducing swap-out processes into memory&lt;/li&gt;
&lt;li&gt;Purpose: improve process mix, free up memory&lt;/li&gt;
&lt;li&gt;Most modern OS doesn&amp;rsquo;t have medium-term scheduler because having sufficient physical memory or using virtual memory&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;operations-on-processes&#34;&gt;Operations on Processes&lt;/h2&gt;
&lt;h3 id=&#34;tree-of-process&#34;&gt;Tree of process&lt;/h3&gt;
&lt;p&gt;Each process is identified by a &lt;strong&gt;unique&lt;/strong&gt; processor identifier (&lt;strong&gt;pid&lt;/strong&gt;)&lt;/p&gt;
&lt;div class=&#34;note info&#34;&gt;&lt;p&gt; `ps-ael` will list complete info of all active processes  in unix &lt;/p&gt;&lt;/div&gt;
&lt;h3 id=&#34;process-creation&#34;&gt;Process Creation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Resource sharing
&lt;ul&gt;
&lt;li&gt;Parent and child processes share &lt;strong&gt;all&lt;/strong&gt; resources&lt;/li&gt;
&lt;li&gt;Child process shares &lt;strong&gt;subset&lt;/strong&gt; of parent&amp;rsquo;s resources&lt;/li&gt;
&lt;li&gt;parent and child share &lt;strong&gt;no&lt;/strong&gt; resources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two possibilities of execution
&lt;ul&gt;
&lt;li&gt;Parent and children &lt;strong&gt;execute concurrently&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;parent &lt;strong&gt;waits until children terminate&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two possibilities of address space
&lt;ul&gt;
&lt;li&gt;Child duplicate of parent (sharing variables)&lt;/li&gt;
&lt;li&gt;Child has a program loaded into it (message passing)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;unixlinux-process-creation&#34;&gt;UNIX/LINUX Process Creation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fork&lt;/code&gt; system call
&lt;ul&gt;
&lt;li&gt;Create a new child process&lt;/li&gt;
&lt;li&gt;The new process duplicates the address space of its parent&lt;/li&gt;
&lt;li&gt;Child &amp;amp; parent execute &lt;strong&gt;concurrently&lt;/strong&gt; after fork&lt;/li&gt;
&lt;li&gt;Child: return value of fork is 0&lt;/li&gt;
&lt;li&gt;Parent: return value of fork is PID of the child process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;execlp&lt;/code&gt; system call
&lt;strong&gt;Load a new binary file&lt;/strong&gt; into memory, &lt;strong&gt;destroying the old code&lt;/strong&gt; (memory content reset)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wait&lt;/code&gt; system call
The parent waits for one of its child processes to complete&lt;/li&gt;
&lt;li&gt;Memory space of &lt;code&gt;fork()&lt;/code&gt;,  A 调用 &lt;code&gt;fork()&lt;/code&gt; 时
&lt;ul&gt;
&lt;li&gt;old implementation: A&amp;rsquo;s child is an extra copy of parent&lt;/li&gt;
&lt;li&gt;current implementation: use copy-on-write technique to store differences in A&amp;rsquo;s child address space
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b4f7e2ea39.png&#34; width=&#34;40%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;process-termination&#34;&gt;Process Termination&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Terminate when the last statement is executed or &lt;code&gt;exit()&lt;/code&gt; is called&lt;/li&gt;
&lt;li&gt;Parent may terminate execution of children processes by specifying its PID (abort)&lt;/li&gt;
&lt;li&gt;Cascading termination
killing (exiting) parent $\longrightarrow​$ killing all its children&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;note info&#34; &lt;p&gt; Control-C , OS在启动时，会产生 console process, 在执行程序时，是 console 再去产生其他程序，按 Control-C 时是用 console 来终止程序&lt;/p&gt;&lt;/div&gt;
&lt;div class = &#34;note info&#34; &lt;p&gt; kill, 通过 OS 来终止程序, 需要权限&lt;/p&gt;&lt;/div&gt;
&lt;h2 id=&#34;interprocess-communication-ipc&#34;&gt;Interprocess Communication (IPC)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;IPC: a set of methods for the exchange of data among multiple threads in one or more processes&lt;/li&gt;
&lt;li&gt;Independent process: cannot affect or be affected by other process&lt;/li&gt;
&lt;li&gt;Cooperating process:  otherwise&lt;/li&gt;
&lt;li&gt;Purposes: information sharing, computation speedup, convenience, modularity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;communication-methods&#34;&gt;Communication Methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;shared memory:
&lt;ul&gt;
&lt;li&gt;Require more careful user synchronization&lt;/li&gt;
&lt;li&gt;implemented by memory access: faster speed&lt;/li&gt;
&lt;li&gt;use memory address to access data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Message passing:
&lt;ul&gt;
&lt;li&gt;No conflict : more efficient&lt;/li&gt;
&lt;li&gt;Use send/recv  message&lt;/li&gt;
&lt;li&gt;Implemented by system call : slower speed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sockets:
&lt;ul&gt;
&lt;li&gt;A network connection identified by IP &amp;amp; port (port number is process)&lt;/li&gt;
&lt;li&gt;Exchange unstructured stream of bytes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Remote Procedure Calls:
&lt;ul&gt;
&lt;li&gt;Cause a procedure to execute in another space&lt;/li&gt;
&lt;li&gt;Parameters and return values are passed by message
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b6d362e44b.png&#34; width=&#34;40%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;shared-memory&#34;&gt;Shared Memory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Establishing a region of shared memory
&lt;ul&gt;
&lt;li&gt;Typically, a shared-memory region resides in the address space of the process creating the shared-memory segment&lt;/li&gt;
&lt;li&gt;Participating processes must agree to &lt;strong&gt;remove memory access constraint&lt;/strong&gt; from OS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Determining the form of the data and the location&lt;/li&gt;
&lt;li&gt;Ensuring data are not written simultaneously by processes&lt;/li&gt;
&lt;li&gt;Consumer and Producer problem (系统里面都有很多这样的问题, Compile(Producer), Link(Consumer))
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Producer&lt;/strong&gt; process produces information that is consumed by a &lt;strong&gt;Consumer&lt;/strong&gt; process&lt;/li&gt;
&lt;li&gt;Buffer as a circular array with size B
&lt;ul&gt;
&lt;li&gt;next free: in&lt;/li&gt;
&lt;li&gt;first available : out&lt;/li&gt;
&lt;li&gt;empty: in = out&lt;/li&gt;
&lt;li&gt;full (in + 1) % B = out&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The solution allows at most (B-1) item in the buffer, otherwise, cannot tell the buffer is fall or empty
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b71433d5d2.png&#34; width=&#34;30%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;while(1){
    while (((in+1) % BUFFER_SIZE) == out); // wait if buffer is full
    buffer[in] = nextProduced;
    in = (in + 1) % BUFFER_SIZE;
}

while(1){
        while(in == out); // wait if buffre is empty
    nextConsumed = buffer[out];
    out = (out+1) % BUFFER_SIZE
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;message-passing-system&#34;&gt;Message-Passing System&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mechanism for processes to communicate and synchronize their actions&lt;/li&gt;
&lt;li&gt;IPC facility provides two operations:
&lt;ul&gt;
&lt;li&gt;Send (Message) &amp;ndash; message size fixed or variable&lt;/li&gt;
&lt;li&gt;Receive(Message)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Message system &amp;ndash; process communicate without resorting to shared variables&lt;/li&gt;
&lt;li&gt;To communicate, processes need to
&lt;ul&gt;
&lt;li&gt;Establish a communication link&lt;/li&gt;
&lt;li&gt;Physical (HW bus, network)
&lt;ul&gt;
&lt;li&gt;Logical (logical properties)  1. Direct or indirect communication 2. Blocking and non-blocking&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Exchange a message via send/receive&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Direct Communication
&lt;ul&gt;
&lt;li&gt;Process must name each other explicitly&lt;/li&gt;
&lt;li&gt;Links are &lt;strong&gt;established automatically&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-to-One&lt;/strong&gt; relationship between links and processes&lt;/li&gt;
&lt;li&gt;The link may be unidirectional, but is usually bi-directional&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;note warning&#34;&gt;&lt;p&gt; Limited modularity, if the name of a process is changed, all old names should be found&lt;/p&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Indirect communication (E-mail)
&lt;ul&gt;
&lt;li&gt;Messages are directed and received from mailboxes&lt;/li&gt;
&lt;li&gt;Each mailbox has a unique ID&lt;/li&gt;
&lt;li&gt;Processes can communicate if the share a mailbox&lt;/li&gt;
&lt;li&gt;Send(A, message) , Receive(A, message), communicate through mailbox A&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Many-to-Many&lt;/strong&gt; relationship between links and processes&lt;/li&gt;
&lt;li&gt;Link established only if processes share a common mailbox&lt;/li&gt;
&lt;li&gt;Mailbox can be owned either by OS or processes&lt;/li&gt;
&lt;li&gt;MailBox 怎么解决一对一的问题？
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b88e397ddd.png&#34; width=&#34;50%&#34;/&gt;
Solutions:
&lt;ul&gt;
&lt;li&gt;Allow a link to be associated with at most two processes&lt;/li&gt;
&lt;li&gt;Allow only one process at a time to execute a receive operation&lt;/li&gt;
&lt;li&gt;Allow the system to select arbitrarily a singe receiver. Sender is notified who the receiver was&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Synchronization
&lt;ul&gt;
&lt;li&gt;Message passing may be either blocking (synchronous) or non-blocking (asynchronous)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blocking send:&lt;/strong&gt; send is blocked until the message is received by receiver or by the mailbox&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nonblocking send:&lt;/strong&gt; sender sends the message and resumes operation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blocking receive:&lt;/strong&gt; receiver is blocked until the message is available&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nonblocking receive:&lt;/strong&gt; receiver receives a valid message or a null (存在一个 token, 来判断是否收到信息)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Buffer implementation (中间存在一个 buffer 来进行消息的储存)
&lt;ul&gt;
&lt;li&gt;Zero capacity: blocking send/receive&lt;/li&gt;
&lt;li&gt;Bounded capacity: if full, sender will be blocked&lt;/li&gt;
&lt;li&gt;Unbounded capacity: sender never blocks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sockets
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b906940d23.png&#34; width=&#34;45%&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Remote Procedure Calls: RPC
Stubs, client-side proxy for the actual procedure on the server
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b90e017de2.png&#34; width=&#34;40%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Examples of CNN</title>
      <link>/courses/deep_learning/cnn_example/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0100</pubDate>
      <guid>/courses/deep_learning/cnn_example/</guid>
      <description>&lt;p&gt;主要包括下面几个&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LeNet-5&lt;/li&gt;
&lt;li&gt;AlexNet&lt;/li&gt;
&lt;li&gt;VGG&lt;/li&gt;
&lt;li&gt;ResNet&lt;/li&gt;
&lt;li&gt;Inception Neural Network&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;classical-cnn&#34;&gt;Classical CNN&lt;/h2&gt;
&lt;h3 id=&#34;lenet-5&#34;&gt;LeNet-5&lt;/h3&gt;
  &lt;img src=&#34;/img/cnn/LeNet-5.png&#34; width=&#34;700px&#34;/&gt;
&lt;p&gt;特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LeNet-5 针对灰度图像而训练，因此输入图片的通道数为 1。&lt;/li&gt;
&lt;li&gt;该模型总共包含了约 6 万个参数，远少于标准神经网络所需。&lt;/li&gt;
&lt;li&gt;典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 CONV layer-&amp;gt;POOL layer-&amp;gt;CONV layer-&amp;gt;POOL layer-&amp;gt;FC layer-&amp;gt;FC layer-&amp;gt;OUTPUT layer。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用。&lt;/li&gt;
&lt;li&gt;当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相关论文：
&lt;a href=&#34;http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726791&amp;amp;tag=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LeCun et.al., 1998. Gradient-based learning applied to document recognition&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;alexnet&#34;&gt;AlexNet&lt;/h3&gt;
  &lt;img src=&#34;/img/cnn/AlexNet.png&#34; width=&#34;700px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;AlexNet 模型与 LeNet-5 模型类似，但是更复杂，包含约 6000 万个参数。另外，AlexNet 模型使用了 ReLU 函数。&lt;/li&gt;
&lt;li&gt;当用于训练图像和数据集时，AlexNet 能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相关论文：
&lt;a href=&#34;http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Krizhevsky et al.,2012. ImageNet classification with deep convolutional neural networks&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;vgg&#34;&gt;VGG&lt;/h3&gt;
  &lt;img src=&#34;/img/cnn/VGG.png&#34; width=&#34;700px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;VGG 又称 VGG-16 网络，“16”指网络中包含 16 个卷积层和全连接层。&lt;/li&gt;
&lt;li&gt;超参数较少，只需要专注于构建卷积层。&lt;/li&gt;
&lt;li&gt;结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。&lt;/li&gt;
&lt;li&gt;VGG 需要训练的特征数量巨大，包含多达约 1.38 亿个参数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相关论文：
&lt;a href=&#34;https://arxiv.org/pdf/1409.1556.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simonvan &amp;amp; Zisserman 2015. Very deep convolutional networks for large-scale image recognition&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;resnet&#34;&gt;ResNet&lt;/h2&gt;
&lt;p&gt;网络越深，容易存在梯度消失问题，使得网络不好训练&lt;/p&gt;
  &lt;img src=&#34;/img/cnn/Residual-block.jpg&#34; width=&#34;800px&#34;/&gt;
&lt;p&gt;上图的结构被称为&lt;strong&gt;残差块（Residual block）&lt;/strong&gt;。通过**捷径（Short cut，或者称跳远连接，Skip connections）**可以将 $a^{[l]}$添加到第二个 ReLU 过程中，直接建立 $a^{[l]}$与 $a^{[l+2]}$之间的隔层联系。表达式如下：&lt;/p&gt;
&lt;p&gt;$$z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$$&lt;/p&gt;
&lt;p&gt;$$a^{[l+1]} = g(z^{[l+1]})$$&lt;/p&gt;
&lt;p&gt;$$z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}$$&lt;/p&gt;
&lt;p&gt;$$a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$$&lt;/p&gt;
&lt;p&gt;构建一个残差网络就是将许多残差块堆积在一起，形成一个深度网络。&lt;/p&gt;
  &lt;img src=&#34;/img/cnn/Residual-Network.jpg&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;为了便于区分，在 ResNets 的论文
&lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;He et al., 2015. Deep residual networks for image recognition&lt;/a&gt;中，非残差网络被称为&lt;strong&gt;普通网络（Plain Network）&lt;/strong&gt;。将它变为残差网络的方法是加上所有的跳远连接。&lt;/p&gt;
&lt;p&gt;在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，训练错误会先减少，然后开始增多。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。&lt;/p&gt;
  &lt;img src=&#34;/img/cnn/ResNet-Training-Error.jpg&#34; width=&#34;500px&#34;/&gt;
&lt;h3 id=&#34;残差网络有效的原因&#34;&gt;残差网络有效的原因&lt;/h3&gt;
&lt;p&gt;假设有一个大型神经网络，其输入为 $X$，输出为 $a^{[l]}$。给这个神经网络额外增加两层，输出为 $a^{[l+2]}$。将这两层看作一个具有跳远连接的残差块。为了方便说明，假设整个网络中都选用 ReLU 作为激活函数，因此输出的所有激活值都大于等于 0。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/Why-do-residual-networks-work.jpg&#34; alt=&#34;Why-do-residual-networks-work&#34;&gt;&lt;/p&gt;
&lt;p&gt;则有：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
a^{[l+2]} &amp;amp;= g(z^{[l+2]}+a^{[l]})&lt;br&gt;
\\ &amp;amp;= g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;当发生梯度消失时，$W^{[l+2]}\approx0$，$b^{[l+2]}\approx0$，则有：&lt;/p&gt;
&lt;p&gt;$$a^{[l+2]} = g(a^{[l]}) = ReLU(a^{[l]}) = a^{[l]}$$&lt;/p&gt;
&lt;p&gt;因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。&lt;/p&gt;
&lt;p&gt;注意，如果 $a^{[l]}$与 $a^{[l+2]}$的维度不同，需要引入矩阵 $W_s$与 $a^{[l]}$相乘，使得二者的维度相匹配。参数矩阵 $W_s$既可以通过模型训练得到，也可以作为固定值，仅使 $a^{[l]}$截断或者补零。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/ResNet-Paper.png&#34; alt=&#34;ResNet-Paper&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图是论文提供的 CNN 中 ResNet 的一个典型结构。卷积层通常使用 Same 卷积以保持维度相同，而不同类型层之间的连接（例如卷积层和池化层），如果维度不同，则需要引入矩阵 $W_s$。&lt;/p&gt;
&lt;h2 id=&#34;1x1-卷积&#34;&gt;1x1 卷积&lt;/h2&gt;
&lt;p&gt;1x1 卷积（1x1 convolution，或称为 Network in Network）指滤波器的尺寸为 1。当通道数为 1 时，1x1 卷积意味着卷积操作等同于乘积操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/1x1-Conv-1.png&#34; alt=&#34;1x1-Conv-1&#34;&gt;&lt;/p&gt;
&lt;p&gt;而当通道数更多时，1x1 卷积的作用实际上类似全连接层的神经网络结构，从而降低（或升高，取决于滤波器组数）数据的维度。&lt;/p&gt;
&lt;p&gt;池化能压缩数据的高度（$n_H$）及宽度（$n_W$），而 1×1 卷积能压缩数据的通道数（$n_C$）。在如下图所示的例子中，用 32 个大小为 1×1×192 的滤波器进行卷积，就能使原先数据包含的 192 个通道压缩为 32 个。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/1x1-Conv-2.png&#34; alt=&#34;1x1-Conv-2&#34;&gt;&lt;/p&gt;
&lt;p&gt;虽然论文
&lt;a href=&#34;https://arxiv.org/pdf/1312.4400.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lin et al., 2013. Network in network&lt;/a&gt;中关于架构的详细内容并没有得到广泛应用，但是 1x1 卷积的理念十分有影响力，许多神经网络架构（包括 Inception 网络）都受到它的影响。&lt;/p&gt;
&lt;h2 id=&#34;inception-网络&#34;&gt;Inception 网络&lt;/h2&gt;
&lt;p&gt;在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 &lt;strong&gt;Inception 网络的作用&lt;/strong&gt;即是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/Motivation-for-inception-network.jpg&#34; alt=&#34;Motivation-for-inception-network&#34;&gt;&lt;/p&gt;
&lt;p&gt;如图，Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。&lt;/p&gt;
&lt;p&gt;相关论文：
&lt;a href=&#34;https://arxiv.org/pdf/1409.4842.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Szegedy et al., 2014, Going Deeper with Convolutions&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;计算成本问题&#34;&gt;计算成本问题&lt;/h3&gt;
&lt;p&gt;在提升性能的同时，Inception 网络有着较大的计算成本。下图是一个例子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/The-problem-of-computational-cost.png&#34; alt=&#34;The-problem-of-computational-cost&#34;&gt;&lt;/p&gt;
&lt;p&gt;图中有 32 个滤波器，每个滤波器的大小为 5x5x192。输出大小为 28x28x32，所以需要计算 28x28x32 个数字，对于每个数，都要执行 5x5x192 次乘法运算。加法运算次数与乘法运算次数近似相等。因此，可以看作这一层的计算量为 28x28x32x5x5x192 = 1.2亿。&lt;/p&gt;
&lt;p&gt;为了解决计算量大的问题，可以引入 1x1 卷积来减少其计算量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/Using-1x1-convolution.png&#34; alt=&#34;Using-1x1-convolution&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于同一个例子，我们使用 1x1 卷积把输入数据从 192 个通道减少到 16 个通道，然后对这个较小层运行 5x5 卷积，得到最终输出。这个 1x1 的卷积层通常被称作&lt;strong&gt;瓶颈层（Bottleneck layer）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;改进后的计算量为 28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。&lt;/p&gt;
&lt;p&gt;只要合理构建瓶颈层，就可以既显著缩小计算规模，又不会降低网络性能。&lt;/p&gt;
&lt;h3 id=&#34;完整的-inception-网络&#34;&gt;完整的 Inception 网络&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/Inception-module.jpg&#34; alt=&#34;Inception-module&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图是引入 1x1 卷积后的 Inception 模块。值得注意的是，为了将所有的输出组合起来，红色的池化层使用 Same 类型的填充（padding）来池化使得输出的宽高不变，通道数也不变。&lt;/p&gt;
&lt;p&gt;多个 Inception 模块组成一个完整的 Inception 网络（被称为 GoogLeNet，以向 LeNet 致敬），如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cnn/Inception-network.jpg&#34; alt=&#34;Inception-network&#34;&gt;&lt;/p&gt;
&lt;p&gt;注意黑色椭圆圈出的隐藏层，这些分支都是 Softmax 的输出层，可以用来参与特征的计算及结果预测，起到调整并防止发生过拟合的效果。&lt;/p&gt;
&lt;p&gt;经过研究者们的不断发展，Inception 模型的 V2、V3、V4 以及引入残差网络的版本被提出，这些变体都基于 Inception V1 版本的基础思想上。顺便一提，Inception 模型的名字来自电影《盗梦空间》。&lt;/p&gt;
&lt;h2 id=&#34;使用开源的实现方案&#34;&gt;使用开源的实现方案&lt;/h2&gt;
&lt;p&gt;很多神经网络复杂细致，并充斥着参数调节的细节问题，因而很难仅通过阅读论文来重现他人的成果。想要搭建一个同样的神经网络，查看开源的实现方案会快很多。&lt;/p&gt;
&lt;h2 id=&#34;迁移学习&#34;&gt;迁移学习&lt;/h2&gt;
&lt;p&gt;在“搭建机器学习项目”课程中，
&lt;a href=&#34;http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Structuring_Machine_Learning_Projects/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5%EF%BC%882%EF%BC%89?id=%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;迁移学习&lt;/a&gt;已经被提到过。计算机视觉是一个经常用到迁移学习的领域。在搭建计算机视觉的应用时，相比于从头训练权重，下载别人已经训练好的网络结构的权重，用其做&lt;strong&gt;预训练&lt;/strong&gt;，然后转换到自己感兴趣的任务上，有助于加速开发。&lt;/p&gt;
&lt;p&gt;对于已训练好的卷积神经网络，可以将所有层都看作是&lt;strong&gt;冻结的&lt;/strong&gt;，只需要训练与你的 Softmax 层有关的参数即可。大多数深度学习框架都允许用户指定是否训练特定层的权重。&lt;/p&gt;
&lt;p&gt;而冻结的层由于不需要改变和训练，可以看作一个固定函数。可以将这个固定函数存入硬盘，以便后续使用，而不必每次再使用训练集进行训练了。&lt;/p&gt;
&lt;p&gt;上述的做法适用于你只有一个较小的数据集。如果你有一个更大的数据集，应该冻结更少的层，然后训练后面的层。越多的数据意味着冻结越少的层，训练更多的层。如果有一个极大的数据集，你可以将开源的网络和它的权重整个当作初始化（代替随机初始化），然后训练整个网络。&lt;/p&gt;
&lt;h2 id=&#34;数据扩增&#34;&gt;数据扩增&lt;/h2&gt;
&lt;p&gt;计算机视觉领域的应用都需要大量的数据。当数据不够时，**数据扩增（Data Augmentation）**就有帮助。常用的数据扩增包括镜像翻转、随机裁剪、色彩转换。&lt;/p&gt;
&lt;p&gt;其中，色彩转换是对图片的 RGB 通道数值进行随意增加或者减少，改变图片色调。另外，&lt;strong&gt;PCA 颜色增强&lt;/strong&gt;指更有针对性地对图片的 RGB 通道进行主成分分析（Principles Components Analysis，PCA），对主要的通道颜色进行增加或减少，可以采用高斯扰动做法来增加有效的样本数量。具体的 PCA 颜色增强做法可以查阅 AlexNet 的相关论文或者开源代码。&lt;/p&gt;
&lt;p&gt;在构建大型神经网络的时候，数据扩增和模型训练可以由两个或多个不同的线程并行来实现。&lt;/p&gt;
&lt;h2 id=&#34;计算机视觉现状&#34;&gt;计算机视觉现状&lt;/h2&gt;
&lt;p&gt;通常，学习算法有两种知识来源：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;被标记的数据&lt;/li&gt;
&lt;li&gt;手工工程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;**手工工程（Hand-engineering，又称 hacks）**指精心设计的特性、网络体系结构或是系统的其他组件。手工工程是一项非常重要也比较困难的工作。在数据量不多的情况下，手工工程是获得良好表现的最佳方式。正因为数据量不能满足需要，历史上计算机视觉领域更多地依赖于手工工程。近几年数据量急剧增加，因此手工工程量大幅减少。&lt;/p&gt;
&lt;p&gt;另外，在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;集成（Ensembling）：独立地训练几个神经网络，并平均输出它们的输出&lt;/li&gt;
&lt;li&gt;Multi-crop at test time：将数据扩增应用到测试集，对结果进行平均&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是由于这些方法计算和内存成本较大，一般不适用于构建实际的生产项目。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Memory Management</title>
      <link>/courses/operating_system/memory_manage/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/memory_manage/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Main memory and registers are the &lt;strong&gt;only storage CPU can access directly&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Collection of processes&lt;/strong&gt; are waiting on disk to be brought into memory and be executed&lt;/li&gt;
&lt;li&gt;Multiple programs are brought into memory to improve resource utilization and response time to users&lt;/li&gt;
&lt;li&gt;A process may be &lt;strong&gt;moved between disk and memory&lt;/strong&gt; during its execution&lt;/li&gt;
&lt;li&gt;Multistep processing of a program
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b937772940.png&#34; width=&#34;30%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;address-binding&#34;&gt;Address Binding&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Address Binding - Compile Time
&lt;ul&gt;
&lt;li&gt;Program is written as symbolic code&lt;/li&gt;
&lt;li&gt;Compiler translates symbolic code into &lt;strong&gt;absolute code&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;If starting location changes &lt;strong&gt;(recompile&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Address Binding - Load Time
&lt;ul&gt;
&lt;li&gt;Complier translates symbolic code into relocatable code&lt;/li&gt;
&lt;li&gt;Relocatable code: machine language that can be run from any memory location&lt;/li&gt;
&lt;li&gt;If starting location changes (reload the code)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Address Binding - Execution Time
&lt;ul&gt;
&lt;li&gt;Compiler translates symbolic code into logical-address (virtual-address) code&lt;/li&gt;
&lt;li&gt;Special hardware (MMU memory management unit) is needed for this scheme&lt;/li&gt;
&lt;li&gt;Most general-purpose OS use this method&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MMU(Memory-management unit)
&lt;ul&gt;
&lt;li&gt;Hardware device that &lt;strong&gt;maps virtual to physical address&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The value in the &lt;strong&gt;relocation register is added to every address&lt;/strong&gt; generated by a user process at the time it is sent to memory
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b998d7bef2.png&#34; width=&#34;40%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Logical VS. Physical Address
&lt;ul&gt;
&lt;li&gt;Logical Address — generated by CPU (a.k.a virtual address)&lt;/li&gt;
&lt;li&gt;Physical address — seen by the memory module&lt;/li&gt;
&lt;li&gt;Compile-time &amp;amp; load time address binding (logical address = physical address)&lt;/li&gt;
&lt;li&gt;Execution-time address binding (logical address $\neq$ physical address)&lt;/li&gt;
&lt;li&gt;The user program deals with logical addresses; it never sees the real physical addresses&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dynamic-loading&#34;&gt;Dynamic loading&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The entire program doesn&amp;rsquo;t need all memory for it to execute, it&amp;rsquo;s a routine is loaded into memory when it is called&lt;/li&gt;
&lt;li&gt;Better memory-space utilization, unused routine is never loaded, particularly useful when large amounts of code are infrequently used (e.g., error handling code)&lt;/li&gt;
&lt;li&gt;No special support from OS is required implemented through program (library, API calls)&lt;/li&gt;
&lt;li&gt;Dynamic Loading Example in C
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dlopen()&lt;/code&gt;: opens a library and prepares it for use&lt;/li&gt;
&lt;li&gt;&lt;code&gt;desym()&lt;/code&gt;: looks up the value of a symbol in a given opened library&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dlclose()&lt;/code&gt;: close a DL library&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;#include &amp;lt;dlfcn.h&amp;gt;
int main() {
    double (*cosine)(double);
    void* handle = dlopen(&amp;quot;/lib/libm.so.6&amp;quot;, RTLD_LAZY);
    cosine = dlsym(handle, &amp;quot;cos&amp;quot;);
    printf(&amp;quot;%f\n&amp;quot;, (*cosine)(2.0)); // load into memory
    dlclose(handle);
} 
&lt;/code&gt;&lt;/pre&gt;
  &lt;img src=&#34;https://i.loli.net/2019/03/15/5c8b9e9388d4f.png&#34; width=&#34;30%&#34;/&gt;
&lt;h3 id=&#34;staticdynamic-linking&#34;&gt;Static/Dynamic Linking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Static Linking: libraries are combined by the loaded into  the program in-memory image
&lt;ul&gt;
&lt;li&gt;Waste memory: duplicated code&lt;/li&gt;
&lt;li&gt;Faster during execution time
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8ba03c16a1b.png&#34; width=&#34;30%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dynamic Linking: Linking postponed until execution time
&lt;ul&gt;
&lt;li&gt;Only one code copy in memory and shared by everyone&lt;/li&gt;
&lt;li&gt;A stub is included in the program in-memory image for each lib reference&lt;/li&gt;
&lt;li&gt;Stub call $\rightarrow$ check if the referred lib is in memory $\rightarrow$ if not, load the lib $\rightarrow$ execute the lib&lt;/li&gt;
&lt;li&gt;DLL (Dynamic link library) on Windows
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8ba17ef00e8.png&#34; width=&#34;10%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;swapping&#34;&gt;Swapping&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A process can be swapped out of memory to a &lt;strong&gt;backing store&lt;/strong&gt;, and later brought back into memory for continuous execution, also used by &lt;strong&gt;midterm scheduling&lt;/strong&gt;, different from context switch&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backing store&lt;/strong&gt;: a chunk of disk, separated from file system, to provide direct access to these memory images&lt;/li&gt;
&lt;li&gt;Free up memory, roll out, roll in, swap lower-priority process with a higher one&lt;/li&gt;
&lt;li&gt;Swap back memory location
&lt;ol&gt;
&lt;li&gt;if binding is done at compile / load time, swap back memory address must be same&lt;/li&gt;
&lt;li&gt;if binding is done at execution time, swap back memory address can be different&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;A process to be swapped == must be idle (不能做I/O)
&lt;ul&gt;
&lt;li&gt;Imagine a process that is waiting for I/O is swapped? 1. Never swap a process with pending I/O 2. I/O operations are done through OS buffers (i.e. a memory space not belongs to any user processes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Major part os swap time is transfer time; total transfer time is directly proportional to the amount of memory swapped&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;memory-allocation&#34;&gt;Memory allocation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fixed-partition allocation (规划停车场)
Each process loads into one partition of fixed-size, degree of multi-programming is bounded by the number of partitions&lt;/li&gt;
&lt;li&gt;Variable-size partition
Hole: block of contiguous free memory, holes of various size are scattered in memory&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multiple-partition-variable-size-method&#34;&gt;Multiple Partition (Variable-size) method&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;First-fit allocate the 1st hole that fits&lt;/li&gt;
&lt;li&gt;Best-fit allocate the smallest hole that fits (must search through the whole list)&lt;/li&gt;
&lt;li&gt;Worst-fit allocate the largest hole (must also search through the whole list)&lt;/li&gt;
&lt;li&gt;First-fit and best-fit better than worst-fit in terms of speed and storage utilization&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;fragmentation-存在零碎的空间&#34;&gt;Fragmentation (存在零碎的空间)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;external fragmentation
Total free memory space is big enough to satisfy a request, but is not contiguous, occur in variable-size allocation&lt;/li&gt;
&lt;li&gt;Internal fragmentation
&lt;ul&gt;
&lt;li&gt;Memory that is internal to a partition but is not being used, occur in fixed-partition allocation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solution: compaction
&lt;ul&gt;
&lt;li&gt;Shuffle the memory contents to place all free memory together in one large block at execution time&lt;/li&gt;
&lt;li&gt;Only if binding is done at execution time
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8ba92f9fed7.png&#34; width=&#34;25%&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;non-contiguous-memory-allocation---paging&#34;&gt;Non-contiguous memory Allocation - Paging&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Divide physical memory into fixed-sized blocks called &lt;strong&gt;frames&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Divide logical address space into blocks of the same size called &lt;strong&gt;pages&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;To run a program of n pages, need to find n free frames and load the program&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;keep track of free frames&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Set up a &lt;strong&gt;page table&lt;/strong&gt; to translate logical to physical addresses&lt;/li&gt;
&lt;li&gt;Benefit
&lt;ul&gt;
&lt;li&gt;Allow the physical-address space of a process to be noncontiguous&lt;/li&gt;
&lt;li&gt;Avoid external fragmentation&lt;/li&gt;
&lt;li&gt;Limited internal fragmentation&lt;/li&gt;
&lt;li&gt;Provide &lt;strong&gt;shared memory/pages&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;page-table&#34;&gt;Page table&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Each entry maps to the base address of a page in physical memory&lt;/li&gt;
&lt;li&gt;A structure maintained by OS for each process
&lt;ul&gt;
&lt;li&gt;page table includes only pages owned by a process&lt;/li&gt;
&lt;li&gt;A process cannot access memory outside its space
&lt;img src=&#34;https://i.loli.net/2019/03/15/5c8baa11b9f0c.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;address-translation-scheme&#34;&gt;Address Translation Scheme&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Logical address is divided into two parts
&lt;ul&gt;
&lt;li&gt;Page number (p)
Use as an index into a page table which contains base address of each page in physical memory, N bits means a process can allocate at most $2^{N}$ pages&lt;/li&gt;
&lt;li&gt;Page offset(d)
Combines with base address to define the physical memory address that is sent to the memory unit, N bits means the page size is $2^{N}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Physical address = page base address + page offset&lt;/li&gt;
&lt;li&gt;example: If page size is 1KB(2^10) and page 2 maps to frame 5. Given 13 bits logical address: (p=2, d=20), what is physical address?
$$5*(1KB) + 20 = 1,010,000,000,000 + 0,000,010,100 = 1,010,000,010,100$$&lt;/li&gt;
&lt;li&gt;Figure
&lt;img src=&#34;https://i.loli.net/2019/03/16/5c8c64134af0b.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;total number of pages dose not need to be the same as the total number of frames&lt;/li&gt;
&lt;li&gt;Given 32 bits logical address, 36 bits physical address and 4KB page size, what does it mean?
&lt;ul&gt;
&lt;li&gt;Page table size: $2^{32} / 2^{12} = 2^{20}$ entries&lt;/li&gt;
&lt;li&gt;Max program memory: $2^{32} = 4GB$&lt;/li&gt;
&lt;li&gt;Total physical memory size: $2^{36} = 64GB$&lt;/li&gt;
&lt;li&gt;Number of bits for page number: $2^{20}$ pages $\rightarrow 20$ bits&lt;/li&gt;
&lt;li&gt;Number of bits for frame number: $2^{24} \text{frames} \rightarrow 2^24$ bits&lt;/li&gt;
&lt;li&gt;number of bits for page offset: 4KB page size = $2^{12}$ bytes $\rightarrow 12$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Page / Frame Size
&lt;ul&gt;
&lt;li&gt;Typically power of 2&lt;/li&gt;
&lt;li&gt;Ranging from 512 bytes to 16 MB/ page, 4KB/8KB page is commonly used&lt;/li&gt;
&lt;li&gt;Larger page size $\rightarrow$ More space waste&lt;/li&gt;
&lt;li&gt;page sizes have grown over time , memory, process, data sets have become larger&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;paging-summary&#34;&gt;Paging Summary&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Paging helps separate user&amp;rsquo;s view of memory and the actual physical memory&lt;/li&gt;
&lt;li&gt;User view&amp;rsquo;s memory: one single contiguous space&lt;/li&gt;
&lt;li&gt;OS maintains a copy of the &lt;strong&gt;page table for each process&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;OS maintains a frame table for managing physical memory
&lt;ol&gt;
&lt;li&gt;One entry for each physical frame&lt;/li&gt;
&lt;li&gt;Indicate whether a frame is free or allocated&lt;/li&gt;
&lt;li&gt;If allocated, to which page of which process or processes&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;implementation-of-page-table&#34;&gt;Implementation of Page Table&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Page table is kept in memory&lt;/li&gt;
&lt;li&gt;Page table base register (PTBR) (需要load到MMU的register)
&lt;ul&gt;
&lt;li&gt;The physical memory address of the page table&lt;/li&gt;
&lt;li&gt;The PTBR value is stored in PCB (Process Control Block)&lt;/li&gt;
&lt;li&gt;Changing the value of PTBR during &lt;strong&gt;Context-switch&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;With PTBR, each memory reference results in 2 memory reads, one for the page table and one for the real address&lt;/li&gt;
&lt;li&gt;The 2-access problem can be solved by, &lt;strong&gt;Translation Look-aside Buffers&lt;/strong&gt;(TLB) which is implemented by &lt;strong&gt;Associative memory&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Associative Memory
All memory entries can be accessed &lt;strong&gt;at the same time&lt;/strong&gt;, lookup time is O(1), each entry corresponds to an associative register, the number of entries are limited (64 ~ 1024)
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8da6019d2c1.png&#34; width=&#34;400px&#34;/&gt;
&lt;div class=&#34;note info&#34;&gt;&lt;p&gt; TLB is a cache for page table shared by all processes &lt;/p&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;TLB must be flushed after a context switch, otherwise, TLB entry must has a PID field (address-space identifiers (ASIDs)), the flush method is preferred.&lt;/li&gt;
&lt;li&gt;Effective Memory-Access Time (EMAT)
&lt;ol&gt;
&lt;li&gt;$20 ns$ for TLB search&lt;/li&gt;
&lt;li&gt;$100 ns$ for memory access&lt;/li&gt;
&lt;li&gt;$70%$ TLB hit-ratio
&lt;ul&gt;
&lt;li&gt;$70$ TLB hit-ratio $\text{EMAT} = 0.7\times (20+100) + (1-0.7) \times (20+100+100) = 150ns$&lt;/li&gt;
&lt;li&gt;$98%$ TLB hit-ration $\text{EMAT} = 0.98\times 120+0.02 \times 220=122ns$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;memory-protection&#34;&gt;Memory Protection&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Each page is associated with a set of &lt;strong&gt;protection bit&lt;/strong&gt; in the page table
&lt;div class=&#34;note info&#34;&gt;&lt;p&gt;(A bit to define read/write/execution permission)&lt;/p&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Common use : valid-invalid bit
&lt;ul&gt;
&lt;li&gt;Valid: the page/frame is in the process&amp;rsquo; logical address space, and is thus a legal page&lt;/li&gt;
&lt;li&gt;Invalid: the page/frame is not in the process&amp;rsquo; logical address space&lt;/li&gt;
&lt;li&gt;potential issues: Un-used page entry cause memory-waste, use page table length register(PTLR)&lt;/li&gt;
&lt;li&gt;Process memory may NOT be on the boundary of a page, memory limit register is still needed
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8db2c2cdea7.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Shared Pages
Paging allows processes share common code, which must be reentrant
&lt;ul&gt;
&lt;li&gt;Reentrant code (pure code): it never change during execution, text editors, compilers, web servers, etc&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Only one copy&lt;/strong&gt; of the shared code needs to be kept in physical memory&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Two (several) virtual addresses&lt;/strong&gt; are mapped to one physical address&lt;/li&gt;
&lt;li&gt;Process keeps a copy of its own private data and code&lt;/li&gt;
&lt;li&gt;Shared code must appear in the same location in the logical address space of all processes
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8db414864da.png&#34; width=400px/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;page-table-memory-structure&#34;&gt;Page Table Memory Structure&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Page table could be huge and difficult to be loaded
&lt;ul&gt;
&lt;li&gt;4GB ($2^{32}$) logical address space with 4KB ($2^{12}$) page, needs 1 million $2^{20}$ page table entry&lt;/li&gt;
&lt;li&gt;Assume each entry need 4 bytes (32 bits), total size = 4MB (MMU 读的时候是需要连续的4MB memory)&lt;/li&gt;
&lt;li&gt;Need to break it into several smaller page tables, better within  a single page size (i.e. 4KB)&lt;/li&gt;
&lt;li&gt;Or reduces the total size of page table&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;hierarchical-paging&#34;&gt;Hierarchical Paging&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Break up the logical address space into multiple page tables
&lt;ul&gt;
&lt;li&gt;12-bit offset (d) $\rightarrow$ 4KB($2^{12}$) page size&lt;/li&gt;
&lt;li&gt;10-bit outer page number $\rightarrow$ 1K $2^{10}$ page table entries&lt;/li&gt;
&lt;li&gt;10-bit inner page number $\rightarrow$ 1K $2^{10}$ page table entries
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8ddfd25900c.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two-level paging example(32-bit address with 4KB ($2^{12}$) page size)
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8dbf784c160.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;example: &lt;img src=&#34;https://i.loli.net/2019/04/01/5ca17e7a97138.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;64-bit Address?
&lt;ol&gt;
&lt;li&gt;2 level: 42(p1) + 10 (p2) + 12 (offset), outer table requires $2^{42} \times 4B = 16 {TB}$ contiguous memory&lt;/li&gt;
&lt;li&gt;6 level: $12(p1) + 10(p2) + 10(p3) + 10(p4) + 10 (p5) + 12 (offset)$, needs 6 memory accesses&lt;/li&gt;
&lt;/ol&gt;
&lt;div class = &#34;note info&#34;&gt;&lt;p&gt;
    SPAPC(32-bit) and linux use 3-level paging, Motorola 68030 (32-bit) use 4-level paging
&lt;/p&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;hashed-page-table&#34;&gt;Hashed Page Table&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Commonly-used for address &amp;gt; 32 bits&lt;/li&gt;
&lt;li&gt;Virtual page number is hashed into a hash table&lt;/li&gt;
&lt;li&gt;The size of the hash table varies, larger hash  table $\rightarrow$  smaller chains in each entry&lt;/li&gt;
&lt;li&gt;Each entry in the hashed table contains
&lt;ul&gt;
&lt;li&gt;Virtual Page Number, Frame Number, Next Pointer&lt;/li&gt;
&lt;li&gt;Pointers waster memory&lt;/li&gt;
&lt;li&gt;Traverse linked list waste time &amp;amp; cause additional memory references
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8dbdd2409f8.png&#34; width=400px/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class = &#34;note info&#34;&gt;&lt;p&gt; 将 entries group 到一起， 存入连续的空间，MMU一次性读进去，可以减少 LinkedList traverse 的时间&lt;/p&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inverted-page-table-很少见&#34;&gt;Inverted Page Table (很少见）&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Maintains NO page table for each process ( 节省 memory 空间）&lt;/li&gt;
&lt;li&gt;Maintains a &lt;strong&gt;frame table&lt;/strong&gt; for the whole memory, one entry for each real frame of memory&lt;/li&gt;
&lt;li&gt;Each entry in the frame table has (PID Page number)&lt;/li&gt;
&lt;li&gt;Eliminate the memory needed for page tables but increase memory access time, each access needs to search the whole frame table (use hashing for the frame table)&lt;/li&gt;
&lt;li&gt;Hard to support &lt;strong&gt;shared/page memory&lt;/strong&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8dc1b979f62.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;segmentation&#34;&gt;Segmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Memory-management scheme that supports user view of memory&lt;/li&gt;
&lt;li&gt;A program is a collection of segments. A segment is a logical unit includes following:
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8dc2e3156bb.png&#34; width=&#34;300px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;segmentation-table&#34;&gt;Segmentation Table&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Logical address: (segmentation #, offset), offset has the same length as physical address&lt;/li&gt;
&lt;li&gt;Maps two-dimensional physical addresses, each table entry has:
&lt;ul&gt;
&lt;li&gt;Base (4 bytes): the start physical address&lt;/li&gt;
&lt;li&gt;Limit (4 bytes): the length of the segment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Segment-table base register(STBR), the physical address of the segmentation table&lt;/li&gt;
&lt;li&gt;Segment-table length register (STLR), the number of segments&lt;/li&gt;
&lt;li&gt;example
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca1834f530a6.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;segmentation-hardware&#34;&gt;Segmentation Hardware&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Limit register is used to check offset length&lt;/li&gt;
&lt;li&gt;MMU allocate memory by assigning an appropriate base address for each segment (physical address cannot overlap between segments)
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca1840045b4b.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Sharing and Protection
&lt;ul&gt;
&lt;li&gt;Protection bits associated with segments
&lt;ul&gt;
&lt;li&gt;Read-only segment (code)&lt;/li&gt;
&lt;li&gt;Read-write segments (data, heap, stack)&lt;/li&gt;
&lt;li&gt;Code sharing occurs at segment level (memory communication, shared library)&lt;/li&gt;
&lt;li&gt;Share segment by having same base in two segment tables&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;segmentation--paging&#34;&gt;Segmentation &amp;amp; paging&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apply segmentation in logical address space&lt;/li&gt;
&lt;li&gt;Apply Paging in physical address space
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8dd9bc03335.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;address-translation&#34;&gt;Address Translation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CPU generates logical address
&lt;ol&gt;
&lt;li&gt;Given to segmentation unit $\rightarrow$ produces liner address&lt;/li&gt;
&lt;li&gt;Linear address given to paging unit $\rightarrow$ generates physical address in main memory&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Segmentation and paging units form equivalent of MMU
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8ddd0a0ca4f.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Let the physical memory size is 521B, the page size is 32B and the logical address of a program can have 8 segments. Given a 12 bits hexadecimal logical address &amp;ldquo;448&amp;rdquo;, translate the address with below page and segment tables
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8dde36a59f1.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Transport Layer</title>
      <link>/courses/computer_network/transport_layer/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/computer_network/transport_layer/</guid>
      <description>&lt;h2 id=&#34;transport-services-and-protocols&#34;&gt;Transport services and protocols&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;provide logical communication between app processes running on different hosts&lt;/li&gt;
&lt;li&gt;transport protocols run in end systems
&lt;ul&gt;
&lt;li&gt;send side: breaks app message into segments, passes to network layer&lt;/li&gt;
&lt;li&gt;receive side: reassembles segments into message passes to app layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;more than one transport protocol available to apps (Internet: TCP and UDP)
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9dd08c3e25b.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;network layer vs. transport layer
&lt;ul&gt;
&lt;li&gt;network layer: logical communication between hosts&lt;/li&gt;
&lt;li&gt;transport layer: logical communication between processes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;reliable, in-order delivery(TCP), congestion control, flow control, connection setup&lt;/li&gt;
&lt;li&gt;unreliable, unordered delivery(UDP): no-frills extension of &amp;ldquo;best-effort&amp;rdquo; IP&lt;/li&gt;
&lt;li&gt;services not available: delay guarantees, bandwidth guarantees&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;multiplexing--demultiplexing&#34;&gt;Multiplexing / demultiplexing&lt;/h2&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9dd2c317a10.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;demultiplexing: delivering received segments to correct socket&lt;/li&gt;
&lt;li&gt;multiplexing: gathering data from multiple sockets, enveloping data with header(later used for demultiplexing)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;demultiplexing&#34;&gt;Demultiplexing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Host receive IP datagrams:
&lt;ul&gt;
&lt;li&gt;each datagram has source IP address, destination IP address&lt;/li&gt;
&lt;li&gt;each datagram carries 1 transport-layer segment&lt;/li&gt;
&lt;li&gt;each segment has source, destination port number&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;host uses IP addresses &amp;amp; port numbers to direct segment to appropriate socket
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9dd47a8dd0d.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;connectionless-demultiplexing&#34;&gt;Connectionless demultiplexing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;create sockets with port number&lt;/li&gt;
&lt;li&gt;UDP socket identified by two-tuple(destination IP address and destination port number)&lt;/li&gt;
&lt;li&gt;when host receives UDP segment: checks destination port number in segment, directs UDP segment to socket with that port number&lt;/li&gt;
&lt;li&gt;IP datagrams with different source IP addresses and/or source port numbers directed to same socket&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;connection-oriented-demultiplexing&#34;&gt;Connection-oriented demultiplexing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;TCP socket identified by 4-tuple: source/destination IP address, source/destination port number&lt;/li&gt;
&lt;li&gt;receiving host uses all four segment to appropriate socket&lt;/li&gt;
&lt;li&gt;Server host may support many simultaneous TCP sockets, each socket identified by its own 4-tuple&lt;/li&gt;
&lt;li&gt;Web servers have different sockets for each connection client, non-persistent HTTP will have different socket for each request
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9dda9a2fa8b.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;udp-user-datagram-protocol-rfc-768&#34;&gt;UDP: User Datagram Protocol (RFC 768)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;no frills&amp;rdquo;, &amp;ldquo;bare bones&amp;rdquo; Internet transport protocol&lt;/li&gt;
&lt;li&gt;&amp;ldquo;best effort&amp;rdquo; service&lt;/li&gt;
&lt;li&gt;connectionless, no handshaking between UDP sender, receiver, each UDP segment handled independently of others&lt;/li&gt;
&lt;li&gt;why using UDP:
&lt;ul&gt;
&lt;li&gt;no connection establishment&lt;/li&gt;
&lt;li&gt;simple: no connection state at sender, receiver&lt;/li&gt;
&lt;li&gt;small segment header&lt;/li&gt;
&lt;li&gt;no connection control: UDP can blast away as fast as desired&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;often used for streaming multimedia apps&lt;/li&gt;
&lt;li&gt;other UDP uses (DNS, SNMP(Simple network manager protocol) 网络管理资料收集)&lt;/li&gt;
&lt;li&gt;reliable transfer over UDP: add reliability at application layer(application-specific error recovery)
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9ddcd48373c.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;udp-checksum&#34;&gt;UDP checksum&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Goal: detect &amp;ldquo;errors&amp;rdquo; in transmitted segment&lt;/li&gt;
&lt;li&gt;sender:
&lt;ul&gt;
&lt;li&gt;treat segment contents as sequence of 16-bit integers&lt;/li&gt;
&lt;li&gt;checksum: addition (1&amp;rsquo;s complement sum) of segment contents&lt;/li&gt;
&lt;li&gt;sender puts checksum value into UDP checksum fields&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Receiver:
&lt;ul&gt;
&lt;li&gt;compute checksum of received segment&lt;/li&gt;
&lt;li&gt;check if computed checksum equals checksum field value
(No, error-detected, Yes, no error-detected)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;principles-of-reliable-data-transfer&#34;&gt;Principles of Reliable data transfer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;top-10 list of important networking topics
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9ddf4ae2bd3.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;characteristics of unreliable channel will determine complexity of reliable data transfer protocol&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rdt_send()&lt;/code&gt;: called from above, passed data to deliver to receiver upper layer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;udt_send()&lt;/code&gt;: called by &lt;code&gt;rdt&lt;/code&gt;, to transfer packet over unreliable channel to receiver&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rdt_rcv()&lt;/code&gt;: called when packet arrives on &lt;code&gt;rcv-side&lt;/code&gt; of channel&lt;/li&gt;
&lt;li&gt;&lt;code&gt;deliver_data()&lt;/code&gt; called by &lt;code&gt;rdt&lt;/code&gt; to deliver data to upper&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reliable-data-transfer&#34;&gt;Reliable data transfer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;incrementally develop sender, receiver sides of reliable data transfer protocol(rdt)&lt;/li&gt;
&lt;li&gt;consider only unidirectional data transfer(but control info will flow on both directions)&lt;/li&gt;
&lt;li&gt;use finite state machines(FSM) to specify sender, receiver&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rdt10-reliable-transfer-over-a-reliable-channel&#34;&gt;Rdt1.0: reliable transfer over a reliable channel&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;underlying channel perfectly reliable
&lt;ul&gt;
&lt;li&gt;no bit errors&lt;/li&gt;
&lt;li&gt;no loss of packets&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;separate FSMs for sender, receiver
&lt;ul&gt;
&lt;li&gt;sender sends data into underlying channel&lt;/li&gt;
&lt;li&gt;receiver read data from underlying channel
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9df830253ec.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rdt20-channel-with-bit-errors&#34;&gt;Rdt2.0: channel with bit errors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;underlying channel may flip bits in pocket: checksum to detect bit errors&lt;/li&gt;
&lt;li&gt;how to recover from errors:
&lt;ul&gt;
&lt;li&gt;acknowledgements(ACKs) receiver explicitly tells sender that packet received OK&lt;/li&gt;
&lt;li&gt;negative acknowledgements(NAKs) receiver explicitly tells sender that packet had errors&lt;/li&gt;
&lt;li&gt;sender retransmits packet on receipt of NAK&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;new mechanisms in rdt2.0
&lt;ul&gt;
&lt;li&gt;error detection&lt;/li&gt;
&lt;li&gt;receiver feedback: control msgs(ACK, NAK)
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9dfa762b88f.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;rdt2.0 has a fatal flow(if ACK/NAK corrupted?)
&lt;ul&gt;
&lt;li&gt;sender doesn&amp;rsquo;t know what happened at receiver&lt;/li&gt;
&lt;li&gt;can&amp;rsquo;t just retransmit: possible duplicate&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Handling duplicates:
&lt;ul&gt;
&lt;li&gt;sender retransmits current packet if ACK/NAK corrupted&lt;/li&gt;
&lt;li&gt;sender adds sequence number to each packet&lt;/li&gt;
&lt;li&gt;receiver discards (doesn&amp;rsquo;t deliver up) duplicate packet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stop and wait (sender sends one packet, then waits for receiver response)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rdt21-sender-handles-garbled-acknaks&#34;&gt;Rdt2.1: sender, handles garbled ACK/NAKs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Sender
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9dfcac45e5e.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Receiver
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9dfd8b78c4b.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rdt22-a-nak-free-protocol&#34;&gt;Rdt2.2: a NAK-free protocol&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;same functionality as rdt2.1, using ACKS only&lt;/li&gt;
&lt;li&gt;instead of NAK, receiver sends ACK for &lt;strong&gt;last packet&lt;/strong&gt; received OK, receiver must explicitly include sequence number of packet being ACKed&lt;/li&gt;
&lt;li&gt;duplicate ACK at sender results in same action as NAK: retransmit current packet&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rdt30-channels-with-errors-and-loss&#34;&gt;Rdt3.0: channels with errors and loss&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;New assumption:
&lt;ul&gt;
&lt;li&gt;underlying channel can also pockets(data or ACKs)&lt;/li&gt;
&lt;li&gt;checksum, sequence number, ACKs, retransmission will be of help, but not enough&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Approach: sender waits &amp;ldquo;reasonable&amp;rdquo; amount of time for ACK&lt;/li&gt;
&lt;li&gt;retransmits if no ACK received in this time&lt;/li&gt;
&lt;li&gt;if packet (or ACK) just delayed (not lost):
&lt;ul&gt;
&lt;li&gt;retransmission will be duplicate, but use of sequence number&lt;/li&gt;
&lt;li&gt;receiver must specify sequence number of packet being ACKed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;requires countdown timer
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9e04ebd9810.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;performance of rdt3.0
1Gbps link, 15 ms prop. delay, 8000 bit packet:
&lt;ul&gt;
&lt;li&gt;sender time
$$ d = L/R = \frac{8000 bits}{10^9bps} = 8 ms$$&lt;/li&gt;
&lt;li&gt;utilization - fraction of time sender busy sending
$$ \frac{L/R}{RTT + L/R} = 0.008/30.0008 = 0.00027$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;stop and wait operation
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9e07305c4bb.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pipelined-protocols&#34;&gt;Pipelined protocols&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;sender allows multiple, &amp;ldquo;in-flight&amp;rdquo; yet-to-be-acknowledged` packets
&lt;ul&gt;
&lt;li&gt;range of sequence numbers must be increased&lt;/li&gt;
&lt;li&gt;buffering at sender and/or receiver
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9e07a57c05d.png&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two generic forms of pipelined protocols: &lt;strong&gt;go-Back-N, selective repeat&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;go-back-n-big-picture&#34;&gt;Go-back-N: big picture&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;sender can have up to N unacked packets in pipeline&lt;/li&gt;
&lt;li&gt;Receiver only sends cumulative ACKs, doesn&amp;rsquo;t ACK packet if there&amp;rsquo;s gap&lt;/li&gt;
&lt;li&gt;Sender has timer for oldest unacked packet, if timer expires, retransmit all unlocked packets
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9e09de9caf2.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;ACK(n): ACKs all packet up to including sequence number n &amp;ndash; &amp;ldquo;cumulative ACK&amp;rdquo; (may receive duplicate ACKs)&lt;/li&gt;
&lt;li&gt;timer for each in-flight packet&lt;/li&gt;
&lt;li&gt;timeout(n): retransmit packet n and all higher sequence number packets in window
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9e0ef5f0e00.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;sender
&lt;img src=&#34;https://i.loli.net/2019/03/29/5c9e0ef5f0e00.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;receiver
&lt;img src=&#34;https://i.loli.net/2019/03/31/5ca036ac9a793.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;example
&lt;img src=&#34;https://i.loli.net/2019/03/31/5ca036ee8e6d8.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;selective-repeat-big-picture&#34;&gt;Selective Repeat: big picture&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Sender can have up to N unacked packets in pipeline&lt;/li&gt;
&lt;li&gt;Receiver ACKs individual packets&lt;/li&gt;
&lt;li&gt;Sender maintains timer for each unacked packet, when timer expires, retransmit only unacked packet
&lt;img src=&#34;https://i.loli.net/2019/03/31/5ca0371c00c83.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Sender
&lt;ul&gt;
&lt;li&gt;if next available sequence number in window, send packet&lt;/li&gt;
&lt;li&gt;timeout(n): resend packet n, restart timer&lt;/li&gt;
&lt;li&gt;ACK(n) in (sendbase, sendbase + N): mark packet n as reader, if n smallest unACked packet, advance window base to next unACKed sequence number&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;receiver
&lt;ul&gt;
&lt;li&gt;send ACK(N)&lt;/li&gt;
&lt;li&gt;out-of-order : buffer&lt;/li&gt;
&lt;li&gt;in-order: deliver, advance window to next not-yet-received packet&lt;/li&gt;
&lt;li&gt;if packet n in [revbase-revbase+N], ACK(n), otherwise ignore
&lt;img src=&#34;https://i.loli.net/2019/03/31/5ca03a0f20daa.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;problem
&lt;img src=&#34;https://i.loli.net/2019/03/31/5ca03dc05ef79.png&#34; width=&#34;500px&#34;/&gt;
window size must be less than or equal t0 half the size of the sequence number space&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;connection-oriented-transport-tcp&#34;&gt;Connection-Oriented Transport: TCP&lt;/h2&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Point-to-Point&lt;/li&gt;
&lt;li&gt;reliable: in-order byte stream: no &amp;ldquo;message boundaries&amp;rdquo;&lt;/li&gt;
&lt;li&gt;pipelined: TCP congestion and flow control set window size&lt;/li&gt;
&lt;li&gt;send &amp;amp; receive buffer&lt;/li&gt;
&lt;li&gt;flow control: sender will not overwhelm receiver&lt;/li&gt;
&lt;li&gt;full duplex data: bi-directional data flow in some connection, maximum segment size(MSS， 控制流量，控制传输速度)&lt;/li&gt;
&lt;li&gt;connection-oriented: handshaking (Exchange of control messages)&lt;/li&gt;
&lt;li&gt;TCP segment structure
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca1b0f974767.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Sequence number: byte stream &amp;ldquo;number&amp;rdquo; of first byte in segment&amp;rsquo;s data&lt;/li&gt;
&lt;li&gt;ACKs: sequence number of next byte expected from other side, cumulative ACK
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca1b4dc5a0aa.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;round-trip-time-estimation-and-timeout&#34;&gt;Round-Trip Time Estimation and Timeout&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;how receiver handles out-of-order segments: TCP spec doesn&amp;rsquo;t say, up to implementor&lt;/li&gt;
&lt;li&gt;how to set TCP timeout value?
&lt;ul&gt;
&lt;li&gt;longer than RTT(Round Trip Time)&lt;/li&gt;
&lt;li&gt;too short: premature timeout (unnecessary retransmission)&lt;/li&gt;
&lt;li&gt;too long: slow reaction to segment loss&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;how to estimate RTT?
&lt;ul&gt;
&lt;li&gt;SampleRTT: measured time from segment transmission until ACT receipt (ignore retransmissions)&lt;/li&gt;
&lt;li&gt;SampleRTT will vary, want estimated RTT &amp;ldquo;smoother&amp;rdquo;, average several recent measurement, not just current SampleRTT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;EstimatedRTT = $(1-\alpha)\cdot \text{EstimatedRTT} + \alpha \cdot \text{SampleRTT} \text{ typically}, \alpha$ is 0.125&lt;/li&gt;
&lt;li&gt;estimate of how much SampleRTT deviates from EstimatedRTT
$DevRTT = (1-\beta) \cdot \text{DevRTT} \cdot \beta \cdot |\text{SampleRTT}-\text{Estimated RTT}|$, typically, $\beta = 0.25$&lt;/li&gt;
&lt;li&gt;set timeout interval
TimeoutInterval = EstimatedRTT + 4 * DevRTT&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tcp-sender-events&#34;&gt;TCP sender events&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;data received from application:
&lt;ul&gt;
&lt;li&gt;create segment with sequence number&lt;/li&gt;
&lt;li&gt;sequence number is a byte-stream number of first data byte in segment&lt;/li&gt;
&lt;li&gt;start timer if not already running (think of timer as for oldest unacked segment)&lt;/li&gt;
&lt;li&gt;expiration interval: timeout Interval&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;timeout: retransmit segment that caused timeout, restart timer&lt;/li&gt;
&lt;li&gt;ACK received: if acknowledges previously unacked segments, update what is known to be acked, start timer if there are outstanding segments&lt;/li&gt;
&lt;li&gt;figure:
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca1febf72143.png&#34; width=&#34;700px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;TCP retransmission Scenarios
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca1ff1ec67a4.png&#34; width=&#34;400px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca1ff540a9f6.png&#34; width=&#34;450px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca1ff7dbea07.png&#34; width=&#34;4550px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tcp-ark-generation&#34;&gt;TCP ARK Generation&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca1ffd31e346.png&#34; width=&#34;600px&#34;/&gt;
&lt;h3 id=&#34;fast-retransmit&#34;&gt;Fast retransmit&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Time-out period often relatively long (long delay before resending lost packet)&lt;/li&gt;
&lt;li&gt;detect lost segment via duplicate ACKs (If segment is lost, there will likely be many duplicate ACKs)&lt;/li&gt;
&lt;li&gt;if Sender receives ACKs for the same data, it supposes that segment after ACKed data was lost&lt;/li&gt;
&lt;li&gt;fast retransmit: resend segment before timer expires
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca200cc1f243.png&#34; width=&#34;450px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Algorithm
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca20110e65d0.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;flow-control&#34;&gt;Flow Control&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Sender won&amp;rsquo;t overflow receiver&amp;rsquo;s buffer by transmitting too much or too fast&lt;/li&gt;
&lt;li&gt;Receive side of TCP connection has a receive buffer
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca2019f72288.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Suppose TCP receiver discards out-of-order segments&lt;/li&gt;
&lt;li&gt;Receiver advertises spare room by including value of  RcvWindow in segments&lt;/li&gt;
&lt;li&gt;Sender limits unACKed data to RcvWindow&lt;/li&gt;
&lt;li&gt;rwnd(receive window) = RcvBuffer - [LastByteRcvd - LastByteRead]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tcp-connection-management&#34;&gt;TCP Connection Management&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Three way handshake
&lt;ol&gt;
&lt;li&gt;Client host sends TCP SYN segment to server, specifies initial sequence number (no data)&lt;/li&gt;
&lt;li&gt;Server host receivers SYN, replies with SYNACK segment, server allocates buffers, specifies server initial sequence number&lt;/li&gt;
&lt;li&gt;client receives SYNACK, replies with ACK segment, which may contain data
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca205d6c7de8.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Closing a connection
&lt;ol&gt;
&lt;li&gt;client end system send TCP FIN control segment to server&lt;/li&gt;
&lt;li&gt;server receives FIN, replies with ACK, closes connection, sends FIN&lt;/li&gt;
&lt;li&gt;client receives FIN, replies with ACK, enters &amp;ldquo;timed wait&amp;rdquo; will respond with ACK to received FINs&lt;/li&gt;
&lt;li&gt;server, receives ACK, connection closed
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca205f6dfd5c.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Figure
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca206338a961.png&#34; width=&#34;400px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca2066d19100.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tcp-congestion-control&#34;&gt;TCP congestion control&lt;/h3&gt;
&lt;h4 id=&#34;principles-of-congestion-control&#34;&gt;Principles of Congestion Control&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;too many sources sending too much data too fast for network to handle&lt;/li&gt;
&lt;li&gt;manifestations: lost packets, long delays&lt;/li&gt;
&lt;li&gt;a top-10 problem&lt;/li&gt;
&lt;li&gt;Cause/costs of congestion: scenario 1, two connections sharing a single hop with infinite buffers, maximum achievable throughput, large delays when congested
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca20729465a5.png&#34; width=&#34;450px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca2079000572.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;scenario 2, one router, finite buffers, and retransmission
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca207dfa7b04.png&#34; width=&#34;400px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca2083286448.png&#34; width=&#34;500px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;always: $\lambda_{in} = \lambda_{out}$&lt;/li&gt;
&lt;li&gt;&amp;ldquo;perfect&amp;rdquo; retransmission only when loss: $\lambda_{in}&amp;rsquo; = \lambda_{out}$&lt;/li&gt;
&lt;li&gt;retransmission of delayed (not lost) packet makes $\lambda_{in}&amp;lsquo;$ larger (than perfect case) for same $\lambda_{out}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scenario 3, four senders, multi-hop paths, timeout/retransmit
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca2092db9321.png&#34; width=&#34;400px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca209691eb47.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;even more crowded&lt;/li&gt;
&lt;li&gt;when packet dropped, any upstream transmission capacity used for that packet was wasted (之前走过的路径全部浪费掉)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;approaches-towards-congestion-control&#34;&gt;Approaches towards congestion control&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;End-end congestion control
&lt;ul&gt;
&lt;li&gt;no explicit feedback from network&lt;/li&gt;
&lt;li&gt;congestion inferred from end-system observed loss, delay&lt;/li&gt;
&lt;li&gt;approach taken by TCP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Network-assisted congestion control:
&lt;ul&gt;
&lt;li&gt;routers provide feedback to end system&lt;/li&gt;
&lt;li&gt;single bit indicating congestion (SNA, TCP/IP ECN, ATM)&lt;/li&gt;
&lt;li&gt;explicit rate sender should send&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CASE: ATM (ABR congestion control)
&lt;ul&gt;
&lt;li&gt;ABR: available bit rate, elastic service
&lt;ul&gt;
&lt;li&gt;if sender&amp;rsquo;s path &amp;ldquo;underloaded&amp;rdquo;, sender should use available bandwidth&lt;/li&gt;
&lt;li&gt;if sender&amp;rsquo;s path congested: sender throttled to minimum guaranteed rate&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RM (resource management) cells:
&lt;ul&gt;
&lt;li&gt;sent by sender, interspersed with data cells&lt;/li&gt;
&lt;li&gt;bit in RM cell set by switches: NI bit: no increase in rate; CI bit: congestion indication&lt;/li&gt;
&lt;li&gt;RM cells returned to sender by receiver, with bits intact
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca20b6a0066b.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;EFCI bit in data cells: set to 1 in congested switch, if data cell preceding RM cell has EFCI set, receiver sets CI bit in returned RM cell&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;tcp-congestion-control-1&#34;&gt;TCP congestion control&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Congestion window(cwnd): imposes a constraint on the rate at which a TCP sender can send traffic into the network,
LastByteSent – LastByteAcked &amp;lt;= cwnd&lt;/li&gt;
&lt;li&gt;MSS: maximum segment size: the maximum amount of data that can be grabbed and placed in a segment&lt;/li&gt;
&lt;li&gt;roughly rate = CongWin / RTT (Bytes/sec)&lt;/li&gt;
&lt;li&gt;How does sender perceive congestion?
&lt;ul&gt;
&lt;li&gt;loss event = timeout or 3 duplicate ACKs&lt;/li&gt;
&lt;li&gt;TCP sender reduces rate (cwnd) after loss event&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;three mechanisms:
&lt;ul&gt;
&lt;li&gt;AIMD&lt;/li&gt;
&lt;li&gt;slow start&lt;/li&gt;
&lt;li&gt;conservative after timeout events&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;aimd&#34;&gt;AIMD&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Approach: increase transmission rate (window size), probing for usable bandwidth, until loss occurs&lt;/li&gt;
&lt;li&gt;Additive increase: increase CongWin by 1 MSS every RTT until loss detected&lt;/li&gt;
&lt;li&gt;multiplicative decrease: cut CongWin in half after loss
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca20c3c5c2a0.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;slow-start&#34;&gt;Slow start&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;when connections begins, CongWIn = 1MSS&lt;/li&gt;
&lt;li&gt;available bandwidth may be &amp;raquo; MSS/RTT&lt;/li&gt;
&lt;li&gt;when connection begins, increase rate exponentially fast until first loss event
&lt;ul&gt;
&lt;li&gt;double congwin every RTT&lt;/li&gt;
&lt;li&gt;done by incrementing congwin for every ACK received
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca20eb32840b.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;inferring loss
&lt;ul&gt;
&lt;li&gt;After 3 duplicate ACKs, cwnd is cut in half, window the grows linearly&lt;/li&gt;
&lt;li&gt;after timeout event, cwnd set to 1 MSS, window then grows exponentially to a threshold, then grows linearly
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca20f8ca48d5.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;tcp-summary&#34;&gt;TCP Summary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;TCP throughput: Let W be the window size when loss occurs, when window is W, throughput is W/RTT, just after loss, windows drops to W/2, throughput is W/(2RTT), average throughput 0.75W/RTT&lt;/li&gt;
&lt;li&gt;TCP future: TCP over &amp;ldquo;long, fat pipes&amp;rdquo;
1500 byte segments, 100ms RTT, want 10Gbps throughputs, requires window size W = 83.333 in flight ($W \cdot 1500 \cdot 8 / 100ms = 10Gbps$), throughput in terms of loss rate: $1.22\cdot MSS/(RTT\sqrt{L})$ , $L = 2\cdot 10^{-10}$&lt;/li&gt;
&lt;li&gt;TCP Fairness, if K TCP sessions share same bottleneck link of bandwidth R, each should have average rate of R/K, the TCP is fair
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca2208dd5029.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>目标检测</title>
      <link>/courses/deep_learning/object_detect/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0100</pubDate>
      <guid>/courses/deep_learning/object_detect/</guid>
      <description>&lt;p&gt;目标检测是计算机视觉领域中一个新兴的应用方向，其任务是对输入图像进行分类的同时，检测图像中是否包含某些目标，并对他们准确定位并标识。&lt;/p&gt;
&lt;h2 id=&#34;目标定位&#34;&gt;目标定位&lt;/h2&gt;
&lt;p&gt;定位分类问题不仅要求判断出图片中物体的种类，还要在图片中标记出它的具体位置，用**边框（Bounding Box，或者称包围盒）**把物体圈起来。一般来说，定位分类问题通常只有一个较大的对象位于图片中间位置；而在目标检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。&lt;/p&gt;
&lt;p&gt;为了定位图片中汽车的位置，可以让神经网络多输出 4 个数字，标记为 $b_x$、$b_y$、$b_h$、$b_w$。将图片左上角标记为 (0, 0)，右下角标记为 (1, 1)，则有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;红色方框的中心点：($b_x$，$b_y$)&lt;/li&gt;
&lt;li&gt;边界框的高度：$b_h$&lt;/li&gt;
&lt;li&gt;边界框的宽度：$b_w$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，训练集不仅包含对象分类标签，还包含表示边界框的四个数字。定义目标标签 Y 如下：&lt;/p&gt;
&lt;p&gt;$$\left[\begin{matrix}P_c\\ b_x\\ b_y\\ b_h\\ b_w\\ c_1\\ c_2\\ c_3\end{matrix}\right]$$&lt;/p&gt;
&lt;p&gt;则有：&lt;/p&gt;
&lt;p&gt;$$P_c=1, Y = \left[\begin{matrix}1\\ b_x\\ b_y\\ b_h\\ b_w\\ c_1\\ c_2\\ c_3\end{matrix}\right]
$$&lt;/p&gt;
&lt;p&gt;其中，$c_n$表示存在第 $n$个种类的概率；如果 $P_c=0$，表示没有检测到目标，则输出标签后面的 7 个参数都是无效的，可以忽略（用 ? 来表示）。&lt;/p&gt;
&lt;p&gt;$$P_c=0, Y = \left[\begin{matrix}0\\ ?\\ ?\\ ?\\ ?\\ ?\\ ?\\ ?\end{matrix}\right]$$&lt;/p&gt;
&lt;p&gt;损失函数可以表示为 $L(\hat y, y)$，如果使用平方误差形式，对于不同的 $P_c$有不同的损失函数（注意下标 $i$指标签的第 $i$个值）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$P_c=1$，即$y_1=1$：&lt;/p&gt;
&lt;p&gt;$L(\hat y,y)=(\hat y_1-y_1)^2+(\hat y_2-y_2)^2+\cdots+(\hat y_8-y_8)^2$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$P_c=0$，即$y_1=0$：&lt;/p&gt;
&lt;p&gt;$L(\hat y,y)=(\hat y_1-y_1)^2$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;除了使用平方误差，也可以使用逻辑回归损失函数，类标签 $c_1,c_2,c_3$ 也可以通过 softmax 输出。相比较而言，平方误差已经能够取得比较好的效果。&lt;/p&gt;
&lt;h2 id=&#34;特征点检测&#34;&gt;特征点检测&lt;/h2&gt;
&lt;p&gt;神经网络可以像标识目标的中心点位置那样，通过输出图片上的特征点，来实现对目标特征的识别。在标签中，这些特征点以多个二维坐标的形式表示。&lt;/p&gt;
&lt;p&gt;通过检测人脸特征点可以进行情绪分类与判断，或者应用于 AR 领域等等。也可以透过检测姿态特征点来进行人体姿态检测。&lt;/p&gt;
&lt;h2 id=&#34;目标检测&#34;&gt;目标检测&lt;/h2&gt;
&lt;p&gt;想要实现目标检测，可以采用**基于滑动窗口的目标检测（Sliding Windows Detection）**算法。该算法的步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;训练集上搜集相应的各种目标图片和非目标图片，样本图片要求尺寸较小，相应目标居于图片中心位置并基本占据整张图片。&lt;/li&gt;
&lt;li&gt;使用训练集构建 CNN 模型，使得模型有较高的识别率。&lt;/li&gt;
&lt;li&gt;选择大小适宜的窗口与合适的固定步幅，对测试图片进行从左到右、从上倒下的滑动遍历。每个窗口区域使用已经训练好的 CNN 模型进行识别判断。&lt;/li&gt;
&lt;li&gt;可以选择更大的窗口，然后重复第三步的操作。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Sliding-windows-detection.png&#34; alt=&#34;Sliding-windows-detection&#34;&gt;&lt;/p&gt;
&lt;p&gt;滑动窗口目标检测的&lt;strong&gt;优点&lt;/strong&gt;是原理简单，且不需要人为选定目标区域；&lt;strong&gt;缺点&lt;/strong&gt;是需要人为直观设定滑动窗口的大小和步幅。滑动窗口过小或过大，步幅过大均会降低目标检测的正确率。另外，每次滑动都要进行一次 CNN 网络计算，如果滑动窗口和步幅较小，计算成本往往很大。&lt;/p&gt;
&lt;p&gt;所以，滑动窗口目标检测算法虽然简单，但是性能不佳，效率较低。&lt;/p&gt;
&lt;h2 id=&#34;基于卷积的滑动窗口实现&#34;&gt;基于卷积的滑动窗口实现&lt;/h2&gt;
&lt;p&gt;相比从较大图片多次截取，在卷积层上应用滑动窗口目标检测算法可以提高运行速度。所要做的仅是将全连接层换成卷积层，即使用与上一层尺寸一致的滤波器进行卷积运算。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Convolution-implementation-of-sliding-windows.png&#34; alt=&#34;Convolution-implementation-of-sliding-windows&#34;&gt;&lt;/p&gt;
&lt;p&gt;如图，对于 16x16x3 的图片，步长为 2，CNN 网络得到的输出层为 2x2x4。其中，2x2 表示共有 4 个窗口结果。对于更复杂的 28x28x3 的图片，得到的输出层为 8x8x4，共 64 个窗口结果。最大池化层的宽高和步长相等。&lt;/p&gt;
&lt;p&gt;运行速度提高的原理：在滑动窗口的过程中，需要重复进行 CNN 正向计算。因此，不需要将输入图片分割成多个子集，分别执行向前传播，而是将它们作为一张图片输入给卷积网络进行一次 CNN 正向计算。这样，公共区域的计算可以共享，以降低运算成本。&lt;/p&gt;
&lt;p&gt;相关论文：
&lt;a href=&#34;https://arxiv.org/pdf/1312.6229.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sermanet et al., 2014. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;边框预测&#34;&gt;边框预测&lt;/h2&gt;
&lt;p&gt;在上述算法中，边框的位置可能无法完美覆盖目标，或者大小不合适，或者最准确的边框并非正方形，而是长方形。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLO（You Only Look Once）算法&lt;/strong&gt;可以用于得到更精确的边框。YOLO 算法将原始图片划分为 n×n 网格，并将
&lt;a href=&#34;http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Convolutional_Neural_Networks/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B?id=%e7%9b%ae%e6%a0%87%e5%ae%9a%e4%bd%8d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;目标定位&lt;/a&gt;一节中提到的图像分类和目标定位算法，逐一应用在每个网格中，每个网格都有标签如：&lt;/p&gt;
&lt;p&gt;$$\left[\begin{matrix}P_c\\ b_x\\ b_y\\ b_h\\ b_w\\ c_1\\ c_2\\ c_3\end{matrix}\right]$$&lt;/p&gt;
&lt;p&gt;若某个目标的中点落在某个网格，则该网格负责检测该对象。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Bounding-Box-Predictions.png&#34; alt=&#34;Bounding-Box-Predictions&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上面的示例中，如果将输入的图片划分为 3×3 的网格、需要检测的目标有 3 类，则每一网格部分图片的标签会是一个 8 维的列矩阵，最终输出的就是大小为 3×3×8 的结果。要得到这个结果，就要训练一个输入大小为 100×100×3，输出大小为 3×3×8 的 CNN。在实践中，可能使用更为精细的 19×19 网格，则两个目标的中点在同一个网格的概率更小。&lt;/p&gt;
&lt;p&gt;YOLO 算法的优点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;和图像分类和目标定位算法类似，显式输出边框坐标和大小，不会受到滑窗分类器的步长大小限制。&lt;/li&gt;
&lt;li&gt;仍然只进行一次 CNN 正向计算，效率很高，甚至可以达到实时识别。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如何编码边框 $b_x$、$b_y$、$b_h$、$b_w$？YOLO 算法设 $b_x$、$b_y$、$b_h$、$b_w$ 的值是相对于网格长的比例。则 $b_x$、$b_y$ 在 0 到 1 之间，而 $b_h$、$b_w$ 可以大于 1。当然，也有其他参数化的形式，且效果可能更好。这里只是给出一个通用的表示方法。&lt;/p&gt;
&lt;p&gt;相关论文：
&lt;a href=&#34;https://arxiv.org/pdf/1506.02640.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Redmon et al., 2015. You Only Look Once: Unified, Real-Time Object Detection&lt;/a&gt;。Ng 认为该论文较难理解。&lt;/p&gt;
&lt;h2 id=&#34;交互比&#34;&gt;交互比&lt;/h2&gt;
&lt;p&gt;**交互比（IoU, Intersection Over Union）**函数用于评价对象检测算法，它计算预测边框和实际边框交集（I）与并集（U）之比：&lt;/p&gt;
&lt;p&gt;$$IoU = \frac{I}{U}$$&lt;/p&gt;
&lt;p&gt;IoU 的值在 0～1 之间，且越接近 1 表示目标的定位越准确。IoU 大于等于 0.5 时，一般可以认为预测边框是正确的，当然也可以更加严格地要求一个更高的阈值。&lt;/p&gt;
&lt;h2 id=&#34;非极大值抑制&#34;&gt;非极大值抑制&lt;/h2&gt;
&lt;p&gt;YOLO 算法中，可能有很多网格检测到同一目标。**非极大值抑制（Non-max Suppression）**会通过清理检测结果，找到每个目标中点所位于的网格，确保算法对每个目标只检测一次。&lt;/p&gt;
&lt;p&gt;进行非极大值抑制的步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将包含目标中心坐标的可信度 $P_c$ 小于阈值（例如 0.6）的网格丢弃；&lt;/li&gt;
&lt;li&gt;选取拥有最大 $P_c$ 的网格；&lt;/li&gt;
&lt;li&gt;分别计算该网格和其他所有网格的 IoU，将 IoU 超过预设阈值的网格丢弃；&lt;/li&gt;
&lt;li&gt;重复第 2~3 步，直到不存在未处理的网格。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述步骤适用于单类别目标检测。进行多个类别目标检测时，对于每个类别，应该单独做一次非极大值抑制。&lt;/p&gt;
&lt;h2 id=&#34;anchor-boxes&#34;&gt;Anchor Boxes&lt;/h2&gt;
&lt;p&gt;到目前为止，我们讨论的情况都是一个网格只检测一个对象。如果要将算法运用在多目标检测上，需要用到 Anchor Boxes。一个网格的标签中将包含多个 Anchor Box，相当于存在多个用以标识不同目标的边框。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Overlapping-objects.png&#34; alt=&#34;Overlapping-objects&#34;&gt;&lt;/p&gt;
&lt;p&gt;在上图示例中，我们希望同时检测人和汽车。因此，每个网格的的标签中含有两个 Anchor Box。输出的标签结果大小从 3×3×8 变为 3×3×16。若两个 $P_c$ 都大于预设阈值，则说明检测到了两个目标。&lt;/p&gt;
&lt;p&gt;在单目标检测中，图像中的目标被分配给了包含该目标中点的那个网格；引入 Anchor Box 进行多目标检测时，图像中的目标则被分配到了包含该目标中点的那个网格以及具有最高 IoU 值的该网格的 Anchor Box。&lt;/p&gt;
&lt;p&gt;Anchor Boxes 也有局限性，对于同一网格有三个及以上目标，或者两个目标的 Anchor Box 高度重合的情况处理不好。&lt;/p&gt;
&lt;p&gt;Anchor Box 的形状一般通过人工选取。高级一点的方法是用 k-means 将两类对象形状聚类，选择最具代表性的 Anchor Box。&lt;/p&gt;
&lt;p&gt;如果对以上内容不是很理解，在“3.9 YOLO 算法”一节中视频的第 5 分钟，有一个更为直观的示例。&lt;/p&gt;
&lt;h2 id=&#34;r-cnn&#34;&gt;R-CNN&lt;/h2&gt;
&lt;p&gt;前面介绍的滑动窗口目标检测算法对一些明显没有目标的区域也进行了扫描，这降低了算法的运行效率。为了解决这个问题，&lt;strong&gt;R-CNN（Region CNN，带区域的 CNN）&lt;strong&gt;被提出。通过对输入图片运行&lt;/strong&gt;图像分割算法&lt;/strong&gt;，在不同的色块上找出&lt;strong&gt;候选区域（Region Proposal）&lt;/strong&gt;，就只需要在这些区域上运行分类器。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/R-CNN.png&#34; alt=&#34;R-CNN&#34;&gt;&lt;/p&gt;
&lt;p&gt;R-CNN 的缺点是运行速度很慢，所以有一系列后续研究工作改进。例如 Fast R-CNN（与基于卷积的滑动窗口实现相似，但得到候选区域的聚类步骤依然很慢）、Faster R-CNN（使用卷积对图片进行分割）。不过大多数时候还是比 YOLO 算法慢。&lt;/p&gt;
&lt;p&gt;相关论文：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R-CNN：
&lt;a href=&#34;https://arxiv.org/pdf/1311.2524.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Girshik et al., 2013. Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fast R-CNN：
&lt;a href=&#34;https://arxiv.org/pdf/1504.08083.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Girshik, 2015. Fast R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Faster R-CNN：
&lt;a href=&#34;https://arxiv.org/pdf/1506.01497v3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ren et al., 2016. Faster R-CNN: Towards real-time object detection with region proposal networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Network Layer</title>
      <link>/courses/computer_network/network_layer/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/computer_network/network_layer/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;transport segment from sending to receiving host&lt;/li&gt;
&lt;li&gt;on sending side encapsulates segments into datagrams&lt;/li&gt;
&lt;li&gt;on receiving side, delivers segments to transport layer&lt;/li&gt;
&lt;li&gt;network layer protocols in every host, router&lt;/li&gt;
&lt;li&gt;router examines header fields in all IP datagrams passing through it&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;two-keys&#34;&gt;Two keys&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Forwarding: move packets from router&amp;rsquo;s input to appropriate router output
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca48fce906b6.png&#34; width=&#34;450px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Routing: determine route taken by packets from source to destination, routing algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;connection-setup&#34;&gt;Connection setup&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;3rd important function in some network architectures: ATM, frame relay, X.25&lt;/li&gt;
&lt;li&gt;before datagrams flow, two end hosts and intervening routers establish virtual connection (routers get involved)&lt;/li&gt;
&lt;li&gt;network vs transport layer connection service:
&lt;ul&gt;
&lt;li&gt;network: between two hosts(may also involve intervening routers in case of VCs)&lt;/li&gt;
&lt;li&gt;transport: between two processes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;network-service-model&#34;&gt;Network Service model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;example services for individual datagrams: guaranteed delivery, guaranteed delivery with less than 40 milliseconds delay&lt;/li&gt;
&lt;li&gt;example services for a flow of datagrams: in order datagram delivery, guaranteed minimum bandwidth to flow,  restrictions on change in inter-packet spacing (封包和封包的间隔的变化(时间差), 变异度)&lt;/li&gt;
&lt;li&gt;建连线有上面的这些机制&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca492c7dbe88.png&#34; width=&#34;650px&#34;/&gt;
&lt;h2 id=&#34;virtual-circuit-and-datagram-networks&#34;&gt;Virtual circuit and datagram networks&lt;/h2&gt;
&lt;h3 id=&#34;connection-and-connection-less-service&#34;&gt;connection and connection-less service&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;datagram network provides network-layer connectionless service&lt;/li&gt;
&lt;li&gt;VC network provides network-layer connection service&lt;/li&gt;
&lt;li&gt;analogous to transport-layer services, service: host-to-host; no choice: network providers one or the other(只能选一种); implementation: in network core&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;virtual-circuits&#34;&gt;Virtual circuits&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;source-to-dest path behaves like telephone circuit (performance-wise, network actions along source-to-dest path)&lt;/li&gt;
&lt;li&gt;call setup, teardown for each call before data can flow&lt;/li&gt;
&lt;li&gt;each packet carries VC identifier (not destination host address)&lt;/li&gt;
&lt;li&gt;every router on source-dest path maintains &amp;ldquo;state&amp;rdquo; for each passing connection&lt;/li&gt;
&lt;li&gt;link, router resources(bandwidth, buffers) may be allocated to VC (dedicated resources = predictable service)
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4968c668c7.png&#34; width=&#34;400px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4969aecdd0.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;vc-implementation&#34;&gt;VC implementation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;VC consists of
&lt;ul&gt;
&lt;li&gt;path from source to destination&lt;/li&gt;
&lt;li&gt;VC numbers, one number for each link along path&lt;/li&gt;
&lt;li&gt;entries in forwarding tables in routers along path&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;packet belonging to VC carries VC number(rather than destination address)&lt;/li&gt;
&lt;li&gt;VC number can be changed on each link (New VC number comes from forwarding table)
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4981ed5b64.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;datagram-networks&#34;&gt;Datagram Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;no call setup at network layer&lt;/li&gt;
&lt;li&gt;routers: no state about end-to-end connections, no network-level concept of &amp;ldquo;connection&amp;rdquo;&lt;/li&gt;
&lt;li&gt;packets forwarded using destination host address, packets between some source-destination pair may take different paths
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4995db5593.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;forwarding table
&lt;ul&gt;
&lt;li&gt;longest prefix matching
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca49a3b909dd.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;datagram-or-vc-network-why&#34;&gt;Datagram or VC network: Why&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Internet (datagram)
&lt;ul&gt;
&lt;li&gt;data exchange among computers, elastic service, no strict timing request&lt;/li&gt;
&lt;li&gt;smart end systems (computer), can adapt, perform control error recovery, simple inside network, complexity at &amp;ldquo;edge&amp;rdquo;&lt;/li&gt;
&lt;li&gt;many link types, different characteristics, uniform service difficult&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ATM (VC)
&lt;ul&gt;
&lt;li&gt;evolved from telephony&lt;/li&gt;
&lt;li&gt;human conversation, strict timing, reliability requirements, need for guaranteed service&lt;/li&gt;
&lt;li&gt;&amp;ldquo;dumb&amp;rdquo; end systems (telephones, complexity inside network)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;whats-inside-a-router&#34;&gt;What&amp;rsquo;s inside a router&lt;/h2&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4a00b51702.png&#34; width=&#34;400px&#34;/&gt;
- routing algorithms / protocol (RIP, OSPF, BGP)
- forwarding algorithms from incoming to outgoing link
&lt;h3 id=&#34;input-port-functions&#34;&gt;Input Port Functions&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4a134239c6.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;given datagram destination, lookup output port using forwarding table in input port memory&lt;/li&gt;
&lt;li&gt;goal: complete input port processing at line speed&lt;/li&gt;
&lt;li&gt;queuing: if datagrams arrive faster than forwarding rate into switch fabric&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;switching-fabrics&#34;&gt;Switching fabrics&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4a2e556669.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;switching via a bus:
&lt;ul&gt;
&lt;li&gt;datagram from input port memory to output port memory via a shared bus&lt;/li&gt;
&lt;li&gt;bus connection: switching speed limited by bus bandwidth&lt;/li&gt;
&lt;li&gt;32 Gbps bus, Cisco 5600: sufficient speed for access and enterprise routers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;switching through a crossbar&lt;/li&gt;
&lt;li&gt;switching via An interconnection network
&lt;ul&gt;
&lt;li&gt;overcome bus bandwidth limitations&lt;/li&gt;
&lt;li&gt;Banyan networks, other interconnection nets initially developed to connect processors in multiprocessor&lt;/li&gt;
&lt;li&gt;advanced design: fragmenting datagram into fixed length cell&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;output-ports&#34;&gt;Output ports&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4a43acefdc.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;buffering: required when datagrams arrive from fabric faster than the transmission rate&lt;/li&gt;
&lt;li&gt;scheduling discipline: chooses among queued datagrams for transmission
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4a4d9c5b79.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;queueing (delay) and loss due to output port buffer overflow&lt;/li&gt;
&lt;li&gt;How much buffering
&lt;ul&gt;
&lt;li&gt;RFC 3439 rule of thumb: average buffering equal to &amp;ldquo;typical&amp;rdquo; RTT (250 millisecond) times link capacity C (C = 10 Gpbs link, 2.5 Gbit buffer)&lt;/li&gt;
&lt;li&gt;Recent recommendation: with N flows, buffering equal to $RTT*C/(\sqrt(N))$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Input port Queuing problem
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4a677b909c.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;HOL (head-of-the-line) blocking&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ip-internet-protocol&#34;&gt;IP: Internet Protocol&lt;/h2&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4a7516558a.png&#34; width=&#34;400px&#34;/&gt;
&lt;h3 id=&#34;ip-datagram-format&#34;&gt;IP datagram format&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4a861461f7.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;16-bit identifier, flags, 13-bit fragmentation offset: fragmentation/reassembly&lt;/li&gt;
&lt;li&gt;time to live: max number remaining hops (decremented at each router)&lt;/li&gt;
&lt;li&gt;upper layer: upper layer protocol to deliver&lt;/li&gt;
&lt;li&gt;header checksum&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;ip-datagram-fragmentation&#34;&gt;IP Datagram Fragmentation&lt;/h4&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4a906a20cf.png&#34; width=&#34;400px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4a9b07232e.png&#34; width=&#34;600px&#34;/&gt;
&lt;h3 id=&#34;ipv4-addressing&#34;&gt;IPv4 addressing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IP address: 32-bit identifier for host, router interface&lt;/li&gt;
&lt;li&gt;interface: connection between host/router and physical link
&lt;ul&gt;
&lt;li&gt;router&amp;rsquo;s typically have multiple interfaces&lt;/li&gt;
&lt;li&gt;host typically has one interface&lt;/li&gt;
&lt;li&gt;ip address associated with each interface&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;subnets&#34;&gt;Subnets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;subnet part (high order bits)&lt;/li&gt;
&lt;li&gt;host part (low order bits)&lt;/li&gt;
&lt;li&gt;device interfaces with same subnet part of IP address&lt;/li&gt;
&lt;li&gt;can physically reach other without intervening router
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4ace603a45.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;to determine the subnets, detach each interface from its host or router, creating islands of isolated networks, each isolated network is called a subnet&lt;/li&gt;
&lt;li&gt;six subnets
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4ade4aa342.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;CIDR: Classless Inter Domain Routing
&lt;ul&gt;
&lt;li&gt;subnet portion of address of arbitrary length&lt;/li&gt;
&lt;li&gt;address format: a.b.c.d/x where x is number bits in subnet portion of address&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Get IP
&lt;ul&gt;
&lt;li&gt;hard-coded by system admin in a file,  &lt;code&gt;unix: /etc/rc.config&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;DHCP: Dynamic Host Configuration Protocol: dynamically get address from as server, &amp;ldquo;plug-and-play&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Get subnet part of IP address: Gets allocated portion of its provider ISP&amp;rsquo;s address space&lt;/li&gt;
&lt;li&gt;ISP get block of address? ICANN (Internet Corporation for Assigned Names and Numbers)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;dhcp-dynamic-host-configuration-protocol&#34;&gt;DHCP: Dynamic Host Configuration Protocol&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Goal: allow host to dynamically obtain its IP address from network server when it joins network, can renew its lease on address in use, allows reuse of address (only hold address while connected an &amp;ldquo;on&amp;rdquo;), support for mobile users who want to join network (more shortly)&lt;/li&gt;
&lt;li&gt;DHCP overview:
&lt;ul&gt;
&lt;li&gt;host broadcasts &amp;ldquo;DHCP discover&amp;rdquo; message&lt;/li&gt;
&lt;li&gt;DHCP server respond with &amp;ldquo;DHCP offer&amp;rdquo; message&lt;/li&gt;
&lt;li&gt;host requests IP address &amp;ldquo;DHCP request&amp;rdquo; message&lt;/li&gt;
&lt;li&gt;DHCP server sends address &amp;ldquo;DHCP ACK&amp;rdquo; message
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4b5f6e31b0.png&#34; width=&#34;400px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/03/5ca4b605e3bf1.png&#34; width=&#34;450px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;hierarchical-addressing&#34;&gt;Hierarchical addressing&lt;/h4&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/04/5ca5a81531772.png&#34; width=&#34;400px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/04/5ca5a7df7e3e2.png&#34; width=&#34;400px&#34;/&gt;
more specific route
&lt;img src=&#34;https://i.loli.net/2019/04/04/5ca5a8c57f578.png&#34; width=&#34;400px&#34;/&gt;
&lt;h4 id=&#34;network-address-translation-nat&#34;&gt;Network Address Translation (NAT)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Motivation: local network use just one IP address as far as outside world is concerned:
&lt;ul&gt;
&lt;li&gt;range of addresses not needed from ISP: just one IP address for all devices&lt;/li&gt;
&lt;li&gt;can change addresses of devices in local network without notifying outside world&lt;/li&gt;
&lt;li&gt;can change ISP without changing addresses of devices in local network&lt;/li&gt;
&lt;li&gt;devices inside local net not explicitly addressable, visible by outside world (a security plus)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Implementation: NAT router must:
&lt;ul&gt;
&lt;li&gt;outgoing datagrams: replace (source IP address, port number) of every outgoing datagram to (NAT IP address, new port number), remote clients/ servers will respond using (NAT IP address, )&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/04/5ca5ac6370caf.png&#34; width=&#34;500px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;16 bit port-number field (65536)&lt;/li&gt;
&lt;li&gt;NAT is controversial
&lt;ul&gt;
&lt;li&gt;routers should only process up to layer 3 (but involved layer 4)&lt;/li&gt;
&lt;li&gt;violates end-to-end argument, NAT possibility must be taken into account by app designers (eg. P2P applications)&lt;/li&gt;
&lt;li&gt;address shortage should instead be solved by IPv6&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NAT traversal problem
&lt;ul&gt;
&lt;li&gt;client wants to connect to server with address 10.0.0.1, server address 10.0.0.1 local to LAN (client can&amp;rsquo;t use it as destination address), only one externally visible NAT address&lt;/li&gt;
&lt;li&gt;solution 1: statically configure NAT to forward incoming connection requests at given port to server (e.g. 123.76.29.7 port 2500 always forwarded to 10.0.0.1 port 2500)&lt;/li&gt;
&lt;li&gt;solution2: Universal Plug and Play (UPnP),  Internet Gateway Device(IGD) protocol. Allows NATed host to:&lt;/li&gt;
&lt;li&gt;learn public IP address&lt;/li&gt;
&lt;li&gt;add /remove port mappings (with lease times)&lt;/li&gt;
&lt;li&gt;automate static NAT port map configuration&lt;/li&gt;
&lt;li&gt;solution3: relaying (used in Skype)
&lt;ul&gt;
&lt;li&gt;NATed client establishes connection to relay&lt;/li&gt;
&lt;li&gt;external client connects to relay&lt;/li&gt;
&lt;li&gt;relay bridges packets between connections&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;icmp-internet-control-message-protocol&#34;&gt;ICMP (Internet Control Message Protocol)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;used by host &amp;amp; routers to communicate network-level information
&lt;ul&gt;
&lt;li&gt;error reporting&lt;/li&gt;
&lt;li&gt;unreachable host, network, port, protocol&lt;/li&gt;
&lt;li&gt;echo request/reply (used by ping)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;network-layer above &amp;ldquo;IP&amp;rdquo;, ICMP messages carried in IP datagrams&lt;/li&gt;
&lt;li&gt;ICMP message: type, code plus first 8 bytes of IP datagram causing error&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;traceroute-and-icmp&#34;&gt;Traceroute and ICMP&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;source sends series of UDP segment to destination
&lt;ul&gt;
&lt;li&gt;first has TTL=1&lt;/li&gt;
&lt;li&gt;second has TTL=2, etc&lt;/li&gt;
&lt;li&gt;unlikely port number&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;when nth datagram arrives to nth router
&lt;ul&gt;
&lt;li&gt;router discards datagram&lt;/li&gt;
&lt;li&gt;and sends to source an ICMP message (type 11, code 0)&lt;/li&gt;
&lt;li&gt;Message includes name of routers &amp;amp; IP address&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;when ICMP message arrives, source calculates RTT&lt;/li&gt;
&lt;li&gt;traceroute does this 3 times&lt;/li&gt;
&lt;li&gt;Stoping criterion
&lt;ul&gt;
&lt;li&gt;UDP segment eventually arrives at destination host&lt;/li&gt;
&lt;li&gt;destination returns ICMP &amp;ldquo;port unreachable&amp;rdquo; packet (type 3, code 3)&lt;/li&gt;
&lt;li&gt;when source gets this ICMP, stops&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ipv6&#34;&gt;IPv6&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Initial motivation: 32-bit address space soon to be completely allocated&lt;/li&gt;
&lt;li&gt;additional motivation:
&lt;ul&gt;
&lt;li&gt;header format helps speed processing/forwarding
&lt;ul&gt;
&lt;li&gt;header changes to facilitate QoS&lt;/li&gt;
&lt;li&gt;IPv6 datagram format: fixed-length 40 byte header, no fragmentation allowed
&lt;img src=&#34;https://i.loli.net/2019/04/04/5ca5e8da43dfe.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;priority: identify priority among datagrams in flow&lt;/li&gt;
&lt;li&gt;flow label: identify datagrams in same &amp;ldquo;flow&amp;rdquo;&lt;/li&gt;
&lt;li&gt;next header: identify upper layer protocol for data&lt;/li&gt;
&lt;li&gt;checksum: removed entirely to reduce processing time at each step&lt;/li&gt;
&lt;li&gt;options: allowed, but outside of header, indicated by &amp;ldquo;Next Header&amp;rdquo; field&lt;/li&gt;
&lt;li&gt;ICMPv6: new version of ICMP, additional message types (&amp;ldquo;packet too big&amp;rdquo;), multicast group management functions&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;transition-from-ipv4-to-ipv6&#34;&gt;Transition From IPv4 to IPv6&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;not all routers can be upgraded simultaneous, no &amp;ldquo;flag days&amp;rdquo;&lt;/li&gt;
&lt;li&gt;operate mixed IPv4 and IPv6 routers: &lt;strong&gt;Tunneling&lt;/strong&gt;, IPv6 carried as payload in IPv4 datagram among IPv4 routers
&lt;img src=&#34;https://i.loli.net/2019/04/04/5ca5eacac3bed.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;routing-algorithm&#34;&gt;Routing Algorithm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Graph abstraction
&lt;img src=&#34;https://i.loli.net/2019/04/04/5ca5ebfacbbcd.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Routing Algorithm: algorithm that finds least-cost path&lt;/li&gt;
&lt;li&gt;Global: all routers have complete topology, link cost info (link-state algorithm)&lt;/li&gt;
&lt;li&gt;Decentralized
&lt;ul&gt;
&lt;li&gt;router knows physically-connected neighbors, link costs to neighbors
&lt;ul&gt;
&lt;li&gt;iterative process of computation, exchange info with neighbors&lt;/li&gt;
&lt;li&gt;&amp;ldquo;distance vector&amp;rdquo; algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;static or dynamic?
&lt;ul&gt;
&lt;li&gt;static: routes change slowly over time&lt;/li&gt;
&lt;li&gt;dynamic: routes change more quickly (periodic update, in response to link cost changes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;link-state-routing-algorithm&#34;&gt;Link-State Routing Algorithm&lt;/h3&gt;
&lt;p&gt;Dijkstra&amp;rsquo;s algorithm (See Algorithm)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Oscillations with congestion-sensitive routing (link cost = amount of carried traffic)
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca8185104528.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-distance-vector-dv-routing-algorithm&#34;&gt;The Distance-Vector (DV) Routing Algorithm&lt;/h3&gt;
&lt;p&gt;Distance Vector Algorithm (see Algorithm)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Node x maintains distance vector&lt;/li&gt;
&lt;li&gt;Node x also maintains its neighbors&amp;rsquo; distance vectors&lt;/li&gt;
&lt;li&gt;Bellman-Ford equation (dynamic programming)
$d_x(y) = min_v (c(x,v) + d_v(y))$ where min is taken over all neighbors v of x
&lt;img src=&#34;https://i.loli.net/2019/04/04/5ca5f6a51904f.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;good news travels fast, bad news travels slow
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca8155a2ec9c.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;Figure b needs 44 iterations before algorithm stabilizes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Poisoned reverse
&lt;ul&gt;
&lt;li&gt;If Z routes through Y to get to X: Z tells Y its (Z&amp;rsquo;s) distance to X is infinite (So y won&amp;rsquo;t route to X via Z)&lt;/li&gt;
&lt;li&gt;loops involving three or more nodes (rather than simply two immediately neighboring nodes) will not be detected by the poisoned reverse technique&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comparison-of-ls-and-dv&#34;&gt;Comparison of LS and DV&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Message complexity
&lt;ul&gt;
&lt;li&gt;LS: with n nodes, E links O(nE) message send&lt;/li&gt;
&lt;li&gt;DV: exchange between neighbors only, convergence time varies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Speed of Convergence
&lt;ul&gt;
&lt;li&gt;LS: $O(n^2)$ algorithm requires $O(nE)$ messages, may have oscillations&lt;/li&gt;
&lt;li&gt;DV: convergence time varies, may be routing loops, count-to-infinity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Robustness: what happens if router malfunctions?
&lt;ul&gt;
&lt;li&gt;LS: node can advertise incorrect link cost, each node computes only its own table&lt;/li&gt;
&lt;li&gt;DV node can advertise incorrect path cost, each node&amp;rsquo;s table used by others, error propagate through network&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hierarchical-routing&#34;&gt;Hierarchical routing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;scale: with 200 million destinations:
&lt;ul&gt;
&lt;li&gt;can&amp;rsquo;t store all destination&amp;rsquo;s in routing tables&lt;/li&gt;
&lt;li&gt;routing table exchange would swamp links&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;administrative autonomy
&lt;ul&gt;
&lt;li&gt;internet = network of networks&lt;/li&gt;
&lt;li&gt;each network admin may want to control routing in its own network&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;aggregate routers into regions &amp;ldquo;autonomous systems (AS)&amp;rdquo;&lt;/li&gt;
&lt;li&gt;routers in same AS run same routing protocol
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;intra-AS&amp;rdquo; routing protocol&lt;/li&gt;
&lt;li&gt;routers in different AS can run different intra-AS running protocol&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gateway router : direct link to router in another AS
&lt;img src=&#34;https://i.loli.net/2019/04/04/5ca5fc1c11b2e.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Inter-AS tasks, suppose router in AS1 receives datagram destined outside of AS1: router should forward packet to gateway router, but which one?
&lt;ul&gt;
&lt;li&gt;AS1 must learn which destination are reachable through AS2, which through AS3
&lt;ul&gt;
&lt;li&gt;propagate this reachability info to all routers in AS1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;hot-potato-routing&#34;&gt;Hot-potato routing&lt;/h4&gt;
&lt;p&gt;Suppose AS1 learns from inter-AS protocol that subnet X is reachable from AS3 and from AS2, to configure forwarding table, router 1d must determine towards which gateway it should forward packets for destination x.(hot-potato routing- send packet towards closest of two routers)
&lt;img src=&#34;https://i.loli.net/2019/04/04/5ca5ff85a0ec8.png&#34; width=&#34;600px&#34;/&gt;&lt;/p&gt;
&lt;h2 id=&#34;routing-in-the-internet&#34;&gt;Routing in the Internet&lt;/h2&gt;
&lt;p&gt;Interior Gateway Protocols (IGP)&lt;/p&gt;
&lt;h3 id=&#34;rip-routing-information-protocol&#34;&gt;RIP (Routing Information Protocol)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;distance vector algorithm&lt;/li&gt;
&lt;li&gt;included in BSD-UNIX distribution in 1982&lt;/li&gt;
&lt;li&gt;distance metric: number of hops (max = 15hops)
&lt;img src=&#34;https://i.loli.net/2019/04/04/5ca600bf47feb.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;exchanged among neighbors every 30 seconds via Response Message (also called advertisement)&lt;/li&gt;
&lt;li&gt;each advertisement: list of up to 25 destination subnets within AS&lt;/li&gt;
&lt;li&gt;Link Failure and Recovery: if no advertisement heard after 180 seconds $\rightarrow$ neighbor/link declared dead
&lt;ul&gt;
&lt;li&gt;routes via neighbor invalidated&lt;/li&gt;
&lt;li&gt;new advertisements send to neighbors&lt;/li&gt;
&lt;li&gt;neighbors in turn send out new advertisements&lt;/li&gt;
&lt;li&gt;link failure info quickly (?) propagates to entire network&lt;/li&gt;
&lt;li&gt;poison reverse: used to prevent ping-pong loops (infinite distance = 16 loops)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RIP table processing
&lt;ul&gt;
&lt;li&gt;RIP routing tables managed by application-level process called route-d (daemon) (Send distance vector)&lt;/li&gt;
&lt;li&gt;advertisements send in UDP packets, periodically repeated
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca81b33efa24.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ospf-open-shortest-path-first&#34;&gt;OSPF (Open Shortest Path First)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;open: publicly available&lt;/li&gt;
&lt;li&gt;uses Link State algorithm:
&lt;ul&gt;
&lt;li&gt;LS packet dissemination&lt;/li&gt;
&lt;li&gt;topology map at each node&lt;/li&gt;
&lt;li&gt;route computation using Dijkstra&amp;rsquo;s algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OSPF advertisement carries one entry per neighbor router&lt;/li&gt;
&lt;li&gt;advertisements disseminated to entire AS (via flooding), carried in OSPF messages directly over IP (rather than TCP or UDP)&lt;/li&gt;
&lt;li&gt;advanced features
&lt;ul&gt;
&lt;li&gt;security: all OSPF messages authenticated(to prevent malicious intrusion)&lt;/li&gt;
&lt;li&gt;multiple same-cost paths allowed (only one path in RIP)&lt;/li&gt;
&lt;li&gt;For each link, multiple cost metrics for different TOS (type of service) (e.g. satellite link cost set &amp;ldquo;low: for best effort: high for real time)&lt;/li&gt;
&lt;li&gt;integrated uni- and multicast support: Multicast OSPF (MOSPF) uses same topology data base as OSPF&lt;/li&gt;
&lt;li&gt;hierarchical OSPF in large domains&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;bgp-border-gateway-protocol-inter-as-routing&#34;&gt;BGP (Border Gateway Protocol) Inter-AS Routing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;the de facto standard&lt;/li&gt;
&lt;li&gt;BGP provides each AS a means to:
&lt;ul&gt;
&lt;li&gt;Obtain subnet reachability information from neighboring ASs&lt;/li&gt;
&lt;li&gt;Propagate reachability information to all AS-internal routers&lt;/li&gt;
&lt;li&gt;Determine &amp;ldquo;good&amp;rdquo; routers to subnets based on reachability information and policy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;allows subnet to advertise its existence to rest of Internet&lt;/li&gt;
&lt;li&gt;Pairs of routers (BGP peers) exchange routing info over semi-permanent TCP connections: BGP sessions (BGP sessions need not correspond to physical links)
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca81da7a1e5d.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;When As2 advertise to a prefix to AS1:
&lt;ul&gt;
&lt;li&gt;AS2 promises it will forward datagrams towards that prefix&lt;/li&gt;
&lt;li&gt;AS2 can aggregate prefixes in its advertisement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Distributing reachability information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using eBGP(external) session between 3a and 1c, AS3 sends prefix reachability info to AS1
&lt;ul&gt;
&lt;li&gt;1c can then use iBGP do distribute new prefix info to all routers in AS1&lt;/li&gt;
&lt;li&gt;1b can then re-advertise new reachability info to AS2 over 1b-to-2a eBGP session&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;when router learns of new prefix, it creates entry for prefix in its forwarding table&lt;/li&gt;
&lt;li&gt;advertised prefix includes BGP attributes: prefix + &amp;ldquo;attributes&amp;rdquo; = &amp;ldquo;route&amp;rdquo;&lt;/li&gt;
&lt;li&gt;two important attributes
&lt;ol&gt;
&lt;li&gt;AS-PATH: contains ASs through which prefix advertisement has passed&lt;/li&gt;
&lt;li&gt;NEXT-HOP: indicates specific internal-AS router to next-hop AS(may be multiple links from current AS to next-hop-AS)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;when gateway router receives route advertisement, use &lt;strong&gt;import policy&lt;/strong&gt; to accept/decline&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;bgp-route-selection&#34;&gt;BGP route selection&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;router may learn about more than 1 route to some prefix, router must select route&lt;/li&gt;
&lt;li&gt;elimination rules:
&lt;ol&gt;
&lt;li&gt;local preference value attribute: policy decision&lt;/li&gt;
&lt;li&gt;shortest AS-PATH&lt;/li&gt;
&lt;li&gt;closest NEXT-HOP router: hot potato routing&lt;/li&gt;
&lt;li&gt;additional criteria&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;bgp-messages-exchanged-using-tcp&#34;&gt;BGP messages exchanged using TCP&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;OPEN:  open TCP connections to peer and authenticates sender&lt;/li&gt;
&lt;li&gt;UPDATE: advertises new path&lt;/li&gt;
&lt;li&gt;KEEPALIVE: keeps connection alive in absence of UPDATES&lt;/li&gt;
&lt;li&gt;NOTIFICATION: reports error in previous message also used to close connection&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;bgp-routing-policy&#34;&gt;BGP routing policy&lt;/h5&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca820d85e63b.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;A, B, C are provider networks&lt;/li&gt;
&lt;li&gt;X, W, Y are customer (of provider networks)&lt;/li&gt;
&lt;li&gt;X is dual-homed: attached  to two networks
&lt;ul&gt;
&lt;li&gt;X dose not want to route from B via X to C, so X will not advertise to B a route to C&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A advertises path AW to B&lt;/li&gt;
&lt;li&gt;B advertises path BAW to X&lt;/li&gt;
&lt;li&gt;should B advertise path BAW to C?
&lt;ul&gt;
&lt;li&gt;No, B gets no &amp;ldquo;revenue&amp;rdquo; for routing CBAW since neither W nor C are B&amp;rsquo;s customers&lt;/li&gt;
&lt;li&gt;B wants to force C route to w via A&lt;/li&gt;
&lt;li&gt;B wants to route only to/from its customers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;different-intra--and-inter-as-routing&#34;&gt;Different Intra- and Inter-AS routing&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Policy
&lt;ul&gt;
&lt;li&gt;Inter-AS: admin wants control over how its traffic routed, who routes through its net&lt;/li&gt;
&lt;li&gt;Intra-AS: single admin, no policy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scale: hierarchical routing saves table size, reduced update traffic&lt;/li&gt;
&lt;li&gt;Performance:
&lt;ul&gt;
&lt;li&gt;Intra-AS: can focus on performance&lt;/li&gt;
&lt;li&gt;Inter-AS: policy may dominate over performance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;broadcast-and-multicast-routing&#34;&gt;Broadcast and multicast routing&lt;/h2&gt;
&lt;h3 id=&#34;broadcast-routing&#34;&gt;Broadcast Routing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;deliver packets from source to all other nodes&lt;/li&gt;
&lt;li&gt;source duplication is inefficient
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca822f9dc436.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;In-network duplication:
&lt;ul&gt;
&lt;li&gt;flooding: when node receives broadcast packet, sends copy to all neighbors (problem: cycles &amp;amp; broadcast storm)&lt;/li&gt;
&lt;li&gt;controlled flooding: node only broadcast packet if it hasn&amp;rsquo;t broadcast same packet before, Node keeps track of packet ids already broadcasted; Reverse Path Forwarding(RPF): only forward packet if it arrived on shortest path between node and source&lt;/li&gt;
&lt;li&gt;spanning tree: no redundant packet received by any node&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;spanning-tree&#34;&gt;Spanning Tree&lt;/h4&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca82424dea45.png&#34; width=&#34;400px&#34;/&gt;
- First construct a spanning tree
- Nodes forward copies only along spanning tree
- Creation:
  - center node
  - each node send unicast join message to center node
  - message forwarded until it arrives at a node already belonging to spanning tree
  &lt;img src=&#34;https://i.loli.net/2019/04/06/5ca824d3bd778.png&#34; width=&#34;400px&#34;/&gt;
&lt;h3 id=&#34;multicast-routing-problem-statement&#34;&gt;Multicast Routing: Problem statement&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;find a tree (or trees) connecting routers having local multicast group members
&lt;ul&gt;
&lt;li&gt;tree: not all paths between routers used&lt;/li&gt;
&lt;li&gt;source-based: different tree from each sender to receivers&lt;/li&gt;
&lt;li&gt;shared-tree: same tree used by all group members&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;sourced-based tree: one tree per source, shorted path trees, reverse path forwarding&lt;/li&gt;
&lt;li&gt;group-shared tree: group uses one tree (minimal spanning tree, center-based trees)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;shortest-path-tree&#34;&gt;Shortest Path tree&lt;/h4&gt;
&lt;p&gt;Dijkstra&amp;rsquo;s algorithm&lt;/p&gt;
&lt;h4 id=&#34;reverse-path-forwarding&#34;&gt;Reverse path forwarding&lt;/h4&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca827388a520.png&#34; width=&#34;400px&#34;/&gt;
&lt;p&gt;Pruning: forwarding tree contains subtrees with no multicast group members: no need to forward datagrams down subtree, &amp;ldquo;prune&amp;rdquo; message send upstream by router with no downstream group members.&lt;/p&gt;
&lt;h4 id=&#34;shared-tree-steiner-tree&#34;&gt;Shared-Tree: Steiner Tree&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;minimum cost tree connecting all routers with attached group members&lt;/li&gt;
&lt;li&gt;problem is NP-complete&lt;/li&gt;
&lt;li&gt;excellent heuristics exists (近似解)&lt;/li&gt;
&lt;li&gt;not used in practice
&lt;ol&gt;
&lt;li&gt;computational complexity&lt;/li&gt;
&lt;li&gt;information about entire network needed&lt;/li&gt;
&lt;li&gt;monolithic(庞大): rerun whenever a router needs to join/leave&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;center-based-trees&#34;&gt;Center-based trees&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;single delivery tree shared by all&lt;/li&gt;
&lt;li&gt;one router identified as &amp;ldquo;center&amp;rdquo; of tree&lt;/li&gt;
&lt;li&gt;to join:
&lt;ul&gt;
&lt;li&gt;edge router sends unicast join-message addressed to center router&lt;/li&gt;
&lt;li&gt;join-message &amp;ldquo;processed&amp;rdquo; by intermediate routers and forwarded towards center&lt;/li&gt;
&lt;li&gt;join-message either hits existing tree branch for this center, or arrives at center&lt;/li&gt;
&lt;li&gt;path taken by join-message becomes new branch of tree for this router&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;internet-multicasting-routing-dvmrp&#34;&gt;Internet Multicasting Routing: DVMRP&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DVMRP: distance vector multicast routing protocol, RFC1075&lt;/li&gt;
&lt;li&gt;flood and prune: reverse path forwarding, source-based tree&lt;/li&gt;
&lt;li&gt;soft state: DVMRP router periodically (1min) &amp;ldquo;forgets&amp;rdquo; branches are pruned
&lt;ul&gt;
&lt;li&gt;multicast data again flows down unpruned branch&lt;/li&gt;
&lt;li&gt;downstream router: re-prune or else continue to receive data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;routers can quickly regraft to tree, following IGMP (internet Group Management protocol) join at leaf&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;protocol-independent-multicast-rip&#34;&gt;Protocol Independent Multicast: RIP&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;not dependent on any specific underlying unicast routing algorithm (works with all)&lt;/li&gt;
&lt;li&gt;Dense: group members densely packed, in &amp;ldquo;close&amp;rdquo; proximity, bandwidth more plentiful
&lt;ul&gt;
&lt;li&gt;group membership by routers assumed until routers explicitly prune&lt;/li&gt;
&lt;li&gt;data-driven construction on multicast tree (e.g. RPF1)&lt;/li&gt;
&lt;li&gt;bandwidth and non-group-router processing profligate (浪费)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sparse: number of networks with group members small w.r.t. number of interconnected networks, group members &amp;ldquo;widely dispersed&amp;rdquo;, bandwidth not plentiful
&lt;ul&gt;
&lt;li&gt;no membership until routers explicitly join&lt;/li&gt;
&lt;li&gt;receiver-driven construction of multicast tree (e.g. center-based)&lt;/li&gt;
&lt;li&gt;bandwidth and non-group-router processing conservative&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Virtual Memory</title>
      <link>/courses/operating_system/virtual_memory/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/virtual_memory/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We don&amp;rsquo;t want to run a program that is entirely in memory
&lt;ol&gt;
&lt;li&gt;Many code for handling unusual errors or conditions&lt;/li&gt;
&lt;li&gt;Certain program routines or features are rarely used&lt;/li&gt;
&lt;li&gt;The same library code used by many programs&lt;/li&gt;
&lt;li&gt;Arrays, lists and tables allocated but not used&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Virtual memory — separation of user logical memory from physical memory
&lt;ul&gt;
&lt;li&gt;To run a extremely &lt;strong&gt;large process&lt;/strong&gt;, logical address space can be much larger than the physical address space&lt;/li&gt;
&lt;li&gt;To increase &lt;strong&gt;CPU/resources utilization&lt;/strong&gt; (higher degree of multiprogramming degree)&lt;/li&gt;
&lt;li&gt;To &lt;strong&gt;simplify programming&lt;/strong&gt; tasks (Free programmer from memory limitation)&lt;/li&gt;
&lt;li&gt;To run programs &lt;strong&gt;faster&lt;/strong&gt; (less I/O would be needed to load or swap)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Virtual memory can be implemented by
&lt;ul&gt;
&lt;li&gt;Demand paging&lt;/li&gt;
&lt;li&gt;Demand segmentation: more complicated due to variable size
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8de51380ac7.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;demand-paging&#34;&gt;Demand Paging&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A program rather than the whole process is brought into memory only when it is needed
&lt;ul&gt;
&lt;li&gt;Less I/O needed $\rightarrow$ Faster response&lt;/li&gt;
&lt;li&gt;Less memory needed $\rightarrow$ More users&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Page is needed when there is a reference to the page
&lt;ul&gt;
&lt;li&gt;Invalid reference $\rightarrow$ abort&lt;/li&gt;
&lt;li&gt;Not-in-memory $\rightarrow$ bring to memory via paging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pure demand paging
&lt;ul&gt;
&lt;li&gt;Start a process with no page&lt;/li&gt;
&lt;li&gt;Never bring a page into memory until it is required&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A swapper (midterm scheduler) manipulates the entire process, whereas a pager is concerned with individual page of a process&lt;/li&gt;
&lt;li&gt;Hardware support
&lt;ul&gt;
&lt;li&gt;Page table : a valid-invalid bit (1 $\rightarrow$ page in memory, $0 \rightarrow$ page no in the memory)&lt;/li&gt;
&lt;li&gt;Secondary memory (swap space, backing store), usually, a high-speed disk (swap device) is use
&lt;img src=&#34;https://i.loli.net/2019/04/01/5ca1897edd6c1.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;page-fault&#34;&gt;Page Fault&lt;/h3&gt;
&lt;p&gt;First reference to a page will trap to OS (page fault trap)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;OS looks at the internal table (PCB) to decide
&lt;ul&gt;
&lt;li&gt;Invalid reference $\rightarrow$ abort&lt;/li&gt;
&lt;li&gt;Just not in memory $\rightarrow$ continue&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Get an empty frame&lt;/li&gt;
&lt;li&gt;Swap the page from disk (swap) space into the frame&lt;/li&gt;
&lt;li&gt;Reset page table, invalid-valid bit = 1&lt;/li&gt;
&lt;li&gt;Restart instruction
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8dea0ea45c7.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;page-replacement&#34;&gt;Page replacement&lt;/h3&gt;
&lt;p&gt;If there is no free frame when a page fault occurs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;swap a frame to backing store&lt;/li&gt;
&lt;li&gt;swap a page from backing store into the frame&lt;/li&gt;
&lt;li&gt;different page &lt;strong&gt;replacement algorithms&lt;/strong&gt; pick different frames for replacement&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;demand-paging-performance&#34;&gt;Demand Paging Performance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Effective Access Time (EAT): $(1-p) \times ma + p \times PFT$
&lt;ul&gt;
&lt;li&gt;P: page fault rate, ma: memory, access time, PFT: page fault time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example ma=200ns, PFT=8ms
&lt;ul&gt;
&lt;li&gt;EAT = 200ns + 7999800 ns $\times$ p&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Access time is proportional to the page fault rate
&lt;ul&gt;
&lt;li&gt;If one access out of 1000 causes a page fault, then EAT = 8.2 ms $\rightarrow$ slowdown by a factor of 40&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Programs tend to have locality of reference&lt;/li&gt;
&lt;li&gt;Locality means program often accesses memory addresses that are close together
&lt;ul&gt;
&lt;li&gt;A single page fault can bring in 4KB memory content&lt;/li&gt;
&lt;li&gt;Greatly reduce the occurrence of page fault&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Major components of page fault time (about 8ms)
&lt;ol&gt;
&lt;li&gt;serve the page-fault interrupt&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;read in the page from disk&lt;/strong&gt; (most expensive)&lt;/li&gt;
&lt;li&gt;Restart the process&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;process-creation-and-virtual-memory&#34;&gt;Process Creation and Virtual memory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Demand paging: only bring in the page containing the first instruction&lt;/li&gt;
&lt;li&gt;Copy-on-Write: the parent and the child process share the same frames initially, and frame-copy when a page is written&lt;/li&gt;
&lt;li&gt;Memory-Mapped File: map a file into the virtual address space to bypass file system calls (e.g. read(), write())&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;copy-on-write&#34;&gt;Copy-on-Write&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;if either process modifies a frame, only then a frame is copied&lt;/li&gt;
&lt;li&gt;COW allows efficient process creation&lt;/li&gt;
&lt;li&gt;Free frames are allocated from a pool of zeroed-out frames, the content of a frame is erased to 0&lt;/li&gt;
&lt;li&gt;Figure: a child process is forked
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8defdbc3767.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;After parent modifies page C
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8df0553a9f2.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;memory-mapped-files&#34;&gt;Memory-Mapped Files&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Approach:
&lt;ul&gt;
&lt;li&gt;MMF allows file I/O to be treated as routine memory access by mapping a disk block to memory frame&lt;/li&gt;
&lt;li&gt;A file  is initially read using demand paging. Subsequent read/writes to/from the file are treated as ordinary memory accesses&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Benefit:
&lt;ul&gt;
&lt;li&gt;Faster file access by using memory access rather than &lt;code&gt;read()&lt;/code&gt; and &lt;code&gt;write()&lt;/code&gt; system calls&lt;/li&gt;
&lt;li&gt;Allows several process to map the SAME file allowing the pages in memory to be SHARED&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Concerns:
&lt;ul&gt;
&lt;li&gt;Security(access control), data lost, more programming efforts
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8df3f01bddd.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;page-replacement-1&#34;&gt;Page replacement&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;when a page fault occurs with on free frame
&lt;ol&gt;
&lt;li&gt;swap out a process,  freeing all its frames&lt;/li&gt;
&lt;li&gt;page replacement, find one not currently used add free it.
Use &lt;strong&gt;dirty bit&lt;/strong&gt; to reduce overhead of page transfers &amp;ndash; only modified pages are written to disk&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Solve two major problems for demand paging
&lt;ul&gt;
&lt;li&gt;Frame-allocation algorithm, determine how many frames to be allocated to a process&lt;/li&gt;
&lt;li&gt;Page-replacement algorithm, select which frame to be replaced&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;page-replacement-algorithms&#34;&gt;Page Replacement Algorithms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Goal: lowest page-fault rate&lt;/li&gt;
&lt;li&gt;Evaluation: running against a string of memory references (reference string) and computing the number of page faults&lt;/li&gt;
&lt;li&gt;Reference String: 1,2,3,4,1,2,5,1,2,3,4,5&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fifo-algorithm&#34;&gt;FIFO algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The oldest page in a FIFO queue is replaced&lt;/li&gt;
&lt;li&gt;3 frames (available memory frames = 3) 9 page faults
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8df74139338.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;FIFO illustrating Belady&amp;rsquo;s Anomaly
More allocated frames doesn&amp;rsquo;t guaranteed less page fault
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8df806c0447.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Figure illustration
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8df86990ad6.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;optimal-belady-algorithm&#34;&gt;Optimal (Belady) Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Replace the page that will not be used for the longest period of time, need future knowledge&lt;/li&gt;
&lt;li&gt;4 frames (6 page faults)&lt;/li&gt;
&lt;li&gt;In practice, we don&amp;rsquo;t have future knowledge, only used for reference and comparison
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e05f397a22.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;lru-algorithm-least-recently-used&#34;&gt;LRU Algorithm (Least Recently Used)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An approximation of optimal algorithm, looking backward, rather than forward&lt;/li&gt;
&lt;li&gt;It replaces the page that has not been used for the longest period of time&lt;/li&gt;
&lt;li&gt;It is often used, and is considered as quite good&lt;/li&gt;
&lt;li&gt;Counter implementation
&lt;ol&gt;
&lt;li&gt;page referenced: time stamp is copied into the counter&lt;/li&gt;
&lt;li&gt;replacement: remove the one with oldest counter, but linear search is required&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Stack implementation
&lt;ol&gt;
&lt;li&gt;page referenced: move to top of the double-linked list&lt;/li&gt;
&lt;li&gt;replacement: remove the page at the bottom
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e078801cb9.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;stack-algorithm&#34;&gt;Stack Algorithm&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;A property of algorithms&lt;/li&gt;
&lt;li&gt;Stack algorithm: the set of pagers in memory for n frames is always a subset of the set of pages that would be in memory with n+1 frames&lt;/li&gt;
&lt;li&gt;Stack algorithms do not suffers from Belady&amp;rsquo;s anomaly&lt;/li&gt;
&lt;li&gt;Both optimal algorithm and LRU algorithm stack algorithm&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;lru-approximation-algorithms&#34;&gt;LRU approximation algorithms&lt;/h3&gt;
&lt;p&gt;Few systems provide sufficient hardware support for the LRU page-replacement&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;additional-reference-bits algorithm&lt;/li&gt;
&lt;li&gt;second-chance algorithm&lt;/li&gt;
&lt;li&gt;enhanced second-chance algorithm&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;counting-algorithms&#34;&gt;Counting Algorithms&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;LFU algorithms (least frequently used)
&lt;ul&gt;
&lt;li&gt;keep a counter for each page&lt;/li&gt;
&lt;li&gt;idea: an actively used page should have a large reference count&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MFU algorithms (most frequently used)
&lt;ul&gt;
&lt;li&gt;idea: the page with the smallest count was probably just brought in and has yet to be used&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both counting algorithms are not common&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;implementation is expensive (overflow)&lt;/li&gt;
&lt;li&gt;do not approximate OPT algorithm very well&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;allocation-of-frames&#34;&gt;Allocation of frames&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;each process needs minimum number of frames&lt;/li&gt;
&lt;li&gt;Fixed allocation
&lt;ul&gt;
&lt;li&gt;Equal allocation &amp;ndash; 100 frames, 5 processes $\rightarrow$ 20 frames/process&lt;/li&gt;
&lt;li&gt;Proportional allocation &amp;ndash; Allocate according to the size of the process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Priority allocation
Using proportional allocation based on priority, instead of size.&lt;/li&gt;
&lt;li&gt;Local allocation: each process select from its own set of allocated frames&lt;/li&gt;
&lt;li&gt;Global allocation : process selects a replacement frame from the set of all frames
&lt;ul&gt;
&lt;li&gt;One process can take away a frame of another process&lt;/li&gt;
&lt;li&gt;e.g. Allow a high-priority process to take frames from a low-priority process&lt;/li&gt;
&lt;li&gt;Good system performance and thus is common used&lt;/li&gt;
&lt;li&gt;A minimum number of frames must be maintained for each process to prevent &lt;strong&gt;trashing&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;trashing&#34;&gt;Trashing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If a process dost not have &amp;ldquo;enough&amp;rdquo; frames to support pages in active page $\rightarrow$ very high paging activity&lt;/li&gt;
&lt;li&gt;A process is trashing if it spending more time paging than executing
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e0d69140ec.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Performance problem caused by trashing
Processes queued for I/O to swap (page fault) $\rightarrow$ low CPU utilization $\rightarrow$ OS increased the degree of multiprogramming $\rightarrow$ new processes take frames from old processes $\rightarrow$ CPU utilization drops even further&lt;/li&gt;
&lt;li&gt;To prevent trashing, must provide enough frames for each process (Working-set model, page-fault frequency)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;working-set-model-实现起来比较繁琐-比较少用&#34;&gt;Working-Set Model (实现起来比较繁琐， 比较少用)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Locality: a set of pages that are actively used together&lt;/li&gt;
&lt;li&gt;Locality model: as a process executes, it moves from locality to locality (program structure, data structure)&lt;/li&gt;
&lt;li&gt;Working-set model
&lt;ol&gt;
&lt;li&gt;working-set window: a parameter $\Delta$&lt;/li&gt;
&lt;li&gt;working set: set of pages in most recent $\Delta$ page reference (an approximation locality)
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e0fedf1e67.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Explanation
&lt;ul&gt;
&lt;li&gt;$WSS_i:$ Working-set size  for process i&lt;/li&gt;
&lt;li&gt;$D = \sum WSS_i:$  Total demand frames&lt;/li&gt;
&lt;li&gt;If $D &amp;gt; m$ (available frames) $\rightarrow$ trashing&lt;/li&gt;
&lt;li&gt;The OS monitors the $WSS_i$ of each process and allocates to the process enough frames: if $D &amp;laquo; m$ , increase degree of MP, if $D &amp;gt; m$, suspend a process.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advantages: prevent thrashing while keeping the degree of multiprogramming as high as possible , optimize CPU utilization&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;page-fault-frequency-scheme&#34;&gt;Page Fault frequency scheme&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;page fault frequency directly measures and controls the page-fault rate to prevent trashing&lt;/li&gt;
&lt;li&gt;Establish upper and lower bounds on the desired page-fault rate of a process&lt;/li&gt;
&lt;li&gt;If page fault rate exceed the upper limit, allocate another frame to the process&lt;/li&gt;
&lt;li&gt;if page fault rate fails below the lower limit, remove a frame from the process
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e249f97b68.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>人脸识别和风格迁移</title>
      <link>/courses/deep_learning/transfer/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0100</pubDate>
      <guid>/courses/deep_learning/transfer/</guid>
      <description>&lt;h2 id=&#34;人脸识别&#34;&gt;人脸识别&lt;/h2&gt;
&lt;p&gt;**人脸验证（Face Verification）&lt;strong&gt;和&lt;/strong&gt;人脸识别（Face Recognition）**的区别：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人脸验证：一般指一个一对一问题，只需要验证输入的人脸图像是否与某个已知的身份信息对应；&lt;/li&gt;
&lt;li&gt;人脸识别：一个更为复杂的一对多问题，需要验证输入的人脸图像是否与多个已知身份信息中的某一个匹配。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一般来说，由于需要匹配的身份信息更多导致错误率增加，人脸识别比人脸验证更难一些。&lt;/p&gt;
&lt;h3 id=&#34;one-shot-学习&#34;&gt;One-Shot 学习&lt;/h3&gt;
&lt;p&gt;人脸识别所面临的一个挑战是要求系统只采集某人的一个面部样本，就能快速准确地识别出这个人，即只用一个训练样本来获得准确的预测结果。这被称为&lt;strong&gt;One-Shot 学习&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;有一种方法是假设数据库中存有 N 个人的身份信息，对于每张输入图像，用 Softmax 输出 N+1 种标签，分别对应每个人以及都不是。然而这种方法的实际效果很差，因为过小的训练集不足以训练出一个稳健的神经网络；并且如果有新的身份信息入库，需要重新训练神经网络，不够灵活。&lt;/p&gt;
&lt;p&gt;因此，我们通过学习一个 Similarity 函数来实现 One-Shot 学习过程。Similarity 函数定义了输入的两幅图像的差异度，其公式如下：&lt;/p&gt;
&lt;p&gt;$$Similarity  = d(img1, img2)$$&lt;/p&gt;
&lt;p&gt;可以设置一个超参数 $τ$ 作为阈值，作为判断两幅图片是否为同一个人的依据。&lt;/p&gt;
&lt;h3 id=&#34;siamese-网络&#34;&gt;Siamese 网络&lt;/h3&gt;
&lt;p&gt;实现 Similarity 函数的一种方式是使用&lt;strong&gt;Siamese 网络&lt;/strong&gt;，它是一种对两个不同输入运行相同的卷积网络，然后对它们的结果进行比较的神经网络。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Siamese.png&#34; alt=&#34;Siamese&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图示例，将图片 $x^{(1)}$、$x^{(2)}$ 分别输入两个相同的卷积网络中，经过全连接层后不再进行 Softmax，而是得到特征向量 $f(x^{(1)})$、$f(x^{(2)})$。这时，Similarity 函数就被定义为两个特征向量之差的 L2 范数：&lt;/p&gt;
&lt;p&gt;$$d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||^2_2$$&lt;/p&gt;
&lt;p&gt;相关论文：
&lt;a href=&#34;http://www.cs.wayne.edu/~mdong/taigman_cvpr14.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taigman et al., 2014, DeepFace closing the gap to human level performance&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;triplet-损失&#34;&gt;Triplet 损失&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Triplet 损失函数&lt;/strong&gt;用于训练出合适的参数，以获得高质量的人脸图像编码。“Triplet”一词来源于训练这个神经网络需要大量包含 Anchor（靶目标）、Positive（正例）、Negative（反例）的图片组，其中 Anchor 和 Positive 需要是同一个人的人脸图像。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Training-set-using-triplet-loss.png&#34; alt=&#34;Training-set-using-triplet-loss&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于这三张图片，应该有：&lt;/p&gt;
&lt;p&gt;$$||f(A) - f(P)||^2_2 + \alpha \le ||f(A) - f(N)||^2_2$$&lt;/p&gt;
&lt;p&gt;其中，$\alpha$ 被称为&lt;strong&gt;间隔（margin）&lt;/strong&gt;，用于确保 $f()$ 不会总是输出零向量（或者一个恒定的值）。&lt;/p&gt;
&lt;p&gt;Triplet 损失函数的定义：&lt;/p&gt;
&lt;p&gt;$$L(A, P, N) = max(||f(A) - f(P)||^2_2 - ||f(A) - f(N)||^2_2 + \alpha, 0)$$&lt;/p&gt;
&lt;p&gt;其中，因为 $||f(A) - f(P)||^2_2 - ||f(A) - f(N)||^2_2 + \alpha$ 的值需要小于等于 0，因此取它和 0 的更大值。&lt;/p&gt;
&lt;p&gt;对于大小为 $m$ 的训练集，代价函数为：&lt;/p&gt;
&lt;p&gt;$$J = \sum^m_{i=1}L(A^{(i)}, P^{(i)}, N^{(i)})$$&lt;/p&gt;
&lt;p&gt;通过梯度下降最小化代价函数。&lt;/p&gt;
&lt;p&gt;在选择训练样本时，随机选择容易使 Anchor 和 Positive 极为接近，而 Anchor 和 Negative 相差较大，以致训练出来的模型容易抓不到关键的区别。因此，最好的做法是人为增加 Anchor 和 Positive 的区别，缩小 Anchor 和 Negative 的区别，促使模型去学习不同人脸之间的关键差异。&lt;/p&gt;
&lt;p&gt;相关论文：
&lt;a href=&#34;https://arxiv.org/pdf/1503.03832.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Schroff et al., 2015,  FaceNet: A unified embedding for face recognition and clustering&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;二分类结构&#34;&gt;二分类结构&lt;/h3&gt;
&lt;p&gt;除了 Triplet 损失函数，二分类结构也可用于学习参数以解决人脸识别问题。其做法是输入一对图片，将两个 Siamese 网络产生的特征向量输入至同一个 Sigmoid 单元，输出 1 则表示是识别为同一人，输出 0 则表示识别为不同的人。&lt;/p&gt;
&lt;p&gt;Sigmoid 单元对应的表达式为：&lt;/p&gt;
&lt;p&gt;$$\hat y = \sigma (\sum^K_{k=1}w_k|f(x^{(i)})_{k} - x^{(j)})_{k}| + b)$$&lt;/p&gt;
&lt;p&gt;其中，$w_k$ 和 $b$ 都是通过梯度下降算法迭代训练得到的参数。上述计算表达式也可以用另一种表达式代替：&lt;/p&gt;
&lt;p&gt;$$\hat y = \sigma (\sum^K_{k=1}w_k
\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k} + b)$$&lt;/p&gt;
&lt;p&gt;其中，$\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k}$ 被称为 $\chi$ 方相似度。&lt;/p&gt;
&lt;p&gt;无论是对于使用 Triplet 损失函数的网络，还是二分类结构，为了减少计算量，可以提前计算好编码输出 $f(x)$ 并保存。这样就不必存储原始图片，并且每次进行人脸识别时只需要计算测试图片的编码输出。&lt;/p&gt;
&lt;h2 id=&#34;神经风格迁移&#34;&gt;神经风格迁移&lt;/h2&gt;
&lt;p&gt;**神经风格迁移（Neural style transfer）**将参考风格图像的风格“迁移”到另外一张内容图像中，生成具有其特色的图像。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Neural-style-transfer.png&#34; alt=&#34;Neural-style-transfer&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;深度卷积网络在学什么&#34;&gt;深度卷积网络在学什么？&lt;/h3&gt;
&lt;p&gt;想要理解如何实现神经风格转换，首先要理解在输入图像数据后，一个深度卷积网络从中都学到了些什么。我们借助可视化来做到这一点。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Visualizing-deep-layers.png&#34; alt=&#34;Visualizing-deep-layers&#34;&gt;&lt;/p&gt;
&lt;p&gt;我们通过遍历所有的训练样本，找出使该层激活函数输出最大的 9 块图像区域。可以看出，浅层的隐藏层通常检测出的是原始图像的边缘、颜色、阴影等简单信息。随着层数的增加，隐藏单元能捕捉的区域更大，学习到的特征也由从边缘到纹理再到具体物体，变得更加复杂。&lt;/p&gt;
&lt;p&gt;相关论文：
&lt;a href=&#34;https://arxiv.org/pdf/1311.2901.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zeiler and Fergus., 2013, Visualizing and understanding convolutional networks&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;代价函数&#34;&gt;代价函数&lt;/h3&gt;
&lt;p&gt;神经风格迁移生成图片 G 的代价函数如下：&lt;/p&gt;
&lt;p&gt;$$J(G) = \alpha \cdot J_{content}(C, G) + \beta \cdot J_{style}(S, G)$$&lt;/p&gt;
&lt;p&gt;其中，$\alpha$、$\beta$ 是用于控制相似度比重的超参数。&lt;/p&gt;
&lt;p&gt;神经风格迁移的算法步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;随机生成图片 G 的所有像素点；&lt;/li&gt;
&lt;li&gt;使用梯度下降算法使代价函数最小化，以不断修正 G 的所有像素点。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;相关论文：
&lt;a href=&#34;https://arxiv.org/pdf/1508.06576v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gatys al., 2015. A neural algorithm of artistic style&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;内容代价函数&#34;&gt;内容代价函数&lt;/h4&gt;
&lt;p&gt;上述代价函数包含一个内容代价部分和风格代价部分。我们先来讨论内容代价函数 $J_{content}(C, G)$，它表示内容图片 C 和生成图片 G 之间的相似度。&lt;/p&gt;
&lt;p&gt;$J_{content}(C, G)$ 的计算过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用一个预训练好的 CNN（例如 VGG）；&lt;/li&gt;
&lt;li&gt;选择一个隐藏层 $l$ 来计算内容代价。$l$ 太小则内容图片和生成图片像素级别相似，$l$ 太大则可能只有具体物体级别的相似。因此，$l$ 一般选一个中间层；&lt;/li&gt;
&lt;li&gt;设 $a^{(C)[l]}$、$a^{(G)[l]}$ 为 C 和 G 在 $l$ 层的激活，则有：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$J_{content}(C, G) = \frac{1}{2}||(a^{(C)[l]} - a^{(G)[l]})||^2$$&lt;/p&gt;
&lt;p&gt;$a^{(C)[l]}$ 和 $a^{(G)[l]}$ 越相似，则 $J_{content}(C, G)$ 越小。&lt;/p&gt;
&lt;h4 id=&#34;风格代价函数&#34;&gt;风格代价函数&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Intuition-about-style-of-an-image.png&#34; alt=&#34;Intuition-about-style-of-an-image&#34;&gt;&lt;/p&gt;
&lt;p&gt;每个通道提取图片的特征不同，比如标为红色的通道提取的是图片的垂直纹理特征，标为黄色的通道提取的是图片的橙色背景特征。那么计算这两个通道的相关性，相关性的大小，即表示原始图片既包含了垂直纹理也包含了该橙色背景的可能性大小。通过 CNN，“风格”被定义为同一个隐藏层不同通道之间激活值的相关系数，因其反映了原始图片特征间的相互关系。&lt;/p&gt;
&lt;p&gt;对于风格图像 S，选定网络中的第 $l$ 层，则相关系数以一个 gram 矩阵的形式表示：&lt;/p&gt;
&lt;p&gt;$$G^{[l](S)}_{kk&amp;rsquo;} = \sum^{n^{[l]}_H}_{i=1} \sum^{n^{[l]}_W}_{j=1} a^{[l](S)}_{ijk} a^{[l](S)}_{ijk&amp;rsquo;}$$&lt;/p&gt;
&lt;p&gt;其中，$i$ 和 $j$ 为第 $l$ 层的高度和宽度；$k$ 和 $k&#39;$ 为选定的通道，其范围为 $1$ 到 $n_C^{[l]}$；$a^{[l](S)}_{ijk}$ 为激活。&lt;/p&gt;
&lt;p&gt;同理，对于生成图像 G，有：&lt;/p&gt;
&lt;p&gt;$$G^{[l](G)}_{kk&amp;rsquo;} = \sum^{n^{[l]}_H}_{i=1} \sum^{n^{[l]}_W}_{j=1} a^{[l](G)}_{ijk} a^{[l](G)}_{ijk&amp;rsquo;}$$&lt;/p&gt;
&lt;p&gt;因此，第 $l$ 层的风格代价函数为：&lt;/p&gt;
&lt;p&gt;$$J^{[l]}_{style}(S, G) = \frac{1}{(2n^{[l]}_Hn^{[l]}_Wn^{[l]}_C)^2} \sum_k \sum_{k&amp;rsquo;}(G^{[l](S)}_{kk&amp;rsquo;} - G^{[l](G)}_{kk&amp;rsquo;})^2$$&lt;/p&gt;
&lt;p&gt;如果对各层都使用风格代价函数，效果会更好。因此有：&lt;/p&gt;
&lt;p&gt;$$J_{style}(S, G) = \sum_l \lambda^{[l]} J^{[l]}_{style}(S, G)$$&lt;/p&gt;
&lt;p&gt;其中，$lambda$ 是用于设置不同层所占权重的超参数。&lt;/p&gt;
&lt;h3 id=&#34;推广至一维和三维&#34;&gt;推广至一维和三维&lt;/h3&gt;
&lt;p&gt;之前我们处理的都是二维图片，实际上卷积也可以延伸到一维和三维数据。我们举两个示例来说明。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/1D-3D-Convolution.png&#34; alt=&#34;1D-3D-Convolution&#34;&gt;&lt;/p&gt;
&lt;p&gt;EKG 数据（心电图）是由时间序列对应的每个瞬间的电压组成，是一维数据。一般来说我们会用 RNN（循环神经网络）来处理，不过如果用卷积处理，则有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入时间序列维度：14 x 1&lt;/li&gt;
&lt;li&gt;滤波器尺寸：5 x 1，滤波器个数：16&lt;/li&gt;
&lt;li&gt;输出时间序列维度：10 x 16&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而对于三维图片的示例，有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入 3D 图片维度：14 x 14 x 14 x 1&lt;/li&gt;
&lt;li&gt;滤波器尺寸：5 x 5 x 5 x 1，滤波器个数：16&lt;/li&gt;
&lt;li&gt;输出 3D 图片维度：10 x 10 x 10 x 16&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Link Layer</title>
      <link>/courses/computer_network/link_layer/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/computer_network/link_layer/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;understand principles behind data link layer services:
&lt;ul&gt;
&lt;li&gt;error detection, correction&lt;/li&gt;
&lt;li&gt;sharing a broadcast channel, multiple access (无线网络)&lt;/li&gt;
&lt;li&gt;link layer addressing (48 bit)&lt;/li&gt;
&lt;li&gt;reliable data transfer, flow control&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;instantiation and implementation of various link layer technologies&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca85427a4c9a.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;hosts and routers are nodes&lt;/li&gt;
&lt;li&gt;communication channels that connect adjacent nodes along communication path are links (wired links, wireless links, LANs)&lt;/li&gt;
&lt;li&gt;layer-2 packet is a frame, encapsulates datagram&lt;/li&gt;
&lt;li&gt;data-link layer has a responsibility of transferring datagram from one node to adjacent node over a link&lt;/li&gt;
&lt;li&gt;datagram transferred by different link protocols over different links&lt;/li&gt;
&lt;li&gt;each link protocol provides different services (may or may not provide rdt(reliable data service) over link)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;services&#34;&gt;Services&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Framing, link access
&lt;ul&gt;
&lt;li&gt;encapsulate datagram into frame, adding header, trailer&lt;/li&gt;
&lt;li&gt;channel access if shared mediums&lt;/li&gt;
&lt;li&gt;&amp;ldquo;MAC&amp;rdquo; (media access control protocol) addresses used in frame headers to identify source, destination&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;reliable delivery between adjacent nodes
&lt;ul&gt;
&lt;li&gt;TCP&lt;/li&gt;
&lt;li&gt;seldom used on low bit-error link(fiber, some twisted pair)&lt;/li&gt;
&lt;li&gt;wireless links high error rates&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;flow control: pacing between adjacent sending and receiving nodes&lt;/li&gt;
&lt;li&gt;error detections:
&lt;ul&gt;
&lt;li&gt;error caused by  noise&lt;/li&gt;
&lt;li&gt;receiver detects presence of errors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error correction: receiver identifies and corrects bit error(s) without resorting to retransmission&lt;/li&gt;
&lt;li&gt;half-duplex and full-duplex
with half duplex, nodes at both ends of link can transmit, but not at same time&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;where-is-link-layer-implemented&#34;&gt;Where is link layer implemented&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca858ae361ff.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;in each and every host&lt;/li&gt;
&lt;li&gt;link layer implemented in &amp;ldquo;adaptor&amp;rdquo; (network interface card)
&lt;ul&gt;
&lt;li&gt;ethernet card, PCMCI card, 802.11 card&lt;/li&gt;
&lt;li&gt;implement link, physical layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;attaches into host&amp;rsquo;s system buses&lt;/li&gt;
&lt;li&gt;combination of hardware, software, firmware&lt;/li&gt;
&lt;li&gt;sending side : encapsulates datagram in frame, address error checking bits, rdt, flow control, etc&lt;/li&gt;
&lt;li&gt;receiving side: look for errors, rdt, flow control etc, extracts datagram, passes to upper layer at receiving side&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;error-detection-and-error-correction&#34;&gt;Error detection and error correction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;EDC: Error detection and correction bits (redundancy)&lt;/li&gt;
&lt;li&gt;D: data protected by error checking, may include header files&lt;/li&gt;
&lt;li&gt;Error detection not 100% reliable
&lt;ul&gt;
&lt;li&gt;protocol may miss some errors, but rarely&lt;/li&gt;
&lt;li&gt;larger EDC field yields better detection and correction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca85a9c09874.png&#34; width=&#34;400px&#34;/&gt;
&lt;h3 id=&#34;parity-checking&#34;&gt;Parity Checking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Single Bit Parity (Detect single bit errors)
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca85b23a266e.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Two dimensional Bit Parity (Detect and correct single bit errors)
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca85b845aa63.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;internet-checksum&#34;&gt;Internet Checksum&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;detect &amp;ldquo;errors&amp;rdquo; (e.g. flipped bits) in transmitted packet (used at transport layer only)&lt;/li&gt;
&lt;li&gt;Sender:
&lt;ul&gt;
&lt;li&gt;treat segment contents as sequence of 16-bit integers&lt;/li&gt;
&lt;li&gt;checksum: addition (1&amp;rsquo;s complement sum) of segment contents&lt;/li&gt;
&lt;li&gt;sender puts checksum value into UDP checksum fields&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Receiver:
&lt;ul&gt;
&lt;li&gt;compute checksum of received checksum&lt;/li&gt;
&lt;li&gt;check if computed checksum equals checksum field value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cyclic-redundancy-check&#34;&gt;Cyclic Redundancy Check&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;view data bits D as a binary number&lt;/li&gt;
&lt;li&gt;choose r+1 bit pattern (generator) G&lt;/li&gt;
&lt;li&gt;goal: choose r CRC bits, R, such that
&lt;ul&gt;
&lt;li&gt;&amp;lt;D,R&amp;gt; exactly divisible by G&lt;/li&gt;
&lt;li&gt;receiver knows G, divides &amp;lt;D,R&amp;gt; by G&lt;/li&gt;
&lt;li&gt;can detect all burst errors less than r+1 bits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;widely used in practice (Ethernet, 802.11 WIFI, ATM)
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca85e5f9fc02.png&#34; width=&#34;400px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca85eca9089e.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;multiple-access-links-and-protocols&#34;&gt;Multiple Access Links and Protocols&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Two types of &amp;ldquo;links&amp;rdquo;
&lt;ul&gt;
&lt;li&gt;Point-to-point: PPP for dial-up access, point-to-point link between Ethernet switch and host&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;broadcast (shared wire or medium)
&lt;ul&gt;
&lt;li&gt;old-fashioned Ethernet&lt;/li&gt;
&lt;li&gt;upstream HFC (hybrid fiber coax)&lt;/li&gt;
&lt;li&gt;802.11 wireless LAN
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca85ff5c1043.png&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;single shared  broadcast channel&lt;/li&gt;
&lt;li&gt;two or more simultaneous transmissions by nodes: collision if node receives two or more signals at the same time&lt;/li&gt;
&lt;li&gt;multiple access protocol:
&lt;ul&gt;
&lt;li&gt;distributed algorithm that determines how nodes share channel, i.e. determine when node can transmit&lt;/li&gt;
&lt;li&gt;communication about channel sharing must use channel itself, no out-of-band channel for coordination&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ideal-multiple-access-protocol&#34;&gt;Ideal Multiple Access Protocol&lt;/h3&gt;
&lt;p&gt;Broadcast channel of rate R bps&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;when one node wants to transmit, it can send at rate R&lt;/li&gt;
&lt;li&gt;when M nodes want to transmit, each can send at average rate R?M&lt;/li&gt;
&lt;li&gt;fully decentralized
&lt;ul&gt;
&lt;li&gt;no special node to coordination transmissions&lt;/li&gt;
&lt;li&gt;no synchronization of clocks, slots&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;simple&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;mac-protocols-a-taxonomy&#34;&gt;MAC protocols: a taxonomy&lt;/h3&gt;
&lt;h4 id=&#34;channel-partitioning-tdma-fdma&#34;&gt;Channel Partitioning: TDMA/ FDMA&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;TDMA: time division multiple access
&lt;ul&gt;
&lt;li&gt;access to channel in &amp;ldquo;rounds&amp;rdquo;&lt;/li&gt;
&lt;li&gt;each station gets fixed length slot(length= packet transmission time) in each round&lt;/li&gt;
&lt;li&gt;unused slots go idle&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FDMA: frequency division multiple access
&lt;ul&gt;
&lt;li&gt;channel spectrum divided into frequency bands&lt;/li&gt;
&lt;li&gt;each station assigned fixed frequency band&lt;/li&gt;
&lt;li&gt;unused transmission time in frequency bands go idle
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca86e895021e.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;random-access&#34;&gt;Random access&lt;/h4&gt;
&lt;p&gt;channel not divided, allow collisions, &amp;ldquo;recover&amp;rdquo; from collisions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;when node has packet to send, transmit at full channel data rate R&lt;/li&gt;
&lt;li&gt;two or more transmitting nodes $\rightarrow$ collision&lt;/li&gt;
&lt;li&gt;random access MAC protocol specifies
&lt;ul&gt;
&lt;li&gt;how to detect collisions&lt;/li&gt;
&lt;li&gt;how to recover from collisions (e.g. via delayed retransmission)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;examples of random access MAC protocols: slotted ALOHA, CSMA, CSMA/CD (ethernet), CSMA/CA&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;slotted-aloha&#34;&gt;Slotted ALOHA&lt;/h5&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca86ff170d36.png&#34; width=&#34;400px&#34;/&gt;
- all frames same size
- time divided into equal size slots (time to )
&lt;ul&gt;
&lt;li&gt;Operation: when node obtains fresh frame, transmits in next slot
&lt;ul&gt;
&lt;li&gt;if no collision:&lt;/li&gt;
&lt;li&gt;if collision: node retransmits frame in each subsequent slot with pro&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cons:
&lt;ul&gt;
&lt;li&gt;collisions, wasting slots&lt;/li&gt;
&lt;li&gt;idle slots&lt;/li&gt;
&lt;li&gt;nodes may be able to detect collision in less than time to transmit pocket&lt;/li&gt;
&lt;li&gt;clock synchronization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pros
&lt;ul&gt;
&lt;li&gt;single&lt;/li&gt;
&lt;li&gt;highly decentralized&lt;/li&gt;
&lt;li&gt;simple&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Efficiency: long-run fraction of successful slots (many nodes, all with many frames to send)
&lt;ul&gt;
&lt;li&gt;suppose: N nodes with many frames to send, each transmits in slot with probability p&lt;/li&gt;
&lt;li&gt;prob that given node has success in a slot = $p(1-p)^{N-1}$&lt;/li&gt;
&lt;li&gt;prob than any node has a success = $Np(1-p)^{N-1}$&lt;/li&gt;
&lt;li&gt;for many nodes, max efficiency $p = 1/e = 0.37$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;pureunslotted-aloha&#34;&gt;Pure(unslotted) ALOHA&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;unslotted Aloha: simpler, no synchronization&lt;/li&gt;
&lt;li&gt;when frame first arrives: transmit immediately&lt;/li&gt;
&lt;li&gt;collision probability increases, frame set at $t_0$ collides with other frames send in $[t_0-1, t_0+1]$
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca87295cba56.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;prob $p*(1-p)^{2(N-1)}$&lt;/li&gt;
&lt;li&gt;for many nodes, max efficiency $p = 1/(2e) = 0.18$&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;csma-carrier-sense-multiple-access&#34;&gt;CSMA (Carrier Sense Multiple Access)&lt;/h5&gt;
&lt;p&gt;CSMA: listen before transmit, if channel sensed idle transmit entire frame, if channel sensed busy, defer transmission&lt;/p&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca87401d7b82.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;collision: entire packet transmission time wasted&lt;/li&gt;
&lt;li&gt;role of distance &amp;amp; propagation delay in determining collision probability&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;csmacd-collision-detection&#34;&gt;CSMA/CD (collision detection)&lt;/h5&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca8753567677.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;collision detection:
&lt;ul&gt;
&lt;li&gt;easy in wried LANS: measure signal strengths, compare transmitted, received signals&lt;/li&gt;
&lt;li&gt;difficult in wireless LANs; received signal strength overwhelmed by local transmission strength&lt;/li&gt;
&lt;li&gt;CSMA/CD used in Ethernet&lt;/li&gt;
&lt;li&gt;CSMA/CA (collision avoidance) used in 802.11&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;taking-turns-mac-protocols&#34;&gt;&amp;ldquo;Taking Turns&amp;rdquo; MAC protocols&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;channel partitioning MAC protocols:
&lt;ul&gt;
&lt;li&gt;share channel efficiently and fairly at high load&lt;/li&gt;
&lt;li&gt;inefficient at low load: delay in channel access, 1/N bandwidth allocated even if only 1 active node&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Random access MAC protocols:
&lt;ul&gt;
&lt;li&gt;efficient at low load: single node can fully utilized channel&lt;/li&gt;
&lt;li&gt;high load: collision overhead&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;taking turns protocols: look for best of both worlds&lt;/li&gt;
&lt;li&gt;Polling
&lt;ul&gt;
&lt;li&gt;master node &amp;ldquo;invites&amp;rdquo; slave nodes to transmit in turn&lt;/li&gt;
&lt;li&gt;typically used with &amp;ldquo;dumb&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Token passing (Taking Turns):
&lt;ul&gt;
&lt;li&gt;control token passed from one node to next sequentially&lt;/li&gt;
&lt;li&gt;token message&lt;/li&gt;
&lt;li&gt;concerns: token overhead, latency, singe point of failure&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;example: Bluetooth, FDDI, IBM Token Ring&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;link-layer-addressing&#34;&gt;Link-Layer Addressing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;32-bit IP address:
&lt;ul&gt;
&lt;li&gt;network-layer address&lt;/li&gt;
&lt;li&gt;used to get datagram to destination IP subnet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MAC (or LAN or physical or Ethernet) address:
&lt;ul&gt;
&lt;li&gt;function: get frame from one interface to another physically-connected interface (same network)&lt;/li&gt;
&lt;li&gt;48 bit MAC address (for most LANS)&lt;/li&gt;
&lt;li&gt;burned in NIC ROM, also sometimes software settable&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Each adapter on LAN has unique LAN address (FF-FF-FF-FF-FF-FF: broadcast address)&lt;/li&gt;
&lt;li&gt;mac address allocation administered by IEEE&lt;/li&gt;
&lt;li&gt;manufacturer buys portion of MAC address space (to assure uniqueness)&lt;/li&gt;
&lt;li&gt;MAC flat address $\rightarrow$ portability, can move LAN card from one LAN to another&lt;/li&gt;
&lt;li&gt;IP hierarchical address not portable&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;arp-address-resolution-protocol&#34;&gt;ARP: address Resolution Protocol&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;how to determine MAC address of B knowing B&amp;rsquo;s IP address?&lt;/li&gt;
&lt;li&gt;Each IP node (host, router) on LAN has ARP table&lt;/li&gt;
&lt;li&gt;ARP table: IP/MAC address mappings for some LAN nodes, TTL(time to live): time after which address mapping will be forgotten (typically 20 minutes)
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca87cadbddda.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;procedure
&lt;ul&gt;
&lt;li&gt;A wants to send datagram to B, and B&amp;rsquo;s MAC address not in A&amp;rsquo;s ARP table&lt;/li&gt;
&lt;li&gt;A broadcast ARP query packet, containing B&amp;rsquo;s IP address, all machines on LAN receive ARP query&lt;/li&gt;
&lt;li&gt;B receivers ARP packet, replies to A with its (B&amp;rsquo;s) MAC address (frames send to A&amp;rsquo;s MAC address(unicast))&lt;/li&gt;
&lt;li&gt;A caches (saves) IP-to-MAC address pair in its ARP table until information becomes old (times out)&lt;/li&gt;
&lt;li&gt;ARP is &amp;ldquo;plug-and-play&amp;rdquo;, node creates their ARP tables without intervention form net administrator&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;example:
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca87f2ac983f.png&#34; width=&#34;550px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ethernet&#34;&gt;Ethernet&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;features:
&lt;ul&gt;
&lt;li&gt;cheap&lt;/li&gt;
&lt;li&gt;first widely used LAN technology&lt;/li&gt;
&lt;li&gt;simpler, cheaper than  token LANs and ATM&lt;/li&gt;
&lt;li&gt;kept up with speed race: 10Mbps - 10 Gbps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;bus topology popular through mid 90-s (all nodes in same collision domain, can collide with each other)&lt;/li&gt;
&lt;li&gt;today: star topology prevails
&lt;ul&gt;
&lt;li&gt;active, switch in center&lt;/li&gt;
&lt;li&gt;each &amp;ldquo;spoke&amp;rdquo; run a (separate) Ethernet protocol (nodes do not collide with each other)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ethernet Frame structure
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca88188bf408.png&#34; width=&#34;500px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;Preamble: 7 bytes with pattern 10101010 followed by one byte with pattern 10101011&lt;/li&gt;
&lt;li&gt;used to synchronize receiver, sender clock rates&lt;/li&gt;
&lt;li&gt;Addresses: 6 bytes, if adapter receives frame with matching destination address, or with broadcast address (e.g. ARP packet), it passes data in frame to network layer protocol, otherwise, adapter discards frame&lt;/li&gt;
&lt;li&gt;Type: indicates higher layer protocol (mostly IP but others possible, e.g. Novell IPX, Apple Talk)&lt;/li&gt;
&lt;li&gt;CRC: checked at receiver, if error is detected, frame is dropped&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unreliable, connectionless
&lt;ul&gt;
&lt;li&gt;connectionless: No handshaking between sending and receiving NICs&lt;/li&gt;
&lt;li&gt;unreliable： receiving NIC(network interface card) doesn&amp;rsquo;t send ACKs or NAKs to sending NIC
&lt;ul&gt;
&lt;li&gt;stream of datagrams passed to network layer can have gaps(missing datagrams)&lt;/li&gt;
&lt;li&gt;gaps will be filled if app is using TCP, otherwise app will see gaps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ethernet&amp;rsquo;s MAC protocol: unslotted CSMA/CD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ethernet-csmacd-algorithm&#34;&gt;Ethernet CSMA/CD algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;NIC receives datagram from network layer, creates frame&lt;/li&gt;
&lt;li&gt;IF NIC senses channel idle, starts frame transmission, IF NIC senses channel busy, waits until channel idle, then transmits&lt;/li&gt;
&lt;li&gt;If NIC transmits entire frame without detecting another transmission, NIC is done with frame&lt;/li&gt;
&lt;li&gt;If NIC detects another transmission while transmitting, aborts and sends jam signal&lt;/li&gt;
&lt;li&gt;After aborting, NIC enters exponential backoff: after mth collision, NIC chooses K at random from $(0, 1,2, &amp;hellip;, 2^m-1)$ NIC waits K*512 bit times, returns to Step2&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Jam signal: make sure all other transmitters are aware of collision: 48 bits&lt;/li&gt;
&lt;li&gt;Bit time: 1 microsecond for 10 Mbps Ethernet, for K=1023,wait time is about 50 msec&lt;/li&gt;
&lt;li&gt;Exponential Backoff (最多连续16次)
&lt;ul&gt;
&lt;li&gt;Goal: adapt retransmission attempts to estimated current load (heavy load: random wait will be longer)&lt;/li&gt;
&lt;li&gt;first collision: choose K from (0,1), delay is K*512 bit transmission times&lt;/li&gt;
&lt;li&gt;after second collision: choose K from {0,1,2,3}&lt;/li&gt;
&lt;li&gt;after ten collisions: choose K from {0,1,2,3,4,&amp;hellip;,1023}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Efficiency (70%)
&lt;ul&gt;
&lt;li&gt;$T_{prop}$ = max prop delay between 2 nodes in LAN&lt;/li&gt;
&lt;li&gt;$T_{trans}$ = time to transmit max-size frame
$$\text{efficiency} = \frac{1}{1+5t_{prop}/t_{trans}}$$&lt;/li&gt;
&lt;li&gt;Efficiency goes to 1 as $t_{prop}$ goes to 0, as t_{trans}$ goes to infinity&lt;/li&gt;
&lt;li&gt;better performance than ALOHA: and simple, cheap, decentralized&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;8023-ethernet-standards-link--physical-layers&#34;&gt;802.3 Ethernet Standards: Link &amp;amp; Physical Layers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;many different Ethernet standards
&lt;ul&gt;
&lt;li&gt;common MAC protocol and frame format&lt;/li&gt;
&lt;li&gt;different speeds: 2Mbps, 10Mbps, 100Mbps, 1Gbps, 10Gbps&lt;/li&gt;
&lt;li&gt;different physical layer media: fiber, cable
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca889e6adb92.png&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Manchester encoding
&lt;ul&gt;
&lt;li&gt;used in 10Base T&lt;/li&gt;
&lt;li&gt;each bit has a transition&lt;/li&gt;
&lt;li&gt;allows clocks in sending and receiving nodes to synchronize to each other, no need for a centralized, global clock among nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;link-layer-switches&#34;&gt;Link-Layer switches&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hubs
&lt;ul&gt;
&lt;li&gt;physical-layer (&amp;ldquo;dumb&amp;rdquo; repeaters, 只处理信号)&lt;/li&gt;
&lt;li&gt;bits coming in one link go out all other links at same rate&lt;/li&gt;
&lt;li&gt;all nodes connected to hub can collide with one another&lt;/li&gt;
&lt;li&gt;no frame buffering&lt;/li&gt;
&lt;li&gt;no CSMA/CD at hub: host NIC detect collisions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Switch: allow multiple simultaneous transmissions
&lt;ul&gt;
&lt;li&gt;link-layer device: smarter than hubs, take active role
&lt;ul&gt;
&lt;li&gt;store, forward Ethernet frames&lt;/li&gt;
&lt;li&gt;examine incoming frame&amp;rsquo;s MAC address, selectively forward frame to one-or-more outgoing links when frame is to be forwarded on segment, uses CSMA/CD to access segment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;transparent, hosts are unaware of presence of switches&lt;/li&gt;
&lt;li&gt;plug-and-play, self-learning, switch do not need to be configured&lt;/li&gt;
&lt;li&gt;switch table:  Each switch has a switch table, each entry (MAC address of host, interface to reach host, time stamp&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;self-learning&#34;&gt;Self-Learning&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Switch table is initially empty&lt;/li&gt;
&lt;li&gt;For each incoming frame received on an interface, the switches stores in its table
&lt;ol&gt;
&lt;li&gt;The MAC address in the frame&amp;rsquo;s source address field&lt;/li&gt;
&lt;li&gt;the interface from which the frame arrived&lt;/li&gt;
&lt;li&gt;the current time&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;the switch deletes an address in the table if no frames are received with that address as the source address after some period of time&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;filteringforwarding&#34;&gt;Filtering/forwarding&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Filtering: whether a frame should be forwarded to some interface or should just be dropped&lt;/li&gt;
&lt;li&gt;Forwarding: determine the interfaces to which a frame should be directed, and then moves the frame to those interfaces&lt;/li&gt;
&lt;li&gt;suppose a frame with destination address DD-DD-DD-DD-DD-DD arrives at the switch on interface x
&lt;ul&gt;
&lt;li&gt;there is no entry in the table for DD-DD-DD-DD-DD-DD, the switch forwards copies of the frame to the output buffers preceding all interfaces (broadcast the frame)&lt;/li&gt;
&lt;li&gt;there is an entry in the table, associating DD-DD-DD-DD-DD-DD with interface x: there is no need to forward the frame to any of the other interfaces, switch performs filtering function by discarding the frame. (送和收在同一区域）&lt;/li&gt;
&lt;li&gt;there is an entry in table, associating DD-DD-DD-DD-DD-DD with interface $y \neq x$, the frame needs to be forwarded to the LAN segment attached to interface y. Switch performs its forwarding function by putting the frame in an output buffer that precedes interface y.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;switches-vs-routers&#34;&gt;Switches vs. Routers&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/04/06/5ca8921f69722.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;both store-and-forward devices: routers are network layer devices, switches are link layer address&lt;/li&gt;
&lt;li&gt;Routers maintain routing tables, implement routing algorithms&lt;/li&gt;
&lt;li&gt;switches maintain switch tables, implement filtering, learning algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ppp-rfc-1557&#34;&gt;PPP [RFC 1557]&lt;/h2&gt;
&lt;p&gt;Point to Point Data Link Control&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one sender, one receiver, one link: easier than broadcast link&lt;/li&gt;
&lt;li&gt;no media Access control&lt;/li&gt;
&lt;li&gt;no need for explicit MAC addressing  (e.g. dialup link)&lt;/li&gt;
&lt;li&gt;popular point-to-point DLC protocols: PPP, HDLC&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ppp-frame&#34;&gt;PPP Frame&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;packet framing: encapsulation of network-layer datagram in data link frame
&lt;ul&gt;
&lt;li&gt;carry network layer data of any network layer protocol (not just IP) at same time&lt;/li&gt;
&lt;li&gt;ability to demultiplex upwards&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;bit transparency: must carry any bit pattern in the data field&lt;/li&gt;
&lt;li&gt;error detection (no correction)&lt;/li&gt;
&lt;li&gt;connection liveness: detect, signal link failure to network layer&lt;/li&gt;
&lt;li&gt;network layer address negotiation: endpoint can learn/configure each other&amp;rsquo;s network address&lt;/li&gt;
&lt;li&gt;data frame
&lt;img src=&#34;https://i.loli.net/2019/04/07/5ca9ba6436d50.png&#34; width=&#34;450px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;Flag: delimiter (framing)&lt;/li&gt;
&lt;li&gt;Address: does nothing&lt;/li&gt;
&lt;li&gt;control: does nothing:&lt;/li&gt;
&lt;li&gt;protocol: upper layer protocol to which frame delivered (e.g. PPP-LCP, IP, IPCP etc)&lt;/li&gt;
&lt;li&gt;info: information&lt;/li&gt;
&lt;li&gt;check: cyclic redundancy check for error-detection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PPP non-requirements
&lt;ul&gt;
&lt;li&gt;no error correction&lt;/li&gt;
&lt;li&gt;no flow control&lt;/li&gt;
&lt;li&gt;out of order delivery&lt;/li&gt;
&lt;li&gt;no need to support multipoint links&lt;/li&gt;
&lt;li&gt;error recovery, flow control, data re-ordering all relegated to higher layers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Byte Stuffing
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;data transparency&amp;rdquo; requirement: data field must be allowed to include flag pattern  &amp;lt;01111110&amp;gt;&lt;/li&gt;
&lt;li&gt;Sender: adds extra &amp;lt;01111110&amp;gt; byte after each &amp;lt;01111110&amp;gt; data byte&lt;/li&gt;
&lt;li&gt;Receiver: two 01111110 bytes in a row: discard first byte, continue data reception; singe 01111110:flag byte&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PPP data Control Protocol
&lt;ul&gt;
&lt;li&gt;before exchanging network-layer data, data link peers must&lt;/li&gt;
&lt;li&gt;configure PPP link (max frame length, authentication)&lt;/li&gt;
&lt;li&gt;learn / configure network layer information&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;atm&#34;&gt;ATM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Virtualization of networks
&lt;ul&gt;
&lt;li&gt;Layering of abstractions: don&amp;rsquo;t sweat the details of the lower layer, only deal with lower layers abstractly&lt;/li&gt;
&lt;li&gt;two layers of addressing: internetwork and local network&lt;/li&gt;
&lt;li&gt;new layer (IP) makes everything homogeneous at internetwork layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Asynchronous Transfer Mode: ATM
&lt;ul&gt;
&lt;li&gt;Goal: integrated, end-end transport of carry voice, video, data&lt;/li&gt;
&lt;li&gt;meeting timing / QoS requirements of voice, video (versus Internet best-effort model)&lt;/li&gt;
&lt;li&gt;&amp;ldquo;next generation&amp;rdquo; telephony (下一代网络电话）&lt;/li&gt;
&lt;li&gt;packet-switching (fixed length packets, 53 bytes, called &amp;ldquo;cells&amp;rdquo;) using virtual circuits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For more information, see 
&lt;a href=&#34;https://www.net.t-labs.tu-berlin.de/teaching/computer_networking/05.09.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computer Networking Website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-protocol-label-switching-mpls&#34;&gt;Multi-protocol label switching (MPLS)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Goal: speed up IP forwarding by using fixed length label (instead of IP address) to do forwarding (20bits)
&lt;ul&gt;
&lt;li&gt;Borrowing ideas from Virtual Circuit (VC) approach&lt;/li&gt;
&lt;li&gt;but IP datagram still keeps IP address
&lt;img src=&#34;https://i.loli.net/2019/04/07/5ca9bfc799f05.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MPLS-enhanced forwarding
&lt;img src=&#34;https://i.loli.net/2019/04/07/5ca9c076ea536.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Multithread Programming</title>
      <link>/courses/operating_system/multithread_programming/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/multithread_programming/</guid>
      <description>&lt;h2 id=&#34;threads&#34;&gt;Threads&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Threads is a light process, basic unit of CPU utilization&lt;/li&gt;
&lt;li&gt;All threads belonging to the same process share &lt;strong&gt;code section&lt;/strong&gt;, &lt;strong&gt;data section&lt;/strong&gt;, and &lt;strong&gt;OS resources&lt;/strong&gt; (e.g. open files and signals)&lt;/li&gt;
&lt;li&gt;Each thread has its own (thread control block) &lt;strong&gt;thread ID, program counter, register set, and a stack&lt;/strong&gt; &lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e26e6c2312.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Example
&lt;ul&gt;
&lt;li&gt;web browser, once thread displays contents while the other thread receives data from network&lt;/li&gt;
&lt;li&gt;web server, one request(thread), better performance as code and resource sharing&lt;/li&gt;
&lt;li&gt;RPC server
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e27ee06cfa.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Benefits of Multithreading
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Responsiveness&lt;/strong&gt;: allow a program to continue running even if part of it is blocked or is performing a lengthy operation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Resource sharing&lt;/strong&gt;: several different threads of activity all within the same address space&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Utilization of MP arch&lt;/strong&gt;: Several thread may be running in parallel on different processors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Economy&lt;/strong&gt;: Allocating memory and resources for process creation is costly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;multicore-programming&#34;&gt;Multicore Programming&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;multithread programming provides a mechanism for more efficient use of multiple cores and improved concurrency (threads can run in parallel)&lt;/li&gt;
&lt;li&gt;Multicore systems putting pressure on system designers and application program (scheduling algorithms use cores to allow the parallel execution&lt;/li&gt;
&lt;li&gt;challenges in Multicore Programming
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dividing activities&lt;/strong&gt;: divide program into concurrent tasks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data splitting&lt;/strong&gt;: divide data accessed and manipulated by the tasks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data dependency&lt;/strong&gt;: synchronize data access&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Balance&lt;/strong&gt;: evenly distribute tasks to cores&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Testing and debugging&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;user-vs-kernel-threads&#34;&gt;User vs. Kernel Threads&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;User thread &amp;ndash; thread management done by user level threads library (Pthreads, Java threads, Win32 threads)&lt;/li&gt;
&lt;li&gt;Kernel threads &amp;ndash; supported by the kernel(OS) directly (Windows 2000, Linux)&lt;/li&gt;
&lt;li&gt;User threads
&lt;ul&gt;
&lt;li&gt;thread library provides support for thread creation, scheduling and deletion&lt;/li&gt;
&lt;li&gt;Generally fast to create and manage&lt;/li&gt;
&lt;li&gt;If the kernel is single-threaded, a user-thread blocks $\rightarrow$ entire process blocks even if other threads are ready to run&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kernel threads
&lt;ul&gt;
&lt;li&gt;The kernel performs thread creation, scheduling, etc.&lt;/li&gt;
&lt;li&gt;Generally slower to create and manage&lt;/li&gt;
&lt;li&gt;If a thread is blocked, the kernel can schedule another thread for execution&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Models (user threads to kernel threads)
Many-to-one, one-to-one(kernel threads 有限制，大部分系统）, Many-to-Many&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;shared-memory-programming&#34;&gt;Shared-Memory Programming&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Definition: processes communicate or work together with each other through a shared memory space which can be accessed by all processes (Faster &amp;amp; more efficient than message passing)&lt;/li&gt;
&lt;li&gt;Many issues as well, &lt;strong&gt;Synchronization&lt;/strong&gt;, &lt;strong&gt;Deadlock&lt;/strong&gt;, &lt;strong&gt;Cache coherence&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Programming techniques (Parallelizing compiler, Unix processes, Threads(Pthread, Java))&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pthread&#34;&gt;Pthread&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Pthread is the implementation of POSIX standard for thread&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pthread_create(thread, attr, routine, arg)&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;thread&lt;/code&gt;: An unique identifier (token) for the new thread&lt;/li&gt;
&lt;li&gt;&lt;code&gt;attr&lt;/code&gt;: it is used to set thread attributes. NULL for the default values&lt;/li&gt;
&lt;li&gt;routine: The routine that the thread will execute once it is created&lt;/li&gt;
&lt;li&gt;arg: A single argument that may be passed to routine &lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e3392a48d7.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;pthread_join(threadId, status)
&lt;ul&gt;
&lt;li&gt;Blocks until the specified threadId thread terminates&lt;/li&gt;
&lt;li&gt;One way to accomplish synchronization between threads&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;pthread_detach(threadId)
&lt;ul&gt;
&lt;li&gt;Once a thread is detached, it can never be joined&lt;/li&gt;
&lt;li&gt;Detach a thread could free some system resources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/21/5c933ded79175.png&#34; width=&#34;400px&#34;/&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;  #include &amp;lt;pthread.h&amp;gt;
  #include &amp;lt;stdio.h&amp;gt; #define NUM_THREADS 5
  void *PrintHello(void *threadId) {
  long* data = static_cast &amp;lt;long*&amp;gt; threadId;
    printf(&amp;quot;Hello World! It&#39;s me, thread #%ld!\n&amp;quot;, *data);
    pthread_exit(NULL);
  }
  int main (int argc, char *argv[]) {
  pthread_t threads[NUM_THREADS];
  for(long tid=0; tid&amp;lt;NUM_THREADS; tid++){
      pthread_create(&amp;amp;threads[tid], NULL, PrintHello, (void *)&amp;amp;tid);
  }
  /* Last thing that main() should do */
    thread_exit(NULL);
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;java-threads&#34;&gt;Java Threads&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Thread is created by Extending Thread class, Implementing the Runnable interface&lt;/li&gt;
&lt;li&gt;Java threads are implemented using a thread library on the host System&lt;/li&gt;
&lt;li&gt;Thread mapping depends on the implementation of JVM&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linux-threads&#34;&gt;Linux threads&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linux does not support multithreading&lt;/li&gt;
&lt;li&gt;Various Pthreads implementation are available for user-level&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;fork&lt;/code&gt; system call &amp;ndash; create a new process and a copy of the associated data of the parent process&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;clone&lt;/code&gt; system call &amp;ndash; create a new process and a link that points to the associated data of the parent process&lt;/li&gt;
&lt;li&gt;A set of flags is used in the clone call for indication of the level of the sharing
&lt;ul&gt;
&lt;li&gt;None of the flag is set $\rightarrow$ clone = fork&lt;/li&gt;
&lt;li&gt;All flags are set $\rightarrow$ parent and child share everything
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e3635b4594.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;threading-issues&#34;&gt;Threading Issues&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Does &lt;code&gt;fork()&lt;/code&gt; duplicate only the calling thread or all threads? Some UNIX system support two versions of &lt;code&gt;fork()&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;execlp()&lt;/code&gt; works the same, replace the entire process, if &lt;code&gt;exec()&lt;/code&gt; is called immediately after forking, then duplicating all threads is unnecessary
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e37759091b.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Thread Cancellation
&lt;ul&gt;
&lt;li&gt;Asynchronous cancellation (one thread terminates the target thread immediately) 等 main thread 有空&lt;/li&gt;
&lt;li&gt;Deferred cancellation (default option) The target thread periodically checks whether it should be terminated, allowing it an opportunity to terminate itself in an orderly fashion (canceled safety)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Signal Handling
&lt;ul&gt;
&lt;li&gt;Signals (synchronous or asynchronous) are used in UNIX systems to notify a process that an event has occurred&lt;/li&gt;
&lt;li&gt;A signal handler is used to process signals
&lt;ol&gt;
&lt;li&gt;Signal is generated by particular event&lt;/li&gt;
&lt;li&gt;Signal is delivered to a process&lt;/li&gt;
&lt;li&gt;Signal is handled&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Thread Pools
&lt;ul&gt;
&lt;li&gt;Create a number of threads in a pool where they await work&lt;/li&gt;
&lt;li&gt;Advantages
&lt;ul&gt;
&lt;li&gt;Usually slightly faster to service a request with an existing thread than create a new thread&lt;/li&gt;
&lt;li&gt;Allows the number of threads in the application(s) to be bound the size of the pool&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;#  Of threads: # of CPUs, expected # of requests, amount of physical memory&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Process Scheduling</title>
      <link>/courses/operating_system/process_schedule/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/process_schedule/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;CPU-I/O burst cycle&lt;/strong&gt;: Process execution consists of a cycle of CPU execution and I/O wait&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generally, there is a large number of short CPU bursts, and a small number of long CPU bursts&lt;/li&gt;
&lt;li&gt;A I/O bound program would typically has many very short CPU bursts&lt;/li&gt;
&lt;li&gt;A CPU-bound program might have a few long CPU bursts
&lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e3c19c049e.png&#34; width=&#34;400px&#34;/&gt;
CPU scheduler: Select from ready queue to execute (i.e. allocates a CPU for the selected process)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;preemptive-vs-non-preemptive&#34;&gt;Preemptive vs. Non-preemptive&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CPU scheduling decisions may take palce when a process:
&lt;ol&gt;
&lt;li&gt;Switches from running to waitting state (IO)&lt;/li&gt;
&lt;li&gt;Switches from running to ready state (Time-sharing)&lt;/li&gt;
&lt;li&gt;Swtiches from waiting to ready state&lt;/li&gt;
&lt;li&gt;Terminates&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Non-preemptive scheduling (不会打断)
&lt;ul&gt;
&lt;li&gt;Scheduling under 1 and 4 (no choice in terms of scheduling)&lt;/li&gt;
&lt;li&gt;The process keeps the CPU until it is terminated or switched to the waitting state&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Preemptive scheduling, Scheduling under all cases 使用率很高&lt;/li&gt;
&lt;li&gt;Preemptive Issues
&lt;ul&gt;
&lt;li&gt;Inconsistent state of shared data, require process synchronization, incurs a cost assocated with access to shared data&lt;/li&gt;
&lt;li&gt;Affect the design of OS kernel
Unix solution: waiting either for a system call to complete or for an I/O block to take palce before doing a context switchd (disable interrupt)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dispatcher&#34;&gt;Dispatcher&lt;/h3&gt;
&lt;p&gt;Dispatcher module gives control of the CPU to the process selected by scheduler&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;switching context&lt;/li&gt;
&lt;li&gt;jumping to the proper location in the selected program
Dispatch latency &amp;ndash; time it takes for the dispatcher to stop one process and start another running&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;scheduling-algorithms&#34;&gt;Scheduling Algorithms&lt;/h2&gt;
&lt;h3 id=&#34;scheduling-criteria&#34;&gt;Scheduling Criteria&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CPU utilization theoretically: $0% ~ 100%$, real systems : $40% ~ 90%$&lt;/li&gt;
&lt;li&gt;Throughput: number of completed processes per time unit&lt;/li&gt;
&lt;li&gt;Turnaround time (submission ~ completion)&lt;/li&gt;
&lt;li&gt;Waiting time (total waiting time in the ready queue)&lt;/li&gt;
&lt;li&gt;Response time (submission ~ the first response is produced (第一个CPU burst(执行)的时间))&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;algorithms&#34;&gt;Algorithms&lt;/h3&gt;
&lt;h4 id=&#34;first-come-first-served-fcfs-scheduling&#34;&gt;First-Come, First-served (FCFS) scheduling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Process (Burst Time) in arriving order: P1(24), P2(3), P3(3)&lt;/li&gt;
&lt;li&gt;Gantt chart &lt;img src=&#34;https://i.loli.net/2019/03/17/5c8e43e08ae78.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Waiting time P1 = 0, P2 =24, P3 =27&lt;/li&gt;
&lt;li&gt;Average waiting time (AWT) (0+24+27)/3 = 17&lt;/li&gt;
&lt;li&gt;Convoy effect: short process behind a long process&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;shortest-job-first-sjf-scheduling&#34;&gt;Shortest-Job-First (SJF) scheduling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Associate with each process the length of its next CPU burst&lt;/li&gt;
&lt;li&gt;A process with shortest burst length  gets the CPU first&lt;/li&gt;
&lt;li&gt;SJF provides the minimum average waiting time (optimal)&lt;/li&gt;
&lt;li&gt;Two schemes
&lt;ul&gt;
&lt;li&gt;Non-preemptive &amp;ndash; once CPU given to a process, it cannot be preempted until its completion&lt;/li&gt;
&lt;li&gt;Preemptive &amp;ndash; if a new process arrives with shorter burst length, preemption happens&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Non-preemptive SJF example&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Process&lt;/th&gt;
&lt;th&gt;Arrival Time&lt;/th&gt;
&lt;th&gt;Burst Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;P1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;P2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;P3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;p4&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&#34;https://i.loli.net/2019/03/18/5c8ef25a8626f.png&#34; width=&#34;400px&#34;/&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;AWT = [(7-0-7) + (12 - 2 -3) + (8 -4-1) + (16-5-4)] /4 = 4
&lt;ul&gt;
&lt;li&gt;Response Time: p1 =0, p2 = 6, p3=3, p4 = 7&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Preemptive SJF example
&lt;img src=&#34;https://i.loli.net/2019/03/18/5c8ef3f119e66.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;AWT = 3&lt;/li&gt;
&lt;li&gt;Response time P1 = 0, P2 = 0, P3 = 0, P4 = 2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;note warning&#34;&gt;&lt;p&gt;SJF difficulty: no way to know length of the next CPU burst&lt;/p&gt;&lt;/div&gt;
- Approximate SJF: the next burst can be predicted as an exponentail average of the measured length of previous CPU bursts (set $\alpha = 1/2$)
  $$\tau_{n+1} = \alpha t_n + (1-\alpha) \tau_n = \frac12 t_n + \frac14 t_{n_1} + \frac18 t_{n-2}​$$
&lt;img src=&#34;https://i.loli.net/2019/03/18/5c8ef586096a8.png&#34; width=&#34;400px&#34;/&gt;
&lt;h4 id=&#34;priority-scheduling&#34;&gt;Priority Scheduling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A priority number is associated with each process&lt;/li&gt;
&lt;li&gt;The CPU is allocated to the highest priority process&lt;/li&gt;
&lt;li&gt;SJF is a priority scheduling where priority is the predicted next CPU burst time&lt;/li&gt;
&lt;li&gt;Problem: Starvation (low priority process never execute)&lt;/li&gt;
&lt;li&gt;Solution: &lt;strong&gt;aging&lt;/strong&gt;(as time progresses increase the priority of process)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;round-robinrr-scheduling&#34;&gt;Round-Robin(RR) Scheduling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Each process gets a small unit of CPU time(time quantum)&lt;/li&gt;
&lt;li&gt;After TQ elapsed, process is preempted and added to the end of the ready queue&lt;/li&gt;
&lt;li&gt;TQ large $\rightarrow$ FIFO&lt;/li&gt;
&lt;li&gt;TQ small $\rightarrow$ (context switch) overhead increases&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;multilevel-queue-scheduling&#34;&gt;Multilevel Queue Scheduling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ready queue is partitioned ino separate queues&lt;/li&gt;
&lt;li&gt;Each queue has its own scheduling algortihm&lt;/li&gt;
&lt;li&gt;Scheduling must be done between queues
&lt;ul&gt;
&lt;li&gt;Fixed priority scheduling: prossibility of starvation
&lt;img src=&#34;https://i.loli.net/2019/03/18/5c8efd2999070.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;mutillevel-feedback-queue-schedule&#34;&gt;Mutillevel Feedback Queue Schedule&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A process can move between the various queues; aging can be implemented&lt;/li&gt;
&lt;li&gt;Idea:  separate processes according to the characteristic of their CPU burst
&lt;ul&gt;
&lt;li&gt;I/O-bound and interactive processes in higher priority queue $\rightarrow$ short CPU burst&lt;/li&gt;
&lt;li&gt;CPU-bound processes in lower priority queue long CPU burst
&lt;img src=&#34;https://i.loli.net/2019/03/18/5c8efe2739cf9.png&#34; width=&#34;350px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;multilevel feedback queue scheduler is defined by the following parameters:
&lt;ul&gt;
&lt;li&gt;Number of queues&lt;/li&gt;
&lt;li&gt;Scheduling algorithm for each queue&lt;/li&gt;
&lt;li&gt;Method used to determin when to upgrade/demote a process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;evaluation-methods&#34;&gt;Evaluation methods&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Deterministic Modeling — takes a particular predetermined workload and defines the performance of each algorithm for the workload&lt;/li&gt;
&lt;li&gt;Queueing Model — mathematical analysis&lt;/li&gt;
&lt;li&gt;Simulation — random-number generator or trace tapes for workload generation&lt;/li&gt;
&lt;li&gt;Implementation — the only completely accurate way for algorithm evaluation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mutli-processor-scheduling--multi-core-processor-scheduling--real-time-scheduling&#34;&gt;Mutli-Processor Scheduling &amp;amp; Multi-Core Processor Scheduling &amp;amp; Real-Time Scheduling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Asymmetric multiprocessing
&lt;ul&gt;
&lt;li&gt;All system activities are handled by a processor (alleviationg the need for data sharing)&lt;/li&gt;
&lt;li&gt;the other only execute user code (allocated by the master)&lt;/li&gt;
&lt;li&gt;far simple than SMP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Symmetric multiprocesiing (SMP):
&lt;ul&gt;
&lt;li&gt;each processor is self-scheduling&lt;/li&gt;
&lt;li&gt;all processor in common ready queue, or each has its own private queue of ready processes&lt;/li&gt;
&lt;li&gt;need synchronization mechanism&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Processor affinity
A process has an affinity for the processor on which it is currently running
&lt;ul&gt;
&lt;li&gt;A process populates its recent used data in cache memory of its running processor&lt;/li&gt;
&lt;li&gt;cache invalidation and repopulation has high cost&lt;/li&gt;
&lt;li&gt;Soft affinity: possible to migrate between processors&lt;/li&gt;
&lt;li&gt;hard affinity: not to migrate to other processor&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NUMA (non-uniform memory access):
&lt;ul&gt;
&lt;li&gt;Occurs in systems containing combines CPU and memory boards&lt;/li&gt;
&lt;li&gt;CPU scheduler and memory-replacement works together&lt;/li&gt;
&lt;li&gt;A process (assigned affinity to a CPU) can be allocated memory on the board where that CPU resides
&lt;img src=&#34;https://i.loli.net/2019/03/18/5c8f03b791a45.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Load-balancing
&lt;ul&gt;
&lt;li&gt;Keep the workload evently distributed across all processors, only necessary on systems where each processor has its own private queue of eligible processes to execute&lt;/li&gt;
&lt;li&gt;Push migration : move(push) processes from overloaded to idle or less-busy processor&lt;/li&gt;
&lt;li&gt;Pull migration: idle processors pulls a waiting task from a busy processor&lt;/li&gt;
&lt;li&gt;Often implemented in parallel&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-core-processor-scheduling&#34;&gt;Multi-core Processor Scheduling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Faster and consumer less power&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;memory stall&lt;/strong&gt;: When access memory, it spends a significant amount of time waiting for the data become available. (e.g. cache miss)&lt;/li&gt;
&lt;li&gt;Multi-threaded multi-core systems
&lt;ul&gt;
&lt;li&gt;Two (or more) hardware threads are assigned to each core (Intel Hyper-threading)&lt;/li&gt;
&lt;li&gt;Takes advantage of memory stall to make progress on another thread while memory retrieve happens
&lt;img src=&#34;https://i.loli.net/2019/03/18/5c8f051f97e4d.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two ways to multithread a processor:
&lt;ul&gt;
&lt;li&gt;coarse-grained: switch to another thread when a memory stall occurs. The cost is high as the instruction pipeline must be flushed&lt;/li&gt;
&lt;li&gt;fine-grained(interleaved): switch between threads at the boundary of an instruction cycle. The architecture design includes logic for thread switching &amp;ndash; cost is low&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scheduling for Multi-threaded multi-core systems
&lt;ul&gt;
&lt;li&gt;1st level: Choose which software thread to run on each hardware thread(logical processor)&lt;/li&gt;
&lt;li&gt;2nd level: How each core decides which hardware thread to run&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;real-time-cpu-scheduling&#34;&gt;Real-Time CPU Scheduling&lt;/h3&gt;
&lt;p&gt;Real-time does not mean speed, but keeping deadlines&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Soft real-time requirements, missing the deadline is unwanted , but is not immediately critical (multimedia streaming)&lt;/li&gt;
&lt;li&gt;Hard real-time requirements, Missing the deadline results in a fundamental failure (nuclear power plant controller)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;read-time-scheduling-algorithms&#34;&gt;Read-Time Scheduling Algorithms&lt;/h3&gt;
&lt;p&gt;Evaluation: Ready, Execution, Deadline&lt;/p&gt;
&lt;h4 id=&#34;rate-monotoinc-rm-algorithm&#34;&gt;Rate-Monotoinc (RM) algorithm&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;short period, higher priority (fixed-priority RTS scheduling algorithm)&lt;/li&gt;
&lt;li&gt;Ex: T1 = (4,1) (red), T2 = (5,2) (orange), T3 = (20,5)(green) (Period, Execution)&lt;/li&gt;
&lt;li&gt;priority: T_1 &amp;gt; T_2 &amp;gt; T_3
&lt;img src=&#34;https://i.loli.net/2019/03/18/5c8f086daa22a.png&#34; width=&#34;450px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;earliest-deadline-firstedf-algorithm&#34;&gt;Earliest-Deadline-First(EDF) algorithm&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Earlier deadline, higher priority (dynamic priority algorithm)&lt;/li&gt;
&lt;li&gt;Ex: T1 = (2, 0.9), T2 = (5, 2.3)
&lt;img src=&#34;https://i.loli.net/2019/03/18/5c8f09455c102.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;operating-system-examples&#34;&gt;Operating System Examples&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Solaris Scheduler&lt;/li&gt;
&lt;li&gt;Windows XP Scheduler&lt;/li&gt;
&lt;li&gt;Linux Scheduler&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Process Synchronization</title>
      <link>/courses/operating_system/process_synchronization/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/process_synchronization/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Concurrent access to shared data may result in data inconsistency&lt;/li&gt;
&lt;li&gt;Maintaining data consistency requires mechanism to ensure the orderly execution of cooperating processes&lt;/li&gt;
&lt;li&gt;Consumer &amp;amp; Producer Problem&lt;/li&gt;
&lt;li&gt;Race condition: the situation where several processes access and manipulate shared data concurrenlty. The final value of the shared data depends upon which process finishes last, commonly described as &lt;strong&gt;critical sectio&lt;/strong&gt;n problem&lt;/li&gt;
&lt;li&gt;To prevent race condition, concurrent processes must be synchronized, on a single-process machine, we could disable interrupt or use non-preemptive CPU scheduling&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;crtical-section&#34;&gt;Crtical Section&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Purpose: a protocal for processes to cooperate&lt;/li&gt;
&lt;li&gt;Probelm description:
&lt;ul&gt;
&lt;li&gt;N process are competing use some shared data&lt;/li&gt;
&lt;li&gt;Each process has a code segment, called critical selection, in which the shared data is accessed&lt;/li&gt;
&lt;li&gt;Ensure that when one process is executing in its critical section, no other process is allowed to execute in its critical selection  $\rightarrow$ &lt;strong&gt;mutually exclusive&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Critical Section Requirements
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Mutual Exclusion:&lt;/strong&gt; if process P in executing in its CS, no other processes can be executing in their CS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Progress&lt;/strong&gt;: if no process is executing in its CS and there exist some processes that wish to enter their CS, these processes cannot be postponed indefinitely&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bounded Waiting&lt;/strong&gt;: A bound must exist on the number of times that other processes are allowed to enter their CS after a process has made a request to enter its CS&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solution&#34;&gt;Solution&lt;/h2&gt;
&lt;h3 id=&#34;algorithm-for-two-processes&#34;&gt;Algorithm for two Processes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;only 2 processes, $P_0$ and $P_1$&lt;/li&gt;
&lt;li&gt;Shared variables
&lt;ul&gt;
&lt;li&gt;int turn; // initially turn = 0&lt;/li&gt;
&lt;li&gt;turn = i $\rightarrow$ $P_i$ can enter its critical section
&lt;img src=&#34;https://i.loli.net/2019/03/18/5c8f267ace3ce.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Mutual exclustion (yes); Progress (No); Bounded-Wait(Yes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;petersons-solution-for-two-processes&#34;&gt;Peterson&amp;rsquo;s solution for Two processes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;shared variables&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;int turn, initially turn =0&lt;/li&gt;
&lt;li&gt;turn = i $\rightarrow$ $P_i$ can enter its critical section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;boolean flag[2]&lt;/code&gt; // initially flag[0] = flag[1] = false&lt;/li&gt;
&lt;li&gt;&lt;code&gt;flag[I] = true&lt;/code&gt; $\rightarrow$ $P_i$ ready to enter its critical section&lt;/li&gt;
&lt;li&gt;Mutual Selection, progress, bounded waiting proof&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;do {
    flag[i] = TRUE; // Pi 是否想要进去
    turn = j; // 先把key交给对方
    while (flag[j] &amp;amp;&amp;amp; turn == j);
    // critical section
    flag[i] = FALSE;
    remainder section
} while(1);
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bakery-algorithm-n-processes&#34;&gt;Bakery Algorithm (n processes)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Before enter its CS, each process receives a number&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Holder of the smallest number enters CS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The numbering scheme always generates number in non-decreasing order; i.e. 1,2,3,3,4,5,5,5&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if processes $P_i$ and $P_j$ receive the same numbe, if $i&amp;lt;j$ then $P_i$ is served first
Bounded-waiting because processes enter CS on a First-come, First Served basis&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;// process i:
do {
  choosing[i] = TRUE;
  num[i] = max(num[0], num[1], ..., num[n-1]) + 1;
  choosing[i] = FALSE;
  for(j =0; j&amp;lt;n; j++){
    while(choosing[j]); // cannot compare when num is being modified
    while((num[j]!=0) &amp;amp;&amp;amp; ((num[j], j)) &amp;lt; (num[i],i))); // FCFS
  }
  // critical section
  num[i] = 0;
  // reminder section
}while(1);
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pthread-lockmutex-routines&#34;&gt;Pthread Lock/Mutex Routines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To use mutex, it must be declared as type &lt;code&gt;pthread_mutex_t&lt;/code&gt; and initialized with &lt;code&gt;pthread()_mutex_init()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A mutex is destoryed with &lt;code&gt;pthread_mutex_destory()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A critical selection can then be protected using &lt;code&gt;pthread_mutex_lock()&lt;/code&gt; and &lt;code&gt;pthread_mutex_unlock()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;condition-variables-cv&#34;&gt;Condition Variables (CV)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CV represent some condition that a thread can:
&lt;ul&gt;
&lt;li&gt;Wait on, until the condition occurs; or&lt;/li&gt;
&lt;li&gt;Notify other waiting threads that the condition has occured&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Three operations on condition variables:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;wait()&lt;/code&gt; — Block until another thread calls &lt;code&gt;signal()&lt;/code&gt; or &lt;code&gt;broadcast()&lt;/code&gt; on the CV&lt;/li&gt;
&lt;li&gt;&lt;code&gt;signal()&lt;/code&gt; — Wake up one thread waiting on the CV&lt;/li&gt;
&lt;li&gt;&lt;code&gt;broadcast()&lt;/code&gt; — Wake up all threads waiting on the CV&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;All condition variable operation must be performed while a mutex is locked&lt;/li&gt;
&lt;li&gt;In Pthread, CV type is pthread_cond_t
&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;pthread_cond_init()&lt;/code&gt; to initalize&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pthread_cond_wait(&amp;amp;theCV, &amp;amp;somelock)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pthread_cond_signal(&amp;amp;theCV)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pthread_cond_broadcast(&amp;amp;theCV)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Example:
&lt;ul&gt;
&lt;li&gt;A thread is designed to take action when x = 0&lt;/li&gt;
&lt;li&gt;Another thread is responsible for decrementing the counter&lt;/li&gt;
&lt;li&gt;All condition variable operation must be performed while a mutex is locked
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c94451f9362f.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Procedure
&lt;ul&gt;
&lt;li&gt;left thread
&lt;ol&gt;
&lt;li&gt;Lock mutex&lt;/li&gt;
&lt;li&gt;Wait()
&lt;ol&gt;
&lt;li&gt;Put the thread into sleep and releases the lock&lt;/li&gt;
&lt;li&gt;Waked up, but the thread is locked&lt;/li&gt;
&lt;li&gt;Re-acquire lock and resume execution&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Release the lock&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;right thread
&lt;ol&gt;
&lt;li&gt;Lock mutex&lt;/li&gt;
&lt;li&gt;Signal()&lt;/li&gt;
&lt;li&gt;Release the lock()&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;threadpool-implementation&#34;&gt;ThreadPool Implementation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;  struct threadpool_t {
    pthread_mutex_t lock;
    pthread_cond_t notify;
    pthread_t *treahd;
    threadpool_task_t *queue;
    int thread_count;
    int queue_size;
    int head;
    int tail;
    int count;
    int shutdown;
    int started;
  };

  typedef struct {
    void (*function) (void*);
    void *argument;
  } threadpool_task_t;

// allocate thread and task queue
pool-&amp;gt;threads = (pthread_t *) malloc(sizeof(pthread_t) * thread_count);
pool-&amp;gt;queue = (threadpool_task_t *) malloc(sizeof(threadpool_task_t) * queue_size);

// threadpool implementation
static void *threadpool_thread(void *threadpool)
{
  threadpool_t *pool = (threadpool_t *) threadpool;
  threadpool_task_t task;

  for(;;){
    // lock must be taken to wait on conditional varaibl
    pthread_mutex_lock(&amp;amp;(pool-&amp;gt;lock));
    while((pool-&amp;gt;count=0) &amp;amp;&amp;amp; (!pool-&amp;gt;shutdown)) {
      pthread_cond_wait(&amp;amp;(pool-&amp;gt;notify), &amp;amp;(pool-&amp;gt;lock));
      task.function = pool-&amp;gt;queue[pool-&amp;gt;head].function;
      task.argument = poll-&amp;gt;queue[pool-&amp;gt;head].argument;
      pool-&amp;gt;head +=1;
      pool-&amp;gt;head = (pool-&amp;gt;head == poll-&amp;gt;queue_size) ? 0 : pool-&amp;gt;head;
      pool-&amp;gt;count -=1;

      pthread_mutex_unlock(&amp;amp;(pool-&amp;gt;lock));
      (*(task.function))(task.argument);
    }
  }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;hardware-support&#34;&gt;Hardware Support&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The CS problem occurs because the modification of a shared variable may be interrupted&lt;/li&gt;
&lt;li&gt;If disable interrupts when in CS,  not feasible in multiprocessor machine, clock interrupts cannot fire in any machine&lt;/li&gt;
&lt;li&gt;HW support solution: atomic instruction
&lt;ul&gt;
&lt;li&gt;atomic: as one uninterruptible unit&lt;/li&gt;
&lt;li&gt;Examples: TestAndSet(var), Swap(a, b)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;atomic-testandset&#34;&gt;Atomic TestAndSet()&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;booelean TestAndSet(bool &amp;amp;lock) {
  bool value = lock;
  lock = TRUE; // return the value of &amp;quot;lock&amp;quot; and set &amp;quot;lock&amp;quot; to ture
  return value;
}
&lt;/code&gt;&lt;/pre&gt;
  &lt;img src=&#34;https://i.loli.net/2019/03/18/5c8f386e68d10.png&#34; width=&#34;400px&#34;/&gt;
 Mutual Exclusion (Yes), Progress(Yes), Bounded-Wait(No)
&lt;h4 id=&#34;atomic-swap&#34;&gt;Atomic Swap()&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Idea: enter CS if &lt;code&gt;lock=false&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Shared data: boolean lock; //initially &lt;code&gt;lock=FALSE&lt;/code&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/18/5c8f398d3e9f6.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;semaphores&#34;&gt;Semaphores&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A tool to generalize the synchronization problem (easier to solve, but no guarantee for correctness)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A record of how many units of a particular resource are available&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if #record = 1 $\rightarrow$ binary semaphore, mutex lock&lt;/li&gt;
&lt;li&gt;if #record &amp;gt; 1 $\rightarrow$ counting semaphore&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accessed only through 2 &lt;strong&gt;atomic&lt;/strong&gt; ops: &lt;code&gt;wait&lt;/code&gt; &amp;amp; &lt;code&gt;signal&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spinlock&lt;/strong&gt; implementation (浪费CPU资源)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;wait(S) {
  while (S&amp;lt;=0);
  S--;
}
signal(S){
  S++;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;POSIX Semaphore (OS support)
Semaphore is part of POSIX standard but it is not belong to Pthread, it can be used with or without thread&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;POSIX Semaphore routines&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sem_init(sem_t *sem, int pshared, unsigned int value)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sem_wait(sem_t *sem)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sem_post(sem_t *sem)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sem_getvalue(sem_t *sem, int *valptr)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sem_destory(sem_t *sem)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;  #include&amp;lt;semaphore.h&amp;gt;
  sem_t sem;
  sem_init(&amp;amp;sem);
  sem_wait(&amp;amp;sem);
// critical section
  sem_post(&amp;amp;sem);
  sem_destory(&amp;amp;sem);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;n-process-critical-section-problem&#34;&gt;n-Process Critical Section Problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;shared data: semaphore mutex; // initially mutex = 1&lt;/li&gt;
&lt;li&gt;Process Pi&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;  do {
    wait(mutex); // pthread_mutex_lock(&amp;amp;mutex)
      critical section
    signal(mutex); // pthread_mutex_unlock(&amp;amp;mutex)
      remainder section
  } while(1);
  Progress? Yes
  Bounded waiting? Depends on the implementation of `wait()`
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;non-busy-waiting-implementation&#34;&gt;Non-busy waiting Implementation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Semaphore is data struct with a queue, may be any queuing strategy&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;  typedef struct {
    int value; // init to 0
    struct process *L // queue
  } semaphore
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;wait() and signal()&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use system calls: &lt;code&gt;block()&lt;/code&gt; and &lt;code&gt;wakeup()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;must be executed atomically&lt;/li&gt;
&lt;li&gt;Ensure atomic wait &amp;amp; signal ops?
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Single-process: disable interrupts&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multi-processor: 1. HW support 2. SW solution(Peterson&amp;rsquo;s solution, Bakery algorithm)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;void wait(semaphore S)
{
  S.value--;
  if(S.value &amp;lt; 0){
    add this process to S.L
    sleep();
  }
}

void signal(semaphore S) {
  S.value++;
  if(S.value&amp;lt;=0){
    remove a process P from S.L;
    wakeup(P);
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;semaphore-with-critical-section&#34;&gt;Semaphore with Critical Section&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c9460c709cdd.png&#34; width=&#34;400px&#34;/&gt;
&lt;h2 id=&#34;classical-synchronization-problems&#34;&gt;Classical Synchronization Problems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Purpose: Used for testing newly proposed synchronization scheme&lt;/li&gt;
&lt;li&gt;Bounded-Buffer (Producer-Consumer)&lt;/li&gt;
&lt;li&gt;Reader-Writer Problem&lt;/li&gt;
&lt;li&gt;Dining-Philosopher Problem&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bounded-buffer-problem&#34;&gt;Bounded-Buffer Problem&lt;/h3&gt;
&lt;p&gt;A poof of n buffers, each capable of holding one item&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Producer:
&lt;ul&gt;
&lt;li&gt;Grad an empty buffer&lt;/li&gt;
&lt;li&gt;place an item into the buffer&lt;/li&gt;
&lt;li&gt;waits if no empty buffer is available&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Consumer
&lt;ul&gt;
&lt;li&gt;grab a buffer and retracts the item&lt;/li&gt;
&lt;li&gt;place the buffer back to the free poll&lt;/li&gt;
&lt;li&gt;waits if all buffers are empty&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;readers-writers-problem&#34;&gt;Readers-Writers Problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A set of shared data objects&lt;/li&gt;
&lt;li&gt;A group of processes (reader processes, writer processes, a writer process has exclusive access to a shared object)&lt;/li&gt;
&lt;li&gt;Different variations involving priority
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;first RW problem&lt;/strong&gt;: no reader will be kept waiting unless a writer is updating a shared object (writer会starvation)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;second RW problem&lt;/strong&gt;: once a writer is ready, it performs the updates as soon as the shared object is released (writer has higher priority than reader; once a writer is ready, no new reader may start reading)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;First Reader-Writer Algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;  // mutual exclustion for write
  semaphore wrt = 1;
  semaphore mutex = 1;
  int readcount = 0;

  Writer() {
    while(TRUE) {
      wait(wrt);
      // Writer Code
      signal(wrt);
    }
  }

  Reader() {
    while(TRUE) {
      wait(mutex);
        readcount++;
        if(readcount==1) // 之后的 Reader 不需要拿lock
          wait(wrt);
      signal(mutex);
      // Reader Code
      wait(mutex);
        readcount--;
        if(readcount == 0)
          signal(wrt);
      signal(mutex);
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dining-philosopher-problem&#34;&gt;Dining-Philosopher Problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;5 person sitting on 5 chairs with 5 chopsticks&lt;/li&gt;
&lt;li&gt;A person is either thinking or eating
&lt;ul&gt;
&lt;li&gt;thinking: no interaction with the rest 4 persons&lt;/li&gt;
&lt;li&gt;eating: need 2 chopsticks at hand&lt;/li&gt;
&lt;li&gt;a person picks up 1 chopsticks at a time&lt;/li&gt;
&lt;li&gt;done eating: put down both chopsticks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;deadlock problem
&lt;ul&gt;
&lt;li&gt;one chopstick as one semaphore&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;starvation problem&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;monitors-a-high-level-language-construct&#34;&gt;Monitors (A high-level language construct)&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;Although semaphores provide a convenient and effective synchronization mechanism, its correctness is depending on the programmer&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All processes access a shared data object must execute &lt;code&gt;wait()&lt;/code&gt; and &lt;code&gt;signal()&lt;/code&gt; in the right order and right place&lt;/li&gt;
&lt;li&gt;This may not be true because honest programming error or uncooperative programmer&lt;/li&gt;
&lt;li&gt;The representation of a monitor type consists of declarations of variables whose values define the state of an instance of the type and the functions(procedures) that implement operations on the type&lt;/li&gt;
&lt;li&gt;The monitor type is similar to a class in O.O. language
&lt;ul&gt;
&lt;li&gt;A procedure within a monitor can access only local variables and the formal parameters&lt;/li&gt;
&lt;li&gt;The local variables of a monitor can be used only by the local procedures&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The monitor ensures that only one process at a time can be &lt;strong&gt;active&lt;/strong&gt; within the monitor
&lt;div class=&#34;note info&#34;&gt;&lt;p&gt; Similar idea is incorporated to many programming language (concurrent pascal, C#, and Java) &lt;/p&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;monitor-introduction&#34;&gt;Monitor introduction&lt;/h3&gt;
&lt;p&gt;High-level synchronization construct that allows the safe sharing of an abstract data type among concurrent processes
&lt;img src=&#34;https://i.loli.net/2019/03/19/5c90db44d3f69.png&#34; width=&#34;400px&#34;/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitor Condition Variables
&lt;ul&gt;
&lt;li&gt;To allow a process to wait within the monitor, a condition variable must be declared as &lt;code&gt;condtion x, y;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Condition variable can only be used with the operations &lt;code&gt;wait()&lt;/code&gt; and &lt;code&gt;signal()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wait()&lt;/code&gt; means that the process invoking this operation is suspended until another process invokes&lt;/li&gt;
&lt;li&gt;&lt;code&gt;signal()&lt;/code&gt; resumes exactly one suspended process. If no process is suspended, the signal operation &lt;strong&gt;has no effect&lt;/strong&gt; (in contrast, signal always change the state of semaphore)
&lt;img src=&#34;https://i.loli.net/2019/03/19/5c90dd71b3c01.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dining-philosophers-example&#34;&gt;Dining-Philosophers Example&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;monitor dp {
  enum {thinking, hungry, eating} state[5]; //current state
  condition self[5]; // delay eating if can&#39;t obtain chopsticks
  void pickup(int i) // pickup chopsticks
  void putdown (int i) // putdown chopsticks
  void test (int i) // try to eat
  void init(){
    for (int i =0; i&amp;lt;5; i++)
      state[i] = thinking;
  }
}

void pickup(int i) {
  state[i] = hungry;
  test(i);
  if (state[i] != eating)
    self[i].wait(); // wait to eat
}

void putdown(int i) {
  state[i] = thinking;
  // check if neighbors are waiting to eat
  test((i+4) % 5);
  test((i+1) % 5);
}

void test(int i) {
  if ((state[(i+4) % 5] != eating) &amp;amp;&amp;amp; (state[(i+1) % 5] != eating) &amp;amp;&amp;amp; (state[i] == hungry)) {
    state[i] = eating;
    self[i].signal(); // if Pi is suspended, resume it, if Pi is not suspended, no effect
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;synchronized-tools-in-java&#34;&gt;Synchronized Tools in JAVA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Synchronized Methods(Monitor)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Synchronized method uses the method receiver as a lock&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two invocations of synchronized methods cannot interleave on the same object&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When one thread is executing a synchronized method for a object, all other threads that invoke synchronized methods the same object block until the first thread exist the object&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;    public class SynchronizedCounter {
        private int c= 0;
        public synchronized void increment() {c++;}
        public synchronized void decrement() {c--;}
        public synchronized int value() {return c;}
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Synchronized Statement (Mutex Lock)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Synchronized blocks uses the expression as a lock&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A synchronized statement can be only be executed once the thread has obtained a lock for the object or the class that has been referred to in the statement&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;useful for improving concurrency with fine-grained&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void run(){
    synchronized(p1)
    {
        int i = 10; // statement without locking requirement
        p1.display(s1);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;atomic-transactions&#34;&gt;Atomic Transactions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Transactions: a collection of instructions (or instructions) that performs a single logic function&lt;/li&gt;
&lt;li&gt;Atomic Transactions: Operations happen as a single logical unit of work, in its entirely, or not at all&lt;/li&gt;
&lt;li&gt;Atomic translation is particular a concern for database system (Strong interest to use DB techniques in OS)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;file-io-example&#34;&gt;File I/O example&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Transaction is a series of read and write operations&lt;/li&gt;
&lt;li&gt;Terminated by commit (transaction successful) or abort (transaction failed) operation&lt;/li&gt;
&lt;li&gt;Aborted transaction must be rolled back to undo any changes it performed (it is part of )&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;log-based-recovery&#34;&gt;Log-Based Recovery&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Record to stable storage information about all modifications by a transaction&lt;/li&gt;
&lt;li&gt;Write-ahead logging: Each log record describes single transaction write operation
&lt;ul&gt;
&lt;li&gt;Transaction time&lt;/li&gt;
&lt;li&gt;Data item name&lt;/li&gt;
&lt;li&gt;Old &amp;amp; new values&lt;/li&gt;
&lt;li&gt;Special Events: &amp;lt;$T_i$, start&amp;gt;, &amp;lt;$T_i$, commmits&amp;gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Log is used to reconstruct the state of the data items modified by the transactions&lt;/li&gt;
&lt;li&gt;Checkpoints
&lt;ul&gt;
&lt;li&gt;when faiulre occurs, must consult the log to determine which transactions must be re-done, searching process is time consuming and redone may not be necessary for all transactions&lt;/li&gt;
&lt;li&gt;use checkpoints to reduce the above overhead, output all log records, modified data, log record &lt;checkpoint&gt; to stable storage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>DeadLocks</title>
      <link>/courses/operating_system/deadlocks/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/deadlocks/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A set of blocked process each holding some resources and waiting to acquire a resource held by another process in the set&lt;/li&gt;
&lt;li&gt;Ex1: 2 processes and 2 tape dirves, each process holds a tape drive, each process requests another tape drive&lt;/li&gt;
&lt;li&gt;Ex2: 2 processes, and semaphores A &amp;amp; B, P1(hold B, wait A): wait(A), signal(B), P2(hold A, wait B): wait(B), signal(A)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;necessary-conditions&#34;&gt;Necessary Conditions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mutual exclustion (only 1 process at a time can use a resource)&lt;/li&gt;
&lt;li&gt;Hold &amp;amp; Wait: a process holding some resources and is waiting for another resource&lt;/li&gt;
&lt;li&gt;No preemption: a resource can be only released by a process voluntarily&lt;/li&gt;
&lt;li&gt;Circular wait: there exist a set {$P_0, P_1, &amp;hellip;, P_n$} of waiting process such that $P_0 \rightarrow P_1 \rightarrow P_2, &amp;hellip; , P_0$&lt;/li&gt;
&lt;li&gt;All four conditions must hold for possible deadlock
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c9194d343ffc.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-model&#34;&gt;System Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Resources type $R_1, R_2, &amp;hellip; , R_m$ E.g. CPU, memory pages, I/O devices&lt;/li&gt;
&lt;li&gt;Each resource type $R_i$ has $W_i$ instances, E.g. a computer has 2 CPUs&lt;/li&gt;
&lt;li&gt;Each process utilizes a resouce as follows:
Request $\rightarrow$ use $\rightarrow$ release&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;resource-alocation-graph&#34;&gt;Resource-Alocation Graph&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;3 processes, P1-P3&lt;/li&gt;
&lt;li&gt;4 resources, R1-R4 (the black dot represent the number of instance)&lt;/li&gt;
&lt;li&gt;Request edges: P1$\rightarrow$ R1: P1 requests R1&lt;/li&gt;
&lt;li&gt;Assignment edges: R2$\rightarrow$ P1: one instance of R2 is allocated to P1&lt;/li&gt;
&lt;li&gt;P1 is hold on an instance of R2 and waiting for an instance of R1
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c9196704420b.png&#34; width=&#34;200px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;if the graph consists a cycle, a deadlock may  exist&lt;/li&gt;
&lt;li&gt;deadlock
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c919780cc361.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;no deadlock
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c9197d43b55e.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;If graph contains no cycle $\rightarrow$ no deadlock&lt;/li&gt;
&lt;li&gt;If graph contains a cype:
&lt;ul&gt;
&lt;li&gt;if one instance per resource type $\rightarrow$ deadlock&lt;/li&gt;
&lt;li&gt;if multiple instances per resource type $\rightarrow$ possibility of deadlock&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;handing-deadlocks&#34;&gt;Handing Deadlocks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ensure the system will never a deadlock state
&lt;ul&gt;
&lt;li&gt;deadlock preventation: ensure that at least one of the 4 necessary conditions cannot hold&lt;/li&gt;
&lt;li&gt;deadlock avoidance: dynamically examines the resource-allocation state before allocation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Allow to enter a deadlock state and then recover
&lt;ul&gt;
&lt;li&gt;deadlock detection&lt;/li&gt;
&lt;li&gt;dedlock recovery&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ignore the problem and pretend that deadlocks never occur in the system (used by most operating systems, including UNIX)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deadlock-prevention&#34;&gt;Deadlock Prevention&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mutual Exclustion(ME): do not require ME on sharable resources
&lt;ul&gt;
&lt;li&gt;e.g. there is no needto ensure ME on read-only files&lt;/li&gt;
&lt;li&gt;Some resources are not shareable, however (e.g. printer)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hold &amp;amp; Wait:
&lt;ul&gt;
&lt;li&gt;When a process requests a resource, it does not hold any resource&lt;/li&gt;
&lt;li&gt;Pre-allocate all resources before executing (resource utilization is low, starvation is possible)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;No preemption
&lt;ul&gt;
&lt;li&gt;When a process is waiting on a resource, all its holding resources are preempted (e.g. P1 request R1, which is allocated P2, which in turn is waiting on R2, R1 can be preempted and reallocated to P1)&lt;/li&gt;
&lt;li&gt;Applied to resources whose states can be easily saved and restored later (e.g. CPU registers &amp;amp; memory)&lt;/li&gt;
&lt;li&gt;It cannot easily be applied to other resources(e.g. printers &amp;amp; tape drives)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Circular wait
&lt;ul&gt;
&lt;li&gt;impose a total ordering of all resources types&lt;/li&gt;
&lt;li&gt;A process requests resources in an increasing order
&lt;ul&gt;
&lt;li&gt;Let $R = {R_0, R_1, &amp;hellip; , R_N}$ be the set of resource types&lt;/li&gt;
&lt;li&gt;When request $R_k$, should release all $R_i$, $i \geq k$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;avoidance-algortihm&#34;&gt;Avoidance Algortihm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;safe state: a system is in a safe state if there exists a sequence of allocations to satisfy requests by all processes (this sequence of allocations is called safe sequence)&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91a399d222f.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;Single instance of resource type (resource-allocation graph (RAG) algorithm based on circle detection)&lt;/li&gt;
&lt;li&gt;Multiple instance of resource type
banker&amp;rsquo;s algorithm based on safe sequence detection&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;resource-allocation-graph-rag-algorithm&#34;&gt;Resource-Allocation Graph (RAG) Algorithm&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Request edge&lt;/li&gt;
&lt;li&gt;Assignment edge&lt;/li&gt;
&lt;li&gt;Claim  edge: $P_i \rightarrow R_j$, process $P_i$ may request $R_j$ in the future&lt;/li&gt;
&lt;li&gt;Clain edge converts to request edge (when a resource is requested by process)&lt;/li&gt;
&lt;li&gt;Assignment edge converts back to claim edge (when a resource is released by a process)&lt;/li&gt;
&lt;li&gt;Resources must be claimed a priori in the system&lt;/li&gt;
&lt;li&gt;Grant a request only if no cycle created&lt;/li&gt;
&lt;li&gt;Check for safety using a &lt;strong&gt;cycle-detection algorithm, $O(n^2)$&lt;/strong&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c94686f2a7e4.png&#34; width=&#34;250px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;R2 cannot be allocated to P2&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;bankers-algorithm&#34;&gt;Banker&amp;rsquo;s algorithm&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Safe State with Safe Sequnce&lt;/li&gt;
&lt;li&gt;use for multiple instance of each resource type&lt;/li&gt;
&lt;li&gt;use a general safety algorithm to pre-determine if any safe sequence exists after allocation&lt;/li&gt;
&lt;li&gt;only proceed the allocation if safe sequence exists&lt;/li&gt;
&lt;li&gt;Procedures:
&lt;ol&gt;
&lt;li&gt;Assume processes need maximum resources&lt;/li&gt;
&lt;li&gt;Find a process that can be satisfied by free resources&lt;/li&gt;
&lt;li&gt;Free the resource usage of the process&lt;/li&gt;
&lt;li&gt;repeat to step 2 until all processes are satisfied&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Example
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91a8e8527a0.png&#34; width=&#34;200px&#34;/&gt;
Safe sequence: $P_1, P_3, P_4, P_2, P_0$
&lt;ul&gt;
&lt;li&gt;if Request (P1) = (1,0,2): P1 allocation $\rightarrow$ 3, 0, 2 (Safe sequence: $P_1, P_3, P_4, P_0, P_2$)&lt;/li&gt;
&lt;li&gt;if request (P4) = (3,3,0): P4 allocation  $\rightarrow$ 3,3,2 (no safe sequence can be found)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deadlock-detection&#34;&gt;Deadlock Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Single instance of each resource type
&lt;ul&gt;
&lt;li&gt;convert request/assignment edges into wait-for graph&lt;/li&gt;
&lt;li&gt;deadlock exists if there is a cycle in the wait-for graph
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91aa599dca0.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multiple-Instance for each Resource type
Total instances: A(7), B(2), C(6) (Request 表示已经发生)
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91aae734f68.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;The system is in a safe state $\rightarrow$ &amp;lt;P_0, P_2, P_3, P_1, P_4&amp;gt; (no deadlock)&lt;/li&gt;
&lt;li&gt;If P2 request = &amp;lt;0, 0, 1&amp;gt; $\rightarrow$ no safe sequence can be found $\rightarrow$ the system is deadlocked&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deadlock-recovery&#34;&gt;Deadlock Recovery&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;process termination
&lt;ul&gt;
&lt;li&gt;abort all deadlocked processes&lt;/li&gt;
&lt;li&gt;abort 1 process at a time until the deadlock cyle is eliminated (which process should we abort first)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Resource preemption
&lt;ul&gt;
&lt;li&gt;select a victim: which one to preempt&lt;/li&gt;
&lt;li&gt;rollback: partial rollback or total rollback?&lt;/li&gt;
&lt;li&gt;starvation: can the same process be preempted always?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>File System Interface</title>
      <link>/courses/operating_system/file_system_interface/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/file_system_interface/</guid>
      <description>&lt;h2 id=&#34;file-concept&#34;&gt;File Concept&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A logical storage unit created by OS (v.s. physical storage unit in disk (sector, track))
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91ae7d9b269.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;file attributes (identifier, Name, type, Location, Size, protection, Last-access time, last-updated time)&lt;/li&gt;
&lt;li&gt;file operations (creating, reading, writing, repositioning, deleting, truncating)&lt;/li&gt;
&lt;li&gt;file types: .exe .com .obj .cc .mov (Hint for OS to operate file in a resonable way)&lt;/li&gt;
&lt;li&gt;Process: open-file table&lt;/li&gt;
&lt;li&gt;OS: system-wide table (process shared)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;open-file-tables--system-wide-table&#34;&gt;Open-File Tables &amp;amp; System-wide table&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Per-process table
&lt;ul&gt;
&lt;li&gt;Tracking all files opened by this process&lt;/li&gt;
&lt;li&gt;Current file pointer for each opened file&lt;/li&gt;
&lt;li&gt;Access rights and accounting information&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;System-wide table
&lt;ul&gt;
&lt;li&gt;Each entry in the per-process table points to this table&lt;/li&gt;
&lt;li&gt;Process-independent information such as disk location, access dates, file size
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91b0ff4221e.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;access-methods&#34;&gt;Access Methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Sequential access
&lt;ul&gt;
&lt;li&gt;Read/write next (block)&lt;/li&gt;
&lt;li&gt;Reset: repositoning the file pointer to the beginning&lt;/li&gt;
&lt;li&gt;Skip/rewind n records&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Direct(relative) access
&lt;ul&gt;
&lt;li&gt;Access an element at an arbitrary positin in a sequence&lt;/li&gt;
&lt;li&gt;File operation include the block # as parameter&lt;/li&gt;
&lt;li&gt;Often use random access to refer the access pattern from direct access
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91cb4992681.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Index Access Methods
&lt;ul&gt;
&lt;li&gt;Index: contains pointers to blocks of a file&lt;/li&gt;
&lt;li&gt;To find a record in a file (search the index file $\rightarrow$ find the pointer, use the pointer to directly access the record)&lt;/li&gt;
&lt;li&gt;with a large file $\rightarrow$ index could become too large
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91cbfd09552.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;directory-structure&#34;&gt;Directory Structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Partition (formatted or raw)
&lt;ul&gt;
&lt;li&gt;raw partiton (no file system): UNIX swap space, database&lt;/li&gt;
&lt;li&gt;Formatted partition with file system is called volume&lt;/li&gt;
&lt;li&gt;a partition can be a portion of a disk or group of multiple disks (distributed system)&lt;/li&gt;
&lt;li&gt;some storage devices (e.g.: floopy disk) dose not and cannot have partition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Directories are used by file system to store the information about the files in the partition
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91cdf01ff41.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Directory Operations (search, create, delete, list, rename, traverse)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;single-level-directory&#34;&gt;Single-Level Directory&lt;/h3&gt;
&lt;p&gt;All files in one directory, filename has to be unique, poor efficiency in locating a file as number of file increases
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91ceedaf6f6.png&#34; width=&#34;400px&#34;/&gt;&lt;/p&gt;
&lt;h3 id=&#34;two-level-dicectory&#34;&gt;Two-Level Dicectory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;a separate dir for each user&lt;/li&gt;
&lt;li&gt;path = user name + file name&lt;/li&gt;
&lt;li&gt;single-level dir problems still exist per user
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91cf80d477b.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tree-structured-directory&#34;&gt;Tree-Structured Directory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Absolute path: starting from the root&lt;/li&gt;
&lt;li&gt;Relative path: starting from a directory&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;acyclic-graph-directory&#34;&gt;Acyclic-Graph Directory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;use links to share files or directories (symbolic link &lt;code&gt;ln&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a file can have multiple absolute paths&lt;/li&gt;
&lt;li&gt;when dose a file actually get deleted?
&lt;ul&gt;
&lt;li&gt;deleting the link but not the file&lt;/li&gt;
&lt;li&gt;deleting the file but leaves the link $\rightarrow$ dangling pointer
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91d075632c0.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;general-graph-directory&#34;&gt;General-Graph Directory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;May contain cycles
&lt;ul&gt;
&lt;li&gt;Reference count dose not work any more (e.g. self-reference file)&lt;/li&gt;
&lt;li&gt;How can we deal with cycles? (Garbage collection)&lt;/li&gt;
&lt;li&gt;First pass traverses the entire graph and marks accessible files or directories&lt;/li&gt;
&lt;li&gt;second pass collect and free everything that is un-marked&lt;/li&gt;
&lt;li&gt;poor performance on millions of files&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Use cycle-detection algorithm when a link is created&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;file-system-mounting--file-sharing&#34;&gt;File-System Mounting &amp;amp; File sharing&lt;/h2&gt;
&lt;h3 id=&#34;file-system-mounting&#34;&gt;File-System Mounting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A file system must be mounted before it can be accessed&lt;/li&gt;
&lt;li&gt;Mount point: the root path that a FS will be mounted to&lt;/li&gt;
&lt;li&gt;Mount timing: boot time, automatically at run-time, manually at run time
&lt;div class=&#34;note info&#34;&gt;&lt;p&gt; mount -t type deivce dir (mount -t ext2 /dev/sda0 /mnt) &lt;/p&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;file-sharing-on-multiple-users&#34;&gt;File sharing On Multiple Users&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Each user: (userID, groupID)
&lt;ul&gt;
&lt;li&gt;ID is associated with every ops/process/thread/ the user issues&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;each file has 3 sets of attributes (owner, group, others)&lt;/li&gt;
&lt;li&gt;Owner attributes describe the privileges for the owner of the file
&lt;ul&gt;
&lt;li&gt;same for group/others attributes&lt;/li&gt;
&lt;li&gt;group/others attributes are set by owner or root&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;access-control-list&#34;&gt;Access-Control List&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We can create an access-control list (ACL) for each user
&lt;ul&gt;
&lt;li&gt;check requested file access against ACL&lt;/li&gt;
&lt;li&gt;problem: unlimited # of users&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3 classes of users $\rightarrow$ 3 ACL (RWX) for each file
&lt;ul&gt;
&lt;li&gt;owner (e.g 7 = RWX = 111)&lt;/li&gt;
&lt;li&gt;group (e.g. 6 = RWX = 110)&lt;/li&gt;
&lt;li&gt;others (e.g. 4 = RWX = 100)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;file-protection&#34;&gt;File Protection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;File owner/creator should be able to control (what can be done, by whom, access control list(ACL))&lt;/li&gt;
&lt;li&gt;file should be kept from (
&lt;ul&gt;
&lt;li&gt;physical damage (reliability): RAID&lt;/li&gt;
&lt;li&gt;improper access (protection): i.e. password&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>File System Implementation</title>
      <link>/courses/operating_system/file_system_implementatin/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/file_system_implementatin/</guid>
      <description>&lt;h2 id=&#34;file-system-structure&#34;&gt;File-System Structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I/O transfer between memory and disk are performed in units of blocks
&lt;ul&gt;
&lt;li&gt;one block is one or more sectors&lt;/li&gt;
&lt;li&gt;one sector is usually 512 bytes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;One OS can support more than 1 FS types (NTFS, FAT32)&lt;/li&gt;
&lt;li&gt;Two design problems in FS
&lt;ul&gt;
&lt;li&gt;interface to user programs&lt;/li&gt;
&lt;li&gt;interface to physical storage (disk)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Layered File System
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91d9f708bcb.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-structure&#34;&gt;Data Structure&lt;/h2&gt;
&lt;h3 id=&#34;on-disk-structure&#34;&gt;On-Disk Structure&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Boot control block (&lt;strong&gt;per partition&lt;/strong&gt;): information needed to boot an OS from that partition
&lt;ul&gt;
&lt;li&gt;typical the first block of the partition (empty means no OS)&lt;/li&gt;
&lt;li&gt;UFS (Unix File Sys): boot block, NTFS: partition boot sector&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Partition Control Block (&lt;strong&gt;per partion&lt;/strong&gt;): partion details
&lt;ul&gt;
&lt;li&gt;details: # of blocks, block size, free-block-size, free FCB pointers&lt;/li&gt;
&lt;li&gt;USF: superblock, NTFS: Master File Table&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;File control block (&lt;strong&gt;per file&lt;/strong&gt;): details regrading a file
&lt;ul&gt;
&lt;li&gt;details: permission, size, location of data blocks&lt;/li&gt;
&lt;li&gt;UFS: inode, NTFS: stored in MFT(relational database)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Directory structure (&lt;strong&gt;pef file system&lt;/strong&gt;): organize files
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91e69eb521e.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;in-memory-structure&#34;&gt;In-Memory Structure&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in-memory partition table: information about each &lt;strong&gt;mounted parition&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;in-memory directory structure: information of &lt;strong&gt;recently accessed directories&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;system-wide open-file table: contain a copy of each &lt;strong&gt;opened file&amp;rsquo;s FCB (file control block)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;per-process open-file table: &lt;strong&gt;pointer (file handler / descriptor)&lt;/strong&gt; to the corresponding entry in the above table&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91e9048c086.png&#34; width=&#34;400px&#34;/&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91e976881de.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;create
&lt;ol&gt;
&lt;li&gt;OS allocates a new FCB&lt;/li&gt;
&lt;li&gt;update directory structure
&lt;ol&gt;
&lt;li&gt;OS reads in the corresponding directory&lt;/li&gt;
&lt;li&gt;Updates the dir structure with the new file name and the FCB&lt;/li&gt;
&lt;li&gt;(After file being closed), OS writes back the directory structure back to disk&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;the file appears in user&amp;rsquo;s dir command&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;virtual-file-system&#34;&gt;Virtual File System&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;VFS provides an object-oriented way of implementing file systems&lt;/li&gt;
&lt;li&gt;VFS allows the same system call interface to be used for different types of FS&lt;/li&gt;
&lt;li&gt;VFS calls the appropriate FS routines based on the partition info
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c91eadf85fd1.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Four main object types defined by Linux VFS:
&lt;ul&gt;
&lt;li&gt;inode (an individual file, file control block)&lt;/li&gt;
&lt;li&gt;file object (an open file)&lt;/li&gt;
&lt;li&gt;superblock object (an entire file system)&lt;/li&gt;
&lt;li&gt;dentry object (an individual directory entry)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;VSF defines a set of operations that must be implemented (e.g. for file object)
&lt;ul&gt;
&lt;li&gt;int open(&amp;hellip;) (open a file)&lt;/li&gt;
&lt;li&gt;ssize_t read() (read from a file)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Directory implementation
&lt;ul&gt;
&lt;li&gt;Linear Lists (list of names with pointers to data blocks, easy to program but poor performance)&lt;/li&gt;
&lt;li&gt;Hash table &amp;ndash; linear list w/hash data structure, constant time for searching, linked list for collosions on a hash entry, hash table usually has fixed number of entires&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;allocation-methods&#34;&gt;Allocation Methods&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An allocation method refers to how disk blocks are allocated for files&lt;/li&gt;
&lt;li&gt;Allocation strategy: Contiguous allocation, Linked allocation, Indexed allocation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contiguous-allocation&#34;&gt;Contiguous Allocation&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c921846bbb60.png&#34; width=&#34;400px&#34;/&gt;
- Each file occupies a set of contiguous blocks
  - num of disk seeks is minimized
  - The dir entry for each file = (starting num, size)
- Both sequential &amp; random access can be implemented efficiently
- Problems
  - External fragmentation $\rightarrow$ compaction
  - file cannot grow $\rightarrow$ extend-based FS
&lt;h4 id=&#34;extent-based-file-system&#34;&gt;Extent-Based File System&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Many newer file system use a modified contiguous allocation scheme&lt;/li&gt;
&lt;li&gt;Extent-based file system allocate disk blocks in extents&lt;/li&gt;
&lt;li&gt;An extent is a contiguous blocks of disks
&lt;ul&gt;
&lt;li&gt;A file contains one or more extents&lt;/li&gt;
&lt;li&gt;An extent: (starting block num, length, pointer to next extent)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Random access become more costly&lt;/li&gt;
&lt;li&gt;Both internal &amp;amp; external fragmentation are possible&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;linked-allocation&#34;&gt;Linked Allocation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;each file is a linked list of blocks
&lt;ul&gt;
&lt;li&gt;each block contains a pointer to the next block&lt;/li&gt;
&lt;li&gt;data portion: block size &amp;ndash; pointer size&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;file read: following through the list
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c921a6e40243.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Problems
&lt;ul&gt;
&lt;li&gt;only good for sequential-access files&lt;/li&gt;
&lt;li&gt;random access requires traversing through the link list&lt;/li&gt;
&lt;li&gt;each access to link listis a disk I/O (link pointer is stored inside the data block)&lt;/li&gt;
&lt;li&gt;space required for pointer (4/512 = 0.78%) (solution: cluster of blocks)&lt;/li&gt;
&lt;li&gt;Reliability (one missing link breaks the whole file)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;fat-file-allocation-table-file-system&#34;&gt;FAT (File Allocation Table) file system&lt;/h4&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c921c3220923.png&#34; width=&#34;400px&#34;/&gt;
- FAT32
  - store all links in a table
  - 32 bits per table entry
  - located in a section of disk at the beginning of each partition
- FAT(table) is often cached in memory
  - Random access is improved
  - Disk head find the location of any block by reading FAT
&lt;h3 id=&#34;index-allocation-example&#34;&gt;Index Allocation Example&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The directory contains the address of the file index block&lt;/li&gt;
&lt;li&gt;each file has its own index block&lt;/li&gt;
&lt;li&gt;index block stores block # for file data
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c921c3220923.png&#34; width=400px/&gt;&lt;/li&gt;
&lt;li&gt;Bring all the pointers together into one location: the index block (one for each file)&lt;/li&gt;
&lt;li&gt;Implement direct and random access efficiently&lt;/li&gt;
&lt;li&gt;No external fragmentation&lt;/li&gt;
&lt;li&gt;Easy to create file (no allocation problem)&lt;/li&gt;
&lt;li&gt;Disadvantages
&lt;ol&gt;
&lt;li&gt;space for index blocks&lt;/li&gt;
&lt;li&gt;how large the index block should be:
&lt;ul&gt;
&lt;li&gt;linked scheme&lt;/li&gt;
&lt;li&gt;multilevel index&lt;/li&gt;
&lt;li&gt;combined scheme (inode in BSD UNIX)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;linked-indexed-scheme&#34;&gt;Linked Indexed Scheme&lt;/h4&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c9220756eea4.png&#34; width=&#34;400px&#34;/&gt;
&lt;h4 id=&#34;multilevel-scheme-two-level&#34;&gt;Multilevel Scheme (two-level)&lt;/h4&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c92213d6c2a5.png&#34; width=&#34;400px&#34;/&gt;
&lt;h4 id=&#34;combined-scheme-unix-inode&#34;&gt;combined scheme: unix inode&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;File pointer: 4B (32 bits)&lt;/li&gt;
&lt;li&gt;Let each data/index block be 4KB
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c92220210861.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;free-space&#34;&gt;Free space&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Free-space list: records all free disk blocks&lt;/li&gt;
&lt;li&gt;Scheme
&lt;ul&gt;
&lt;li&gt;Bit vector&lt;/li&gt;
&lt;li&gt;Linked List (same as linked allocation)&lt;/li&gt;
&lt;li&gt;Grouping (same as linked index allocation)&lt;/li&gt;
&lt;li&gt;Counting (same as contiguous allocation)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;file system usually manage free space in the same way as a file&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bit-vector&#34;&gt;Bit vector&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bit vector(bitmap): one bit foreach block&lt;/li&gt;
&lt;li&gt;simplicity, efficient (HW support bit-manipulation instruction)&lt;/li&gt;
&lt;li&gt;bitmap must be cached for gooe performance
&lt;ul&gt;
&lt;li&gt;A 1-TB(4KB block) disk needs 32MB bitmap
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c9223f4ac1e5.png&#34; width=&#34;250px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>/courses/parallel_computing/introduction/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/parallel_computing/introduction/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mass Storage System</title>
      <link>/courses/operating_system/mass_storage_system/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/mass_storage_system/</guid>
      <description>&lt;h2 id=&#34;disk-structure&#34;&gt;Disk Structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Disk drives are addressed as large 1-dim array of logical blocks (logical block: smallest unit of transfer(sector))&lt;/li&gt;
&lt;li&gt;Logical blocks are mapped onto disk sequentially
&lt;ul&gt;
&lt;li&gt;Sector 0: 1st sector of 1st track on the outermost cyl&lt;/li&gt;
&lt;li&gt;go from outermost cylinder to innermost one&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Disk drive attached to a computer by an I/O bus&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sectors-per-track&#34;&gt;Sectors Per Track&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Constant linear velocity (CLV)
&lt;ul&gt;
&lt;li&gt;density of bits per track is uniform&lt;/li&gt;
&lt;li&gt;more sectors on a track in outer cylinders&lt;/li&gt;
&lt;li&gt;keeping same data rate
increase rotation speed in inner cylinders&lt;/li&gt;
&lt;li&gt;applications: CD-ROM and DVD-ROM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Constant angular velocity (CAV)
&lt;ul&gt;
&lt;li&gt;keep same rotation speed&lt;/li&gt;
&lt;li&gt;larger bit density on inner tracks&lt;/li&gt;
&lt;li&gt;keep same data rate&lt;/li&gt;
&lt;li&gt;applications: hard disks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;disk-scheduling&#34;&gt;Disk Scheduling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Disk-access time has 3 major components
&lt;ul&gt;
&lt;li&gt;Seek time: move disk arm to the desired cylinder&lt;/li&gt;
&lt;li&gt;rotational latency: rotate disk head to the desired sector&lt;/li&gt;
&lt;li&gt;read time: constant transfer time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Disk bandwidth:
number of bytes transferred / (complete of last req - start of first req)
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c92296114e16.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Minimize seek time&lt;/li&gt;
&lt;li&gt;illustrate with a request queue(0-199) (98, 183, 37, 122, 14, 124, 65, 67)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;FCFS (First come first served)&lt;/li&gt;
&lt;li&gt;SSTF(Shortest-seek-time-first)
&lt;ul&gt;
&lt;li&gt;SSTF scheduling is a form of SJF scheduling; may cause starvation of some requests&lt;/li&gt;
&lt;li&gt;total head movement: 236 cylinders&lt;/li&gt;
&lt;li&gt;common and has a natural appeal, but not optimal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SCAN scheduling
&lt;ul&gt;
&lt;li&gt;disk head move from one end to the other end&lt;/li&gt;
&lt;li&gt;A.k.a elvator algorithm&lt;/li&gt;
&lt;li&gt;total head movement: 236 cylinders&lt;/li&gt;
&lt;li&gt;perform better for disks with heavy load&lt;/li&gt;
&lt;li&gt;No staravation problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;C-SCAN scheduling
&lt;ul&gt;
&lt;li&gt;Disk head move in one direction only&lt;/li&gt;
&lt;li&gt;A variant of SCAN to provide more uniform wait time&lt;/li&gt;
&lt;li&gt;More uniform wait time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;C-LOOK scheduling
&lt;ul&gt;
&lt;li&gt;version of C-SCAN&lt;/li&gt;
&lt;li&gt;Disk head moves only to the last request location&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Performance is also influenced by the file-allocation method
&lt;ul&gt;
&lt;li&gt;Contiguous: less head movement&lt;/li&gt;
&lt;li&gt;Indexed &amp;amp; linked: greater head movement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;disk-management&#34;&gt;Disk Management&lt;/h2&gt;
&lt;h3 id=&#34;disk-formatting&#34;&gt;Disk Formatting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Low-level formatting (or physical formatting): dividing a disk into sectors that disk controller can read and write&lt;/li&gt;
&lt;li&gt;each sector = header + data area + trailer
&lt;ul&gt;
&lt;li&gt;header &amp;amp; trailer : sector number and ECC (error-correction code)&lt;/li&gt;
&lt;li&gt;ECC is calculated based on all bytes in data area&lt;/li&gt;
&lt;li&gt;data area size: 512B, 1KB, 4KB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OS does the next 2 steps to use the disk
&lt;ul&gt;
&lt;li&gt;partition the disk into one or more groups of cylinders&lt;/li&gt;
&lt;li&gt;logical formatting (i.e. creation of a file system)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;boot-block&#34;&gt;Boot Block&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bootstrap program
&lt;ul&gt;
&lt;li&gt;Initialize CPU, registers, device, controllers, memory, and then starts OS&lt;/li&gt;
&lt;li&gt;First boostrap code stored in ROM&lt;/li&gt;
&lt;li&gt;complete bootstrap in the boot block of the boot disk (aka system disk)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;windows-2000&#34;&gt;Windows 2000&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Run bootstrap code in ROM&lt;/li&gt;
&lt;li&gt;Read boot code in MBR (Master Boot Record)&lt;/li&gt;
&lt;li&gt;Find boot partition from partition table&lt;/li&gt;
&lt;li&gt;read boot sector/block and continue booting
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c922eec7cc23.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bad-blocks&#34;&gt;Bad Blocks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Simple disks like IDE disks&lt;/li&gt;
&lt;li&gt;Sophisticated disks like SCSI disks&lt;/li&gt;
&lt;li&gt;Sector sparing (forwarding): remap bad block to a spare one
&lt;ul&gt;
&lt;li&gt;Could affect disk-scheduling performance&lt;/li&gt;
&lt;li&gt;A few spare sectors in each cylinder during formatting&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sector slipping: ships sectors all down one spot&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;swap-space-management&#34;&gt;Swap-Space Management&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;swap-space: Virtual memory use disk space (swap-space) as an extension of main memory&lt;/li&gt;
&lt;li&gt;UNIX: allows use of multiple swap spaces&lt;/li&gt;
&lt;li&gt;Location
&lt;ul&gt;
&lt;li&gt;part of a normal file system (e.g. NT) less efficient&lt;/li&gt;
&lt;li&gt;separate disk partition (raw partition) size is fixed&lt;/li&gt;
&lt;li&gt;allows access to both types (e.g. LINUX)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;swap-space-allocation&#34;&gt;Swap Space Allocation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;1st version: copy entire process between contiguous disk regions and memory&lt;/li&gt;
&lt;li&gt;2nd version: copy pages to swap space
&lt;ul&gt;
&lt;li&gt;Solaris 1: text segments read from file system, thrown away when pageout, only anonymous memory (stack, heap, etc) store in swap space&lt;/li&gt;
&lt;li&gt;Solaris 2: swap-space allocation only when pageout rather than vitrual memory creation time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data structures for Swapping
&lt;img src=&#34;https://i.loli.net/2019/03/20/5c9232c43fbc0.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;raid-structure&#34;&gt;RAID Structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RAID = Redundant Arrays of Inexpensive Disks
&lt;ul&gt;
&lt;li&gt;Provide reliability via redundacy&lt;/li&gt;
&lt;li&gt;improve performance via parallelism&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RAID is arranged into different levels (Striping, mirror, Error-correcting code (ECC) &amp;amp; Parity bit)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;raid-0--raid-1&#34;&gt;RAID 0 &amp;amp; RAID 1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RAID 0: non-redundant striping
&lt;ul&gt;
&lt;li&gt;improve performance via parallelism&lt;/li&gt;
&lt;li&gt;I/O bandwidth is proportional to the striping count (Both read and write BW increase by N times)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RAID 1: Mirrored disks
&lt;ul&gt;
&lt;li&gt;Provide reliability via redundancy
&lt;ul&gt;
&lt;li&gt;Read BW increases by N times&lt;/li&gt;
&lt;li&gt;Write BW remains the same
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c946b246195c.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;raid-2-hamming-code&#34;&gt;RAID 2: Hamming code&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;E.g.: 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Hamming_code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hamming code&lt;/a&gt; (7,4)
&lt;ul&gt;
&lt;li&gt;4 data bits (on 4 disks) + 3 parity bits (on 3 disks)&lt;/li&gt;
&lt;li&gt;each parity bit is linear code of 3 data bits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Recover from any single disk failure
&lt;ul&gt;
&lt;li&gt;can detect up to two disk (i.e. bits) error&lt;/li&gt;
&lt;li&gt;but can only &amp;ldquo;correct&amp;rdquo; one bit error&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;better space efficient than RAID1 (75% overhead)
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c946b80ad6ba.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;raid-3--4-parity-bit&#34;&gt;RAID 3 &amp;amp; 4: Parity Bit&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Disk controller can detect whether a sector has been read correctly&lt;/li&gt;
&lt;li&gt;a single parity bit is enough to correct error from a single disk failure&lt;/li&gt;
&lt;li&gt;RAID 3: bit-level striping; RAID 4: Block-level striping&lt;/li&gt;
&lt;li&gt;Even though space efficiency&lt;/li&gt;
&lt;li&gt;Cost to compute &amp;amp; store parity bit&lt;/li&gt;
&lt;li&gt;RAID4 has higher I/O throughput, because controller does not need to reconstruct block from multiple disks
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c946bf615735.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;raid-5-distributed-parity&#34;&gt;RAID 5: Distributed Parity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Spread data &amp;amp; parity across all disks&lt;/li&gt;
&lt;li&gt;Prevent over use of a single disk
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c946c1f69fc1.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Read BW increases by N times, because all four disks can serve a read request&lt;/li&gt;
&lt;li&gt;write BW:
&lt;ul&gt;
&lt;li&gt;Method 1: (1) read out all unmodified (N-2) data bits (2) re-compute parity bit (3) write both modified bit and parity bit to disks (BW = N/(N-2+2) = 1) remains the same&lt;/li&gt;
&lt;li&gt;Method 2: (1) only read the parity bit and modified bit. (2) re-compute parity bit by the difference. (3) write both modified bit and parity bit (BW = N/(2+2) = N/4 times faster)
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c946d18c09fb.png&#34; width=&#34;500px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;raid-6-p--q-dual-parity-redundancy&#34;&gt;RAID 6: P + Q Dual Parity Redundancy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Like RAID 5, but stores extra redundant information to guard against multiple disk failure&lt;/li&gt;
&lt;li&gt;Use Ecc code (i.e. Error correction code) instead of single parity bit&lt;/li&gt;
&lt;li&gt;Parity bits are also striped across disks
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c946dc4da720.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hybird-raid&#34;&gt;Hybird RAID&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RAID 0+1: Stripe then replicate&lt;/li&gt;
&lt;li&gt;RAID 1+0: Replicate then stripe
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c946e29393a7.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;First level control by a controller. Therefore, RAID 10 has better fault tolerance than RAID01 when multiple disk failes&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>I/O system</title>
      <link>/courses/operating_system/io_system/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/operating_system/io_system/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The two main jobs of a computer I/O and computation&lt;/li&gt;
&lt;li&gt;I/O devices: tape, HD, mouse, joystick, screen&lt;/li&gt;
&lt;li&gt;I/O subsytems: the methods to control all I/O devices&lt;/li&gt;
&lt;li&gt;Two conflicting trends
&lt;ul&gt;
&lt;li&gt;Standardization of HW/SW interfaces&lt;/li&gt;
&lt;li&gt;Board variety of I/O devices&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Device drivers: a uniform device-access interface to the I/O subsystem(Simliar to system calls between apps and OS)&lt;/li&gt;
&lt;li&gt;I/O Hardware
&lt;ul&gt;
&lt;li&gt;Port: A connection point between I/O diveces and the host(E.g. USB ports)&lt;/li&gt;
&lt;li&gt;Bus: A set of wires and a well-defined protocal that specifies message sent over the wires (E.g. PCI bus)&lt;/li&gt;
&lt;li&gt;Controller: A collecton of electronics that can operate a port, a bus, or a device (A controller could have its own processor, memory, etc(E.g. SCSI controller))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c94719d776dd.png&#34; width=&#34;400px&#34;/&gt;
&lt;h2 id=&#34;basic-io-method-port-mapped-io&#34;&gt;Basic I/O method (Port-mapped I/O)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Each I/O port (device) is identified by a unique port address&lt;/li&gt;
&lt;li&gt;Each I/O port consists of four registers (1~4 Bytes)
&lt;ul&gt;
&lt;li&gt;Data-in regsiter: read by the host to get input&lt;/li&gt;
&lt;li&gt;Data-out register: written by the host to send output&lt;/li&gt;
&lt;li&gt;status register: read by the host to check I/O status&lt;/li&gt;
&lt;li&gt;Control register: written by the host to control the device&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Program interact with an I/O port through special I/O instructions (differnet from memory access)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;io-methods-categorization&#34;&gt;I/O methods Categorization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Depends on how to address a deive
&lt;ul&gt;
&lt;li&gt;port-mapped I/O: use different address space from memory, access by special I/O instruction(e.g. IN, OUT)&lt;/li&gt;
&lt;li&gt;Memory-mapped I/O: Reserve specific memory space for device, Access by standard data-transfer instruction (e.g. MOV) (more efficient for large memory I/O, vulnerable to accidental modification)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Depends on how to interact with a deivce:
&lt;ul&gt;
&lt;li&gt;Poll(busy-waiting): processor periodically check status register of a device&lt;/li&gt;
&lt;li&gt;Interrupt: device notify processor of its completion&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Depending on who to control the transer:
&lt;ul&gt;
&lt;li&gt;Programmed I/O: transfer controlled by CPU&lt;/li&gt;
&lt;li&gt;Direct memory access(DMA) I/O: controlled by DMA controller(A special purpose controller, design for large data transfer)
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c94740d3444a.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kernel-io-subsystem&#34;&gt;Kernel I/O Subsystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I/O scheduling &amp;ndash; improve system performance by ordering the jobs in I/O queue (e.g. disk I/O order scheduling)&lt;/li&gt;
&lt;li&gt;Buffering &amp;ndash; store data in memory while transferring between I/O deivces
&lt;ul&gt;
&lt;li&gt;Speed mismatch between devices&lt;/li&gt;
&lt;li&gt;Devices with different data-transfer sizes&lt;/li&gt;
&lt;li&gt;Support copy semantics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Caching &amp;ndash; fast momory that holds copies of data (Key to performance)&lt;/li&gt;
&lt;li&gt;Spooling &amp;ndash; holds output for a device(e.g. printing, cannot accept interleaved files)&lt;/li&gt;
&lt;li&gt;Error handlig &amp;ndash; when I/O error happens(e.g. SCSI device returns error information)&lt;/li&gt;
&lt;li&gt;I/O protection(privileged instructions)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;blocking-and-nonblocking-io&#34;&gt;Blocking and Nonblocking I/O&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Blocking &amp;ndash; process suspended until I/O completed
&lt;ul&gt;
&lt;li&gt;Easy to use and understand&lt;/li&gt;
&lt;li&gt;Insufficient for some needs&lt;/li&gt;
&lt;li&gt;Use for synchronous communication &amp;amp; I/O&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Nonblocking
&lt;ul&gt;
&lt;li&gt;Implemented via multi-threading&lt;/li&gt;
&lt;li&gt;Returns quickly with count of bytes read or wriiten&lt;/li&gt;
&lt;li&gt;Use for asynchronous communication &amp;amp; I/O&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transforming-io-requests-to-hardware-operations&#34;&gt;Transforming I/O requests to Hardware operations&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c947676a3809.png&#34; width=&#34;400px&#34;/&gt;
&lt;h3 id=&#34;performance-and-improving&#34;&gt;Performance and Improving&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I/O is a major factor in system performance
&lt;ul&gt;
&lt;li&gt;It placs heavy demancs on the CPU to execute device driver code&lt;/li&gt;
&lt;li&gt;The resulting context switches stress the CPU and its hareware caches&lt;/li&gt;
&lt;li&gt;I/O loads dwon the memory bus during data copy bewteen controllers and physcial memory&lt;/li&gt;
&lt;li&gt;Interrupt handling is a relatively expensive task&lt;/li&gt;
&lt;li&gt;Busy-waiting could be more efficient than interrupt-driven if I/O time is small&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Improving Performance
&lt;ul&gt;
&lt;li&gt;Reduce number of context switches&lt;/li&gt;
&lt;li&gt;Reduce data copying&lt;/li&gt;
&lt;li&gt;Reduce interrupts by using large transers, smart controllers, polling&lt;/li&gt;
&lt;li&gt;Use DMA&lt;/li&gt;
&lt;li&gt;Balance CPU, memory, bus and I/O performance for highest throughput&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;application-io-interface&#34;&gt;Application I/O interface&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Device drivers: a uniform device-access interface to the I/O subsystem; hide the differences among device controllers from the I/O sub-system of OS&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chapter1</title>
      <link>/books/book1/chapter1/</link>
      <pubDate>Wed, 19 Aug 2020 09:43:23 +0800</pubDate>
      <guid>/books/book1/chapter1/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;







  
    &lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;/post/pymolcheatsheet/&#34;&gt;PymolCheatsheet&lt;/a&gt;.
  
  &lt;p&gt;








  


















&lt;/p&gt;

  
  
&lt;/div&gt;

  


&lt;p&gt;I ❤️ Academic 😄&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter2</title>
      <link>/books/book1/chapter2/</link>
      <pubDate>Wed, 19 Aug 2020 09:43:23 +0800</pubDate>
      <guid>/books/book1/chapter2/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>NGLVIEW</title>
      <link>/post/nglview/</link>
      <pubDate>Sat, 15 Aug 2020 09:25:09 +0800</pubDate>
      <guid>/post/nglview/</guid>
      <description>&lt;h2 id=&#34;nglview-show-pdb-structure&#34;&gt;NGLVIEW Show PDB Structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;show raw pdb structure with different representation&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;  &amp;lt;/style&amp;gt;
  &amp;lt;script src=&amp;quot;/js/ngl.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
  &amp;lt;script&amp;gt;
    document.addEventListener(&amp;quot;DOMContentLoaded&amp;quot;, function () {
      var stage = new NGL.Stage(&amp;quot;viewport&amp;quot;, {backgroundColor: &amp;quot;black&amp;quot;});
      stage.loadFile(&amp;quot;rcsb://1cfd&amp;quot;).then(function(component) {
        // component.addRepresentation(&amp;quot;cartoon&amp;quot;, {colorScheme: &amp;quot;sstruc&amp;quot;});
        component.addRepresentation(&amp;quot;cartoon&amp;quot;, {color: &amp;quot;gray&amp;quot;, opacity: 0.7});
        component.addRepresentation(&amp;quot;ball+stick&amp;quot;, {sele: &amp;quot;12-13&amp;quot;});
        component.autoView(&amp;quot;12-13&amp;quot;);
        })
      stage.mouseControls.remove(&amp;quot;scroll&amp;quot;, NGL.MouseActions.zoomScroll)
      &amp;lt;!-- stage.keyControls.add(&amp;quot;r&amp;quot;, NGL.KeyActions.autoView); --&amp;gt;
    });
  &amp;lt;/script&amp;gt;

  &amp;lt;div id=&amp;quot;viewport&amp;quot; style=&amp;quot;width:100%; height:400px; margin: 0 auto; overflow:hidden;&amp;quot;&amp;gt; &amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0&#34;&gt;
  &lt;script src=&#34;/js/ngl.js&#34;&gt;&lt;/script&gt;
  &lt;script&gt;
    document.addEventListener(&#34;DOMContentLoaded&#34;, function () {
      var stage = new NGL.Stage(&#34;viewport&#34;, {backgroundColor: &#34;black&#34;});
      stage.loadFile(&#34;rcsb://1cfd&#34;).then(function(component) {
        // component.addRepresentation(&#34;cartoon&#34;, {colorScheme: &#34;sstruc&#34;});
        component.addRepresentation(&#34;cartoon&#34;, {color: &#34;gray&#34;, opacity: 0.7});
        component.addRepresentation(&#34;ball+stick&#34;, {sele: &#34;12-13&#34;});
        component.autoView(&#34;12-13&#34;);
        })
      stage.mouseControls.remove(&#34;scroll&#34;, NGL.MouseActions.zoomScroll)
      &lt;!-- stage.keyControls.add(&#34;r&#34;, NGL.KeyActions.autoView); --&gt;
    });
  &lt;/script&gt;
  &lt;div id=&#34;viewport&#34; style=&#34;width:100%; height:400px; margin: 0 auto; overflow:hidden;&#34;&gt; &lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;show two structures and align&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;  &amp;lt;script&amp;gt;
    document.addEventListener(&amp;quot;DOMContentLoaded&amp;quot;, function () {
      var stage = new NGL.Stage(&amp;quot;viewport2&amp;quot;, {backgroundColor: &amp;quot;black&amp;quot;});
      stage.mouseControls.remove(&amp;quot;scroll&amp;quot;, NGL.MouseActions.zoomScroll)
      Promise.all([
        stage.loadFile(&#39;rcsb://1cll&#39;).then(function(o) {
          o.addRepresentation(&#39;cartoon&#39;, {color: &#39;lightgreen&#39;})
          o.autoView()
          return o;
        }),
        stage.loadFile(&#39;rcsb://1cfd&#39;).then(function(o) {
          o.addRepresentation(&#39;cartoon&#39;, {color: &#39;tomato&#39;})
          o.autoView()
          return o;
        })
      ]).then(function (ol) {
        var s1 = ol[0].structure
        var s2 = ol[1].structure
        NGL.superpose(s1, s2, true, &amp;quot;1-63.CA&amp;quot;, &amp;quot;1-63.CA&amp;quot;)
        ol[ 0 ].updateRepresentations({ position: true })
      })
    });
  &amp;lt;/script&amp;gt;
  &amp;lt;div id=&amp;quot;viewport2&amp;quot; style=&amp;quot;width:100%; height:400px; margin: 0 auto; overflow:hidden;&amp;quot;&amp;gt; &amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;script&gt;
    document.addEventListener(&#34;DOMContentLoaded&#34;, function () {
      var stage = new NGL.Stage(&#34;viewport2&#34;, {backgroundColor: &#34;black&#34;});
      stage.mouseControls.remove(&#34;scroll&#34;, NGL.MouseActions.zoomScroll)
      Promise.all([
        stage.loadFile(&#39;rcsb://1cll&#39;).then(function(o) {
          o.addRepresentation(&#39;cartoon&#39;, {color: &#39;lightgreen&#39;})
          o.autoView()
          return o;
        }),
        stage.loadFile(&#39;rcsb://1cfd&#39;).then(function(o) {
          o.addRepresentation(&#39;cartoon&#39;, {color: &#39;tomato&#39;})
          o.autoView()
          return o;
        })
      ]).then(function (ol) {
        var s1 = ol[0].structure
        var s2 = ol[1].structure
        NGL.superpose(s1, s2, true, &#34;1-63.CA&#34;, &#34;1-63.CA&#34;)
        ol[ 0 ].updateRepresentations({ position: true })
      })
    });
  &lt;/script&gt;
  &lt;div id=&#34;viewport2&#34; style=&#34;width:100%; height:400px; margin: 0 auto; overflow:hidden;&#34;&gt; &lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;add a pdb INPUT for showing the structure and double click for a full screen&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;  &amp;lt;script&amp;gt;
    var stage;
    document.addEventListener(&amp;quot;DOMContentLoaded&amp;quot;, function () {
      stage = new NGL.Stage(&amp;quot;viewport3&amp;quot;);
      stage.mouseControls.remove(&amp;quot;scroll&amp;quot;, NGL.MouseActions.zoomScroll)
      stage.viewer.container.addEventListener(&amp;quot;dblclick&amp;quot;, function () {
          stage.toggleFullscreen();
      });
      function handleResize () {
        stage.handleResize();
      }
      document.getElementById(&amp;quot;pdbidInput&amp;quot;).addEventListener(&amp;quot;keydown&amp;quot;, function (e) {
        if (e.keyCode === 13) {
          stage.removeAllComponents();
          var url = &amp;quot;rcsb://&amp;quot; + e.target.value + &amp;quot;.mmtf&amp;quot;;
          stage.loadFile(url, {defaultRepresentation: true});
          e.target.value = &amp;quot;&amp;quot;;
          e.target.blur();
        }
      });
    });
  &amp;lt;/script&amp;gt;
  &amp;lt;div id=&amp;quot;viewport3&amp;quot; style=&amp;quot;width:100%; height:400px;&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;
  &amp;lt;div style=&amp;quot;text-align:center; position:relative; bottom:3em;&amp;quot;&amp;gt;
    &amp;lt;div style=&amp;quot;display:inline-block;&amp;quot;&amp;gt;
      &amp;lt;input id=&amp;quot;pdbidInput&amp;quot; style=&amp;quot;opacity:0.7; width:4em;&amp;quot;/&amp;gt;
    &amp;lt;/div&amp;gt;
  &amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;script&gt;
    var stage;
    document.addEventListener(&#34;DOMContentLoaded&#34;, function () {
      stage = new NGL.Stage(&#34;viewport3&#34;);
      stage.mouseControls.remove(&#34;scroll&#34;, NGL.MouseActions.zoomScroll)
      stage.viewer.container.addEventListener(&#34;dblclick&#34;, function () {
          stage.toggleFullscreen();
      });
      function handleResize () {
        stage.handleResize();
      }
      document.getElementById(&#34;pdbidInput&#34;).addEventListener(&#34;keydown&#34;, function (e) {
        if (e.keyCode === 13) {
          stage.removeAllComponents();
          var url = &#34;rcsb://&#34; + e.target.value + &#34;.mmtf&#34;;
          stage.loadFile(url, {defaultRepresentation: true});
          e.target.value = &#34;&#34;;
          e.target.blur();
        }
      });
    });
  &lt;/script&gt;
  &lt;div id=&#34;viewport3&#34; style=&#34;width:100%; height:400px;&#34;&gt;&lt;/div&gt;
  &lt;div style=&#34;text-align:center; position:relative; bottom:3em;&#34;&gt;
    &lt;div style=&#34;display:inline-block;&#34;&gt;
      &lt;input id=&#34;pdbidInput&#34; style=&#34;opacity:0.7; width:4em;&#34;/&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;h2 id=&#34;draw-custom-shape&#34;&gt;Draw custom shape&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;show arrow beteen two atoms and zoom&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;  &amp;lt;script&amp;gt;
    document.addEventListener(&amp;quot;DOMContentLoaded&amp;quot;, function () {
    var stage = new NGL.Stage(&amp;quot;viewport4&amp;quot;, {backgroundColor: &amp;quot;white&amp;quot;});
    stage.mouseControls.remove(&amp;quot;scroll&amp;quot;, NGL.MouseActions.zoomScroll)
    stage.loadFile(&amp;quot;rcsb://1crn.mmtf&amp;quot;).then(function (o){
      o.addRepresentation(&amp;quot;cartoon&amp;quot;, {opacity: 0.3, depthWrite: false, flatShaded: false, side: &amp;quot;double&amp;quot;})
      const ap1 = o.structure.getAtomProxy();
      const ap2 = o.structure.getAtomProxy();
      var idx = o.structure.getAtomIndices(new NGL.Selection(&amp;quot;(10.CA) or (11.CA)&amp;quot;))
      ap1.index = idx[0];
      ap2.index = idx[1];
      var shape = new NGL.Shape(&#39;shape&#39;, { dashedCylinder: true });
      shape.addArrow(ap1.positionToVector3(), ap2.positionToVector3(), [ 0, 1, 1 ], 0.3)
      var shapeComp = stage.addComponentFromObject( shape );
      o.addRepresentation(&#39;ball+stick&#39;, {sele:&amp;quot;10, 11&amp;quot;})
      shapeComp.addRepresentation( &amp;quot;buffer&amp;quot; );
      var center = o.getCenter(&amp;quot;10, 11&amp;quot;)
      var zoom = o.getZoom(&amp;quot;10, 11&amp;quot;)
      stage.animationControls.zoomMove(center, zoom, 0);
      });
    });
  &amp;lt;/script&amp;gt;
  &amp;lt;div id=&amp;quot;viewport4&amp;quot; style=&amp;quot;width:100%; height:400px; margin: 0 auto; overflow:hidden;&amp;quot;&amp;gt; &amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;script&gt;
    document.addEventListener(&#34;DOMContentLoaded&#34;, function () {
    var stage = new NGL.Stage(&#34;viewport4&#34;, {backgroundColor: &#34;white&#34;});
    stage.mouseControls.remove(&#34;scroll&#34;, NGL.MouseActions.zoomScroll)
    stage.loadFile(&#34;rcsb://1crn.mmtf&#34;).then(function (o){
      o.addRepresentation(&#34;cartoon&#34;, {opacity: 0.3, depthWrite: false, flatShaded: false, side: &#34;double&#34;})
      const ap1 = o.structure.getAtomProxy();
      const ap2 = o.structure.getAtomProxy();
      var idx = o.structure.getAtomIndices(new NGL.Selection(&#34;(10.CA) or (11.CA)&#34;))
      ap1.index = idx[0];
      ap2.index = idx[1];
      var shape = new NGL.Shape(&#39;shape&#39;, { dashedCylinder: true });
      shape.addArrow(ap1.positionToVector3(), ap2.positionToVector3(), [ 0, 1, 1 ], 0.3)
      var shapeComp = stage.addComponentFromObject( shape );
      o.addRepresentation(&#39;ball+stick&#39;, {sele:&#34;10, 11&#34;})
      shapeComp.addRepresentation( &#34;buffer&#34; );
      var center = o.getCenter(&#34;10, 11&#34;)
      var zoom = o.getZoom(&#34;10, 11&#34;)
      stage.animationControls.zoomMove(center, zoom, 0);
      });
    });
  &lt;/script&gt;
  &lt;div id=&#34;viewport4&#34; style=&#34;width:100%; height:400px; margin: 0 auto; overflow:hidden;&#34;&gt; &lt;/div&gt;
&lt;!-- ## Load A trajectory

&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function () {
  var stage = new NGL.Stage(&#34;viewport5&#34;, {backgroundColor: &#34;white&#34;});
  stage.loadFile(&#39;rcsb://2MI7.pdb&#39;).then(function (o) {
    NGL.autoLoad(&#39;rcsb://2MI7.pdb&#39;).then(function (frames) {
      o.addTrajectory(frames, {
        initialFrame: 0,
        deltaTime: 200
      })
      o.addRepresentation(&#39;licorice&#39;, {scale: 0.5})
      o.addRepresentation(&#39;spacefill&#39;, {sele: &#39;not :B&#39;})
      o.addRepresentation(&#39;cartoon&#39;)
      o.addRepresentation(&#39;backbone&#39;)
      stage.autoView()})
    })
  })
&lt;/script&gt;
&lt;div id=&#34;viewport5&#34; style=&#34;width:100%; height:400px; margin: 0 auto; overflow:hidden;&#34;&gt; &lt;/div&gt; --&gt;&lt;blockquote&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>PymolCheatsheet</title>
      <link>/post/pymolcheatsheet/</link>
      <pubDate>Fri, 14 Aug 2020 11:36:48 +0800</pubDate>
      <guid>/post/pymolcheatsheet/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Select around:
&lt;ul&gt;
&lt;li&gt;select active, byres all with 5 of ligands&lt;/li&gt;
&lt;li&gt;select ligand_water, ((ligands)around 3.2) and (resn HOH)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Change radius
&lt;ul&gt;
&lt;li&gt;alter active_water, vdw=0.5&lt;/li&gt;
&lt;li&gt;rebuild&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;color and cartoon setting
&lt;ul&gt;
&lt;li&gt;set cartoon_color, slate&lt;/li&gt;
&lt;li&gt;set transparency 0.4&lt;/li&gt;
&lt;li&gt;set cartoon_fancy_helices, 1&lt;/li&gt;
&lt;li&gt;set cartoon_highlight_color, grey50&lt;/li&gt;
&lt;li&gt;bg_color, white&lt;/li&gt;
&lt;li&gt;set antialias, 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;load netcdf
&lt;ul&gt;
&lt;li&gt;load rna.prmtop&lt;/li&gt;
&lt;li&gt;load_traj min2.rst, rna, 0, nc&lt;/li&gt;
&lt;li&gt;load rna.prmtop, traj&lt;/li&gt;
&lt;li&gt;load_traj prod_2us.nc, traj, 0, nc, start=8215, stop=8215&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create from an object
&lt;ul&gt;
&lt;li&gt;create name, (selection), [, source_state [, target_state]]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;remove solvent and ions
&lt;ul&gt;
&lt;li&gt;remove solvent&lt;/li&gt;
&lt;li&gt;remove inorganic&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hydrogen bond
&lt;ul&gt;
&lt;li&gt;distance test, interface, nucleic, mode=2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;align with specified residue
&lt;ul&gt;
&lt;li&gt;align 1cfd_unbound///1&lt;em&gt;68/, 1cll_bound///1&lt;/em&gt;68/&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Draw RNA dssr block
&lt;ul&gt;
&lt;li&gt;set ambient, 0.6&lt;/li&gt;
&lt;li&gt;dssr_block block_file=face+edge   block_color=C yellow&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rename an object
&lt;ul&gt;
&lt;li&gt;set_name rna rna01&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Print resName of an object
&lt;ul&gt;
&lt;li&gt;iterate bca. all , print (resn) 
&lt;a href=&#34;https://pymolwiki.org/index.php/Iterate&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Iterate * PyMOLWiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Python API save to a name
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;myspace = {&amp;quot;resname&amp;quot;: []}
cmd.iterate(&amp;quot;bca. all&amp;quot;, &amp;quot;resname.append(resn)&amp;quot;, space=myspace)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Get XYZ of a atom
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cmd.get_coords(&amp;quot;/1cll///20/CA&amp;quot;)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Calcualte helix angle between
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;angle_between_helices 1cll///71*76/, 1cll///86*92/, visualize=1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Calcualte helix every frame
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(30):
  pymol.cmd.set(&amp;quot;state&amp;quot;, i+1)
  angle.append(anglebetweenhelices.angle_between_helices(&amp;quot;1dmo///71*76/&amp;quot;, &amp;quot;1dmo///86*92/&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>OPENMP Overview</title>
      <link>/post/openmp/openmp_introduction_1/</link>
      <pubDate>Wed, 12 Aug 2020 10:59:55 +0800</pubDate>
      <guid>/post/openmp/openmp_introduction_1/</guid>
      <description>&lt;h2 id=&#34;reference-books&#34;&gt;Reference Books&lt;/h2&gt;








  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  

  
  
  
  
  
    
  
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34;  href=&#34;https://raw.githubusercontent.com/zhanghaomiao/putImage/master/post/20200821190253.png&#34;&gt;
    &lt;img src=&#34;https://raw.githubusercontent.com/zhanghaomiao/putImage/master/post/20200821190253.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
  
    
  
  
  &lt;a data-fancybox=&#34;gallery-gallery&#34;  href=&#34;https://raw.githubusercontent.com/zhanghaomiao/putImage/master/post/20200821190819.png&#34;&gt;
    &lt;img src=&#34;https://raw.githubusercontent.com/zhanghaomiao/putImage/master/post/20200821190819.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
  
  
&lt;/div&gt;
&lt;h2 id=&#34;idea-of-openmp&#34;&gt;Idea of OpenMP&lt;/h2&gt;
&lt;p&gt;Distribured and shared-memory computers
&lt;img src=&#34;https://raw.githubusercontent.com/zhanghaomiao/putImage/master/post/20200821211114.png&#34; alt=&#34;distrubted-shared-memory&#34;&gt;&lt;/p&gt;
&lt;p&gt;Fork jon programming model, program starts as a single thread of execution, and a team of threads is forked at the begining of parallel region and joined at the end
&lt;img src=&#34;https://raw.githubusercontent.com/zhanghaomiao/putImage/master/post/20200821203712.png&#34; alt=&#34;Fork-join-mp&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;feature-set&#34;&gt;Feature Set&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Creating Teams of Threads
&lt;ul&gt;
&lt;li&gt;Specifiy the parallel region by inserting a &lt;code&gt;parallel&lt;/code&gt; directive before the code that is to be executed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sharing Work among Threads
&lt;ul&gt;
&lt;li&gt;Work sharing and loops&lt;/li&gt;
&lt;li&gt;&lt;code&gt;while&lt;/code&gt; construct cannot be shared, work should be independent&lt;/li&gt;
&lt;li&gt;Giving distinct pieces of work to individual threads&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OpenMP Memory Model
&lt;ul&gt;
&lt;li&gt;Shared-memory model, by default, data is shared among the threads and is visble to all of them.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;flush&lt;/code&gt; operation makes sure that the thread calling it has same values for shared data objects.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Thread Synchronization
&lt;ul&gt;
&lt;li&gt;By default, OpenMP gets threads to wait at the end of a &lt;strong&gt;work-sharing construct or parallel region&lt;/strong&gt; until all threads in the team executing it have finished their portion of work. (&lt;strong&gt;Barrier&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Synchronization Point&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;explicit and implicit barriers&lt;/li&gt;
&lt;li&gt;start and end of critical regions&lt;/li&gt;
&lt;li&gt;points where locks are required or released&lt;/li&gt;
&lt;li&gt;anywhere the progarmmer has inserted a flush directive&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;performance-considerations&#34;&gt;Performance Considerations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Amdahl&amp;rsquo;s law&lt;/strong&gt;
$$S=\frac{1}{\left(f_{\text {par}} / P+\left(1-f_{\text {par}}\right)\right)}$$
&lt;ul&gt;
&lt;li&gt;$f_{\text par}$ parallel fraction of the code, idel case is 1&lt;/li&gt;
&lt;li&gt;$p$ number of processors&lt;/li&gt;
&lt;li&gt;If only $80%$ percent of code runs is parallel
&lt;ul&gt;
&lt;li&gt;maximal speedup on 16 processors is 4&lt;/li&gt;
&lt;li&gt;maximal speedup on 32 processors is 4.4&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Other Considerations
&lt;ul&gt;
&lt;li&gt;overheads by forking and joining threads, thread synchronization and memeory accessed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>蓄水池采样</title>
      <link>/post/reservior_sampling/</link>
      <pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/reservior_sampling/</guid>
      <description>&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;
&lt;p&gt;给定一个数据流，数据流长度N很大，且N直到处理完所有数据之前都不可知，请问如何在只遍历一遍数据（O(N)）的情况下，能够随机选取出m个不重复的数据。&lt;/p&gt;
&lt;h2 id=&#34;解法&#34;&gt;解法&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;如果接收的数据量小于m，则依次放入蓄水池。&lt;/li&gt;
&lt;li&gt;当接收到第i个数据时，i &amp;gt;= m，在[0, i]范围内取以随机数d，若d的落在[0, m-1]范围内，则用接收到的第i个数据替换蓄水池中的第d个数据。&lt;/li&gt;
&lt;li&gt;重复步骤2。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;解释&#34;&gt;解释&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;这里需要证明处理完这整个数据流之后，所以数被选择的概率都能有 m / N&lt;/li&gt;
&lt;li&gt;i&amp;lt;=m 时， 第 i 个数据进入 reservior 的概率是 1&lt;/li&gt;
&lt;li&gt;i&amp;gt;m 时， 从 [1, i] 选择随机数 d, 如果 d &amp;lt;= m , 则使用第 i 个数据替换 第 d 个数据， 第 i 个数据进入 reservoir概率是   m / i&lt;/li&gt;
&lt;li&gt;i&amp;lt;=m时， 第(m+1) 次会替换掉池中数据的概率  m/(m+1), 替换第 i 个数据概率 1/m, 则 m+1 次替换掉 i个数据概率 为  1/(m+1), 不被替换概率为 m/(m+1), 第 (m+2) 次处理不替换第 i 个数据概率 (m+1) / (m+2)， 依次计算可得第 i 个数据不被替换的概率为 m / N&lt;/li&gt;
&lt;li&gt;i &amp;gt; m 时， 接收 i+1 个数据可能替换第 i 个数据， 第 i 个数据不被替换概率 i / N&lt;/li&gt;
&lt;li&gt;i&amp;lt;=m 时 ，第 i 个数据留在 reservoir 概率是 $1*m/N$, i&amp;gt;m 时， 第 i 个数据留在 reservoir 概率为 $m /i * i/ N = m / N$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;练习&#34;&gt;练习&lt;/h2&gt;
&lt;p&gt;reservior 深度为 1&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://leetcode-cn.com/problems/random-pick-index/&#34;&gt;https://leetcode-cn.com/problems/random-pick-index/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://leetcode-cn.com/problems/linked-list-random-node&#34;&gt;https://leetcode-cn.com/problems/linked-list-random-node&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;iframe allow=&#34;autoplay *; encrypted-media *;&#34; frameborder=&#34;0&#34; height=&#34;150&#34; style=&#34;width:100%;max-width:660px;overflow:hidden;background:transparent;&#34; sandbox=&#34;allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation&#34; src=&#34;https://embed.music.apple.com/cn/album/%E8%B6%85%E4%BA%BA%E4%B8%8D%E4%BC%9A%E9%A3%9E/536247746?i=536248204&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>用 PyQT 创建一个识别公式的应用</title>
      <link>/post/image_latex_app/</link>
      <pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/image_latex_app/</guid>
      <description>&lt;p&gt;在科研论文中，经常需要输入LaTeX公式，这是一件很令人头疼的事，因为公式输入很复杂且容易出错。MathPix这个软件简化了这个过程，可以直接通过图片识别出 LaTeX 公式。以前这个软件是免费的, 后来开始收费且价格不菲. 不过推出了 API 调用方式，一个月3000条免费，后面超出的价格也不贵。&lt;/p&gt;
&lt;p&gt;为了更好的使用这个API，一键操作，仿照以前的 MathPix界面，用 pyqt 写了一个简略的界面，不过也可以实现基本功能。&lt;/p&gt;
&lt;h2 id=&#34;ide&#34;&gt;IDE&lt;/h2&gt;
&lt;p&gt;用的是 PyCharm, 不像 Qt 有 QtCreater, python 版本的 pyqt 没找到官方的 IDE, 因此用的是PyCharm. 在 PyCharm 上加了两个 External Tools:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;qt designer, 设计 UI 界面
&lt;img src=&#34;/img/pyQT/pyqt_tool.png&#34; width=&#34;450px&#34; /&gt;&lt;/li&gt;
&lt;li&gt;pyUIC5, 将 UI file 转为 .py file
&lt;img src=&#34;/img/pyQT/pyuic_tool.png&#34; width=&#34;450px&#34; /&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;qt-designer&#34;&gt;Qt Designer&lt;/h2&gt;
&lt;p&gt;主要用来设计UI界面，设计好的界面如下
&lt;img src=&#34;/img/pyQT/pyqt_ui.png&#34; width=&#34;450px&#34; /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;图片显示界面， 可以从剪切板里导入图片，或者是从应用外拖进图片&lt;/li&gt;
&lt;li&gt;Convert, 触发调用API命令，并将结果显示&lt;/li&gt;
&lt;li&gt;Qt Web, 一个网页显示界面，在 convert， 生成 LaTeX 代码之后，通过调用 MathJax Api， 显示生成后的 LaTeX 公式， 方便与原始图片进行比较&lt;/li&gt;
&lt;li&gt;PasteBoard 从剪切板导入图片&lt;/li&gt;
&lt;li&gt;生成后的 LaTeX 代码，通过 copy 将文字导入到剪切板， 三个copy 对应三种不同返回格式&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;用到的对象如下&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/pyQT/object.png&#34; width=&#34;450px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里有一点需要注意的是，label 标签对应的类不是 QLabel, 因为如果 QLabel 要支持拖拽功能，需要在 QLabel 下重新定义一个子类，并让子类重写&lt;code&gt;dragEnterEvent&lt;/code&gt; 和 &lt;code&gt;dropEvent&lt;/code&gt; 两个函数，这样，需要对 Qt Designer 的 QLabel 进行提升，这里我将提升的类命名为 &lt;code&gt;ImageArea&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;代码&#34;&gt;代码&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/img/pyQT/code.png&#34; width=&#34;650px&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;app_run.py&lt;/code&gt; 主程序，程序代码的入口，定义了整个对象，以及对象之间如何交互&lt;/li&gt;
&lt;li&gt;&lt;code&gt;customClass.py&lt;/code&gt; 定义了 &lt;code&gt;ImageArea&lt;/code&gt; 类， 让 &lt;code&gt;QLabel&lt;/code&gt; 支持拖拽&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dealWithApi.py&lt;/code&gt;  MathPix 的调用接口&lt;/li&gt;
&lt;li&gt;&lt;code&gt;qtDesigner.py&lt;/code&gt; 通过 &lt;code&gt;pyUIC&lt;/code&gt; 将 ui 文件转换为 py 文件&lt;/li&gt;
&lt;li&gt;&lt;code&gt;showLaTeXImage.py&lt;/code&gt; 将生成的LaTeX 公式文本，用 MathJax 显示&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;演示&#34;&gt;演示&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/img/pyQT/moive.gif&#34; width=&#34;650px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For the source code, see at &lt;a href=&#34;https://github.com/zhanghaomiao/image_to_latex_app&#34;&gt;https://github.com/zhanghaomiao/image_to_latex_app&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>T-SNE</title>
      <link>/post/ml/t-sne/</link>
      <pubDate>Fri, 04 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/ml/t-sne/</guid>
      <description>&lt;h2 id=&#34;prepare&#34;&gt;Prepare&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Perplexity
Measurement of how well a probability model predicts a sample.
&lt;ul&gt;
&lt;li&gt;Definition 
$$H(P) = -\sum_x p(x) \log_2 p(x)$$
$$\text{Perplexity} = 2^{H(p)}$$&lt;/li&gt;
&lt;li&gt;Understanding
&lt;ul&gt;
&lt;li&gt;$H(p)$ be the average of bits to decode the information&lt;/li&gt;
&lt;li&gt;$\text{Perplexity}$ is the total amount of all possible information&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Usage
In the NLP, the best language model is one that predicts an unseen test set gives the highest P
$$ PP(w) = p (w_1w_2\dots w_N )^{-1/N}$$
Minmizing perplexity is the same as maximizing probability&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gaussian Distriubtion and T Distribution
&lt;ul&gt;
&lt;li&gt;T distribution is used when the samples is small&lt;/li&gt;
&lt;li&gt;T distribution is heavy tail
&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/4/41/Student_t_pdf.svg&#34; alt=&#34;Student_t_pdf.svg&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;t-sne-method&#34;&gt;T-SNE method&lt;/h2&gt;
&lt;h3 id=&#34;sne-method-stochastic-neighbor-embedding&#34;&gt;SNE method (Stochastic Neighbor Embedding)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;High Dimension Space
$$p_{j | i}=\frac{\exp \left(-\left|x_{i}-x_{j}\right|^{2} / 2 \sigma_{i}^{2}\right)}{\sum_{k \neq i} \exp \left(-\left|x_{i}-x_{k}\right|^{2} / 2 \sigma_{i}^{2}\right)}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Low dimension space&lt;/p&gt;
&lt;p&gt;$$q_{j | i}=\frac{\exp \left(-\left|y_{i}-y_{j}\right|^{2}\right)}{\sum_{k \neq i} \exp \left(-\left|y_{i}-y_{k}\right|^{2}\right)}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cost function
$$C=\sum_{i} K L\left(P_{i} | Q_{i}\right)=\sum_{i} \sum_{j} p_{j | i} \log \frac{p_{j | i}}{q_{j | i}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to choose $\sigma$ for different points&lt;/p&gt;
&lt;p&gt;Different region have different density, so the $\sigma$ determines how many effective neighbors needs to be considered. As the $\sigma$ increase, the entropy of the distribution is increased. And the entroy has an expontential form with &lt;strong&gt;perplexity&lt;/strong&gt;. Using the &lt;strong&gt;perplexity&lt;/strong&gt; to determine the $\sigma$ with every point has a fixed &lt;strong&gt;perplexity&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Problem
&lt;ul&gt;
&lt;li&gt;The cost function is not symmetric, it focus on retaining the local structure of the data&lt;/li&gt;
&lt;li&gt;Dealing with outlier&lt;/li&gt;
&lt;li&gt;Crowding problem : The area of the two-dimensional map that is available to accommodate distant datapoints will not be large enough compared with the area available to accommodate nearby datapoints&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;t-sne-advantages&#34;&gt;T-SNE advantages&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Symmetric SNE&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Pairwise similarities in the low dimensional map $q_{ij}$
$$q_{i j}=\frac{\exp \left(-\left|y_{i}-y_{j}\right|^{2}\right)}{\sum_{k \neq l} \exp \left(-\left|y_{k}-y_{l}\right|^{2}\right)}$$&lt;/li&gt;
&lt;li&gt;High dimensional $p_{ij}$
$$P_{ij} = \frac {p_{j|i} + p_{i|j}}{2n}$$&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;Crowding problem
&lt;img src=&#34;/img/tsne_01.png&#34;/&gt;
The gradient could be negative, so the low dimensional space could be expanded, and the heavy tail T-Distribution is preferred.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Easy to compute the gradient&lt;/p&gt;
&lt;p&gt;$$\frac{\delta C}{\delta y_{i}}=4 \sum_{j}\left(p_{i j}-q_{i j}\right)\left(y_{i}-y_{j}\right)\left(1+\left|y_{i}-y_{j}\right|^{2}\right)^{-1}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Data : data Set $\mathcal{X} = {x_1, x_2, \dots, x_n}$&lt;/li&gt;
&lt;li&gt;cost function parameters: perplexity $Perp$&lt;/li&gt;
&lt;li&gt;optimzation paraemters: number of iterations $T$, learning rate $\eta$, momentum $\alpha(t)$&lt;/li&gt;
&lt;li&gt;Output: low-dimensional data representation $\mathcal{Y}^{T} = {y_1, y_2, \dots, y_n}$&lt;/li&gt;
&lt;li&gt;Steps
&lt;ol&gt;
&lt;li&gt;compute pairwise affinities $p_{j|i}$ with perplexity $Perp$&lt;/li&gt;
&lt;li&gt;Set $p_{ij} = (p_{j|i} + p_{i|j}) / 2n$, sample initial solution $\mathcal{Y}^{(0)} = {y_1, y_2, \dots, y_n}$ from $\mathcal{N}(0, 10^{-4}I)$&lt;/li&gt;
&lt;li&gt;for $t = 1$ to $T$ do
&lt;ul&gt;
&lt;li&gt;compute low-dimensional affinities $q_ij$&lt;/li&gt;
&lt;li&gt;compute gradient $\partial C / \partial \mathcal{Y}$&lt;/li&gt;
&lt;li&gt;set
$$\mathcal{Y}^{(t)}=\mathcal{Y}^{(t-1)}+\eta \frac{\delta C}{\delta \gamma}+\alpha(t)\left(\mathcal{Y}^{(t-1)}-\mathcal{Y}^{(t-2)}\right)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;code-with-python&#34;&gt;Code with python&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pylab


def Hbeta(D=np.array([]), beta=1.0):
    &amp;quot;&amp;quot;&amp;quot;
        Compute the perplexity and the P-row for a specific value of the
        precision of a Gaussian distribution.
    &amp;quot;&amp;quot;&amp;quot;

    # Compute P-row and corresponding perplexity
    P = np.exp(-D.copy() * beta)
    sumP = sum(P)
    H = np.log(sumP) + beta * np.sum(D * P) / sumP
    P = P / sumP
    return H, P


def x2p(X=np.array([]), tol=1e-5, perplexity=30.0):
    &amp;quot;&amp;quot;&amp;quot;
        Performs a binary search to get P-values in such a way that each
        conditional Gaussian has the same perplexity.
    &amp;quot;&amp;quot;&amp;quot;

    # Initialize some variables
    print(&amp;quot;Computing pairwise distances...&amp;quot;)
    (n, d) = X.shape
    sum_X = np.sum(np.square(X), 1)
    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)
    P = np.zeros((n, n))
    beta = np.ones((n, 1))
    logU = np.log(perplexity)

    # Loop over all datapoints
    for i in range(n):

        # Print progress
        if i % 500 == 0:
            print(&amp;quot;Computing P-values for point %d of %d...&amp;quot; % (i, n))

        # Compute the Gaussian kernel and entropy for the current precision
        betamin = -np.inf
        betamax = np.inf
        Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]
        (H, thisP) = Hbeta(Di, beta[i])

        # Evaluate whether the perplexity is within tolerance
        Hdiff = H - logU
        tries = 0
        while np.abs(Hdiff) &amp;gt; tol and tries &amp;lt; 50:

            # If not, increase or decrease precision
            if Hdiff &amp;gt; 0:
                betamin = beta[i].copy()
                if betamax == np.inf or betamax == -np.inf:
                    beta[i] = beta[i] * 2.
                else:
                    beta[i] = (beta[i] + betamax) / 2.
            else:
                betamax = beta[i].copy()
                if betamin == np.inf or betamin == -np.inf:
                    beta[i] = beta[i] / 2.
                else:
                    beta[i] = (beta[i] + betamin) / 2.

            # Recompute the values
            (H, thisP) = Hbeta(Di, beta[i])
            Hdiff = H - logU
            tries += 1

        # Set the final row of P
        P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP

    # Return final P-matrix
    print(&amp;quot;Mean value of sigma: %f&amp;quot; % np.mean(np.sqrt(1 / beta)))
    return P


def pca(X=np.array([]), no_dims=50):
    &amp;quot;&amp;quot;&amp;quot;
        Runs PCA on the NxD array X in order to reduce its dimensionality to
        no_dims dimensions.
    &amp;quot;&amp;quot;&amp;quot;

    print(&amp;quot;Preprocessing the data using PCA...&amp;quot;)
    (n, d) = X.shape
    X = X - np.tile(np.mean(X, 0), (n, 1))
    (l, M) = np.linalg.eig(np.dot(X.T, X))
    Y = np.dot(X, M[:, 0:no_dims])
    return Y


def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0):
    &amp;quot;&amp;quot;&amp;quot;
        Runs t-SNE on the dataset in the NxD array X to reduce its
        dimensionality to no_dims dimensions. The syntaxis of the function is
        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.
    &amp;quot;&amp;quot;&amp;quot;

    # Check inputs
    if isinstance(no_dims, float):
        print(&amp;quot;Error: array X should have type float.&amp;quot;)
        return -1
    if round(no_dims) != no_dims:
        print(&amp;quot;Error: number of dimensions should be an integer.&amp;quot;)
        return -1

    # Initialize variables
    X = pca(X, initial_dims).real
    (n, d) = X.shape
    max_iter = 1000
    initial_momentum = 0.5
    final_momentum = 0.8
    eta = 500
    min_gain = 0.01
    Y = np.random.randn(n, no_dims)
    dY = np.zeros((n, no_dims))
    iY = np.zeros((n, no_dims))
    gains = np.ones((n, no_dims))

    # Compute P-values
    P = x2p(X, 1e-5, perplexity)
    P = P + np.transpose(P)
    P = P / np.sum(P)
    P = P * 4.									# early exaggeration
    P = np.maximum(P, 1e-12)

    # Run iterations
    for iter in range(max_iter):

        # Compute pairwise affinities
        sum_Y = np.sum(np.square(Y), 1)
        num = -2. * np.dot(Y, Y.T)
        num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))
        num[range(n), range(n)] = 0.
        Q = num / np.sum(num)
        Q = np.maximum(Q, 1e-12)

        # Compute gradient
        PQ = P - Q
        for i in range(n):
            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)

        # Perform the update
        if iter &amp;lt; 20:
            momentum = initial_momentum
        else:
            momentum = final_momentum
        gains = (gains + 0.2) * ((dY &amp;gt; 0.) != (iY &amp;gt; 0.)) + \
                (gains * 0.8) * ((dY &amp;gt; 0.) == (iY &amp;gt; 0.))
        gains[gains &amp;lt; min_gain] = min_gain
        iY = momentum * iY - eta * (gains * dY)
        Y = Y + iY
        Y = Y - np.tile(np.mean(Y, 0), (n, 1))

        # Compute current value of cost function
        if (iter + 1) % 10 == 0:
            C = np.sum(P * np.log(P / Q))
            print(&amp;quot;Iteration %d: error is %f&amp;quot; % (iter + 1, C))

        # Stop lying about P-values
        if iter == 100:
            P = P / 4.

    # Return solution
    return Y


if __name__ == &amp;quot;__main__&amp;quot;:
    print(&amp;quot;Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.&amp;quot;)
    print(&amp;quot;Running example on 2,500 MNIST digits...&amp;quot;)
    X = np.loadtxt(&amp;quot;mnist2500_X.txt&amp;quot;)
    print(X.shape)
    labels = np.loadtxt(&amp;quot;mnist2500_labels.txt&amp;quot;)
    Y = tsne(X, 2, 50, 20.0)
    pylab.scatter(Y[:, 0], Y[:, 1], 20, labels)
    pylab.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;outcome-for-mnist-dataset&#34;&gt;Outcome for mnist dataset&lt;/h2&gt;
&lt;img src=&#34;/img/tsne_02.png&#34;&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perplexity Intuition (and its derivation) - Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/lvdmaaten/bhtsne&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub - lvdmaaten/bhtsne: Barnes-Hut t-SNE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://lvdmaaten.github.io/tsne/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;t-SNE – Laurens van der Maaten&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>机器学习面试总结</title>
      <link>/post/ml/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/ml/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/</guid>
      <description>&lt;h2 id=&#34;讲一下随机森林gbdtxgboost&#34;&gt;讲一下随机森林，GBDT，XGBoost&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;随机森林：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;集成算法， 有放回的随机选择特征， 构建决策树&lt;/li&gt;
&lt;li&gt;不同基学习器是独立的&lt;/li&gt;
&lt;li&gt;特征随机和和样本随机&lt;/li&gt;
&lt;li&gt;bagging 方法， 最后做投票&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;集成学习，方差偏差都比较低， 泛化性能比较好&lt;/li&gt;
&lt;li&gt;高维数据集处理能力很好，并确定最重要的变量&lt;/li&gt;
&lt;li&gt;可以处理确缺失数据&lt;/li&gt;
&lt;li&gt;可以并行&lt;/li&gt;
&lt;li&gt;不需要归一化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;回归问题表现不好，不能连续输出&lt;/li&gt;
&lt;li&gt;无法了解模型内部&lt;/li&gt;
&lt;li&gt;忽略属性之间的相关性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参数调配&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网格搜索， GridSearchCV&lt;/li&gt;
&lt;li&gt;贪心算法， 固定其他参数， 把这个参数选好&lt;/li&gt;
&lt;li&gt;随机搜索&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GBDT (Gradient Boosting Decsion Tree)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每一次计算减少上次出现的残差， 为了消除残差，训练一个新的模型， 训练好的弱分类器累加到现有模型中&lt;/li&gt;
&lt;li&gt;回归树，不是分类树。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XGBoost&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GBDT 是机器学习算法， XGBoost 是该算法的实现&lt;/li&gt;
&lt;li&gt;GBDT 算法基于经验损失函数的负梯度构造新的决策树， 在构建完成之后进行剪枝， 而 XGBoost 在决策树构造阶段加入了正则项&lt;/li&gt;
&lt;li&gt;GBDT 在模型只使用了一阶导数信息， XGBoost 对损失函数进行二阶泰勒展开。&lt;/li&gt;
&lt;li&gt;根据百分位列举几个可能成为分隔点的候选者， 从中找最佳的分割点&lt;/li&gt;
&lt;li&gt;传统 GBDT 采用 CART 作为基分类器， XGBoost 采用了与随机森林类似的策略， 支持对数据采样&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;梯度提升和梯度下降的区别&#34;&gt;梯度提升和梯度下降的区别&lt;/h2&gt;
&lt;p&gt;利用损失函数相对于模型的负梯度信息来对模型进行更新， 梯度下降过程中， 模型以参数化形式表示，而梯度提升， 模型不是以参数化形式表示，直接定义在函数空间中， 大大提高了可以使用的模型种类。&lt;/p&gt;
&lt;h2 id=&#34;随机森林如何处理缺失值&#34;&gt;随机森林如何处理缺失值&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;数值型变量：给缺失值一些估计值，例如众数和中位数 描述型变量：缺失部分用出现次数最多的数代替&lt;/li&gt;
&lt;li&gt;利用中位数和出现次数做最多的数代替，引入权重， 需要替换的数据先和其他数据做相似度测量, 补全缺失的点是相似的点会具有较高的权重 (所有数据放进随机森林运行一遍，记录每一组数据在决策树中分类路径，判断哪组数据和缺失数据路径最相似， 引入相似度矩阵，记录数据之间的相似度)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;adaboost-和-xgboost-区别&#34;&gt;AdaBoost 和 XGBoost 区别&lt;/h2&gt;
&lt;h2 id=&#34;详细解释-auc&#34;&gt;详细解释 AUC&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;TP: 实际这正类， 预测为正类&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TN: 实际为负类，预测为负类&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FN: 实际为正类， 预测为负类 漏报&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FP: 实际为负类，预测为正类 误报&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TPR = TP / (TP + FN) // 正实例被预测正确的比例 (RECALL) precision 指的是分类正确的正样本个数占分类器判断为正样本的个数比例&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FPR = FP / (FP + TN) // 负实例被误报的比例&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;选定不同的阈值， 判断是否为正例， 不同的阈值对应不同的指标， 把这些指标连接起来，即为 ROC 曲线&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解释： 从所有为1的样本从随机选择一个样本， 从所有为0的样本中选择一个样本， 根据分类器对两个样本进行预测，样本1预测为1 的概率为 p1, 样本2预测的概率为 p2, p1 &amp;gt; p2 的概率为AUC&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;随机森林如何评估特征的重要性-xgboost-如何寻找最优特征&#34;&gt;随机森林如何评估特征的重要性， Xgboost 如何寻找最优特征&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;基于基尼指数
看每个特征在随机森林中的每棵树做了多少共享， 取平均值，利用Gini指数, 算利用这个特征分离后的基尼指数之差，取上平均值即可判定&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于带外数据
未被抽取样本的集合，使用袋外数据计算第 T_i 颗决策树的校验误差 e1，随机改变 OOB 中的第 J 列， 保持其他列不变， 对第j列进行随机上下置换， 得到误差e2, 利用 e1 - e2  来刻画特征 $j$ 的重要性. 这里可以选择不同的树来加权平均。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Xgboost
xgboost 在训练过程中可以给出各个特征的评分， 最大的增益会被选出作为依据， 从而记忆了每个特征在模型训练时的重要性
xgboost 属于 boosting 集成方法， 样本不放回，每轮计算样本不重复。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;欧氏距离和其他距离的区别&#34;&gt;欧氏距离和其他距离的区别&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;欧氏距离样本不同属性之间的差别等同看待， 这有时候不满足条件, 当其他特征比较大是，很多特征接近于 0， 没有考虑维度之间的相关性&lt;/li&gt;
&lt;li&gt;曼哈顿距离： 异常值的分类结果影响比欧式距离药效， 各维度的贡献基本一样。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;logistic-regression-为什么需要对特征进行离散化&#34;&gt;Logistic Regression 为什么需要对特征进行离散化&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;LR属于广义线性模型， 表达能力受限， 离散化为 N 个后， 每个变量有单独的权重， 为模型引入了非线性， 提升了模型的表达能力&lt;/li&gt;
&lt;li&gt;速度快，稀疏向量内积乘法运算速度较快， 计算结果更容易存储和扩展&lt;/li&gt;
&lt;li&gt;更具有鲁棒性，数据偏离后也不会造成较大影响&lt;/li&gt;
&lt;li&gt;离散后的特征可以进行交叉， 提升表达能力&lt;/li&gt;
&lt;li&gt;特征离散化后，会更加稳定&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lr-与-svm-区别和联系&#34;&gt;LR 与 SVM 区别和联系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;相同点
&lt;ul&gt;
&lt;li&gt;都是分类算法， 本质上都是找超平面&lt;/li&gt;
&lt;li&gt;监督学习算法&lt;/li&gt;
&lt;li&gt;判别式模型， 不关心数据怎么生成， 只关心数据的差别， 然后利用数据的差别来进行分类&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;不同点
&lt;ul&gt;
&lt;li&gt;SVM 只考虑了 support vectors, 也就是和分类最相关的少数点去学习分类器。而逻辑回归通过非线性映射减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重；&lt;/li&gt;
&lt;li&gt;LR 的损失函数是交叉熵， SVM 的损失函数是 hinge loss, SVM 对样本数据的依赖减少， 提高了训练效率&lt;/li&gt;
&lt;li&gt;LR 是参数模型， SVM 是非参数化模型， 参数模型是指数据服从某一分布， 该分布由一些参数确定。非参数模型不需要对数据的总体分布做假设，只知道总体是一个随机变量，不需要知道分布的具体形式。&lt;/li&gt;
&lt;li&gt;LR 可以得到每一个类的概率， SVM 无法得到&lt;/li&gt;
&lt;li&gt;LR 不依赖与样本之间的距离， SVM 基于样本之间的距离&lt;/li&gt;
&lt;li&gt;解决非线性问题时， SVM 可以采用核函数机制， LR 通常不采样核函数计算&lt;/li&gt;
&lt;li&gt;SVM 损失函数自带正则， 而 LR 需要在损失函数之外添加正则项&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;什么是熵-联合熵-条件熵-相对熵-互信息&#34;&gt;什么是熵， 联合熵， 条件熵， 相对熵， 互信息&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;熵
衡量系统的有序成都， 熵越大，系统越混乱， 信息熵表示离散事件的出现概率， 一个系统越是有序，其信息熵则越低。&lt;/li&gt;
&lt;li&gt;联合熵
两个变量的联合分布，H(x,y)&lt;/li&gt;
&lt;li&gt;条件熵
在随机变量 X 发生的前提下， 随机变量 Y 发生so带来的熵的定义则为 条件熵  H(Y|X)
H(Y|X) = H(X,Y) - H(X)&lt;/li&gt;
&lt;li&gt;相对熵， KL-Divergence
D(q||p)&lt;/li&gt;
&lt;li&gt;互信息
两个随机变量X，Y的联合分布和各自独立分布乘积的相对熵&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;牛顿法和梯度梯度下降法&#34;&gt;牛顿法和梯度梯度下降法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;牛顿法是二阶收敛， 梯度法是一阶收敛， 两种都只是局部算法， 梯度法仅考虑了方向， 牛顿法不仅考虑了方向，还兼顾了步长&lt;/li&gt;
&lt;li&gt;牛顿法需要没次计算 Hessian 矩阵， 计算比较复杂&lt;/li&gt;
&lt;li&gt;拟牛顿法使用正定矩阵来近似 Hessian 矩阵， 从而简化了运算的复杂度。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kmeans-算法复杂度&#34;&gt;Kmeans 算法复杂度&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;O(tKmn): repeat t times, has K centroids, m data, n features;&lt;/li&gt;
&lt;li&gt;O((m + k)n) : m data, n feautres, K centroids.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;协方差和相关性区别&#34;&gt;协方差和相关性区别&lt;/h2&gt;
&lt;p&gt;相关性是协方差的标准化形式， 协方差本身很难作比较， 而相关性可以把数值scale 到 -1， 1 之间， 因此可以比较其相关性&lt;/p&gt;
&lt;h2 id=&#34;navie-bayes&#34;&gt;Navie Bayes&lt;/h2&gt;
&lt;p&gt;因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的.&lt;/p&gt;
&lt;h2 id=&#34;详细说些-em-算法&#34;&gt;详细说些 EM 算法&lt;/h2&gt;
&lt;p&gt;在概率模型中寻找参数的极大似然估计算法, 最大化后验概率，概率模型依赖于无法观测的隐变量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E步: 假设参数已知，计算隐变量的后验概率。&lt;/li&gt;
&lt;li&gt;M步：带入隐变量的后验概率，最大化似然估计，计算参数值
M 步得到的参数值用于 E 步计算中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;似然函数意义： 通过观测的数据推测参数值, 假设参数已经知道，使得观测的数据概率最大。&lt;/p&gt;
&lt;h2 id=&#34;knn-中-k--如何选择&#34;&gt;KNN 中 K  如何选择&lt;/h2&gt;
&lt;p&gt;选择较小的 K 值， 较小领域中的训练实例来预测，学习的近似误差变小， 学习的估计误差增大， 即模型容易发生过拟合。 选择较大的 K 值， 容易发生欠拟合， 增大意味着模型变得简单
KNN 为有监督学习的算法， 一般用交叉验证的方法选择最优的K值&lt;/p&gt;
&lt;h2 id=&#34;最大似然估计和贝叶斯估计的区别&#34;&gt;最大似然估计和贝叶斯估计的区别&lt;/h2&gt;
&lt;p&gt;最大似然是对点估计， 贝叶斯推断是对分布的估计
最大似然估计求出的是最有可能的参数值， 而贝叶斯推断求解的是参数的分布
另外，贝叶斯推断加入了先验概率， 通过先验和似然来求解后验分布，最大似然直接使用似然函数&lt;/p&gt;
&lt;h2 id=&#34;什么是最大熵模型&#34;&gt;什么是最大熵模型&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>矩阵求导</title>
      <link>/post/ml/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/ml/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</guid>
      <description>&lt;h2 id=&#34;定义&#34;&gt;定义&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;标量 $f$ 对矩阵 $\mathbf{X}$ 的导数， 定义为
$$\frac{\partial f}{\partial \mathbf{X}} = \left[\frac{\partial f}{\partial X_{ij}}\right]$$
在求导时不宜拆开矩阵， 需要找到一个整体的算法&lt;/li&gt;
&lt;li&gt;一元微积分中的导数与微分的关系有
$$df = f&amp;rsquo;(x) dx$$&lt;/li&gt;
&lt;li&gt;多元的导数与微分的关系
$$df = \sum_{i=1}^{n}\frac{\partial f}{\partial x_i} dx_i = \frac{\partial f}{\partial \mathbf{x}}^{t} d\mathbf{x}$$&lt;/li&gt;
&lt;li&gt;矩阵导数与微分建立联系：
$$d f=\sum_{i=1}^{m} \sum_{j=1}^{n} \frac{\partial f}{\partial X_{i j}} d X_{i j}=\operatorname{tr}\left(\frac{\partial f^{T}}{\partial X} d X\right)$$
$tr$ 表示方阵对角元素之和&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;运算法则&#34;&gt;运算法则&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;加减法 $d(\mathbf{X}\pm \mathbf{Y}) = d\mathbf{X} \pm d\mathbf{Y}$&lt;/li&gt;
&lt;li&gt;乘法 $d(\mathbf{XY}) = (d\mathbf{X})\mathbf{Y} + \mathbf{X}d\mathbf{Y}$&lt;/li&gt;
&lt;li&gt;转置 $d(\mathbf{X}^T) = (d\mathbf{X})^T$&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;参考 matrix cookbook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;计算&#34;&gt;计算&lt;/h2&gt;
&lt;p&gt;建立矩阵与微分联系时， 在求出左侧的微分 $df$, 需要写出右侧形式的导数
$$df = tr\left(\frac{\partial f}{\partial \mathbf{X}}^T d\mathbf{X}\right)$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标量上迹 $a = tr(a)$&lt;/li&gt;
&lt;li&gt;转置 $tr(A^T) = tr(A)$&lt;/li&gt;
&lt;li&gt;加减法 $tr(A\pm B) = tr(A) \pm tr(B)$&lt;/li&gt;
&lt;li&gt;矩阵乘法交换 $tr(AB) = tr(BA)$&lt;/li&gt;
&lt;li&gt;矩阵乘法/逐元乘法交换 $\text{tr}(A^T(B \odot C))=\text{tr}((A\odot B)^TC)$, 其中 A, B, C 尺寸相同&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Linear Regression $l = || \mathbf{Xw - y}||^2$,  solve $\mathbf{w}, \mathbf{y}$ is a $m\times 1$ vector, $\mathbf X$ has the shape $m\times n$, and $w$&#39;s shape is $n\times 1$, and the $l$ is a scalar&lt;/p&gt;
&lt;p&gt;In this example, we need to solve a scalar&amp;rsquo;s derivate with respect to a vector. As the defintion, we can&amp;rsquo;t calcuate the derivate of a vector or a matrix directly. Instead, we need to calcuate the differential of a matrix.
$$\begin{aligned}l &amp;amp;= \mathbf{(Xw - y)}^T(\mathbf{Xw - y}) \&lt;br&gt;
dl &amp;amp;= (X d \boldsymbol{w})^{T}(X \boldsymbol{w}-\boldsymbol{y})+(X \boldsymbol{w}-\boldsymbol{y})^{T}(X d \boldsymbol{w})\&lt;br&gt;
\end{aligned}$$
As the first and second form is a dot product between two vector, so we can get
$$dl = 2(X \boldsymbol{w}-\boldsymbol{y})^{T} \boldsymbol{X} d \boldsymbol{w}$$
The differential form of $dl$
$$dl = \frac{\partial l}{\partial \mathbf{w}}^T d\mathbf{w}$$
The formula transforms to
$$\frac{\partial l}{\partial \boldsymbol{w}}=2 X^{T}(X \boldsymbol{w}-\boldsymbol{y})$$
Set the partial form to $0$, we could get
$$\mathbf{X^TXw} = \mathbf{X^Ty}$$
and the $w$ will be
$$\mathbf{w = (X^TX)^{-1}X^Ty}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://zhuanlan.zhihu.com/p/24709748&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;矩阵求导术（上）知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;matrix cookbook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Ensemble Method</title>
      <link>/post/ml/ensemble/</link>
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/post/ml/ensemble/</guid>
      <description>&lt;p&gt;Combine multiple base-learners to imporve applicability across different domains or generalization performance in a specific task&lt;/p&gt;
&lt;h2 id=&#34;ensemble-strategy&#34;&gt;Ensemble Strategy&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;将多个学习器结合可以带来三个方面的好处&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;增强泛化性能&lt;/li&gt;
&lt;li&gt;计算有可能陷入局部最小， 降低局部最小的风险&lt;/li&gt;
&lt;li&gt;扩大假设空间


















&lt;figure id=&#34;figure-three-advantages&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/ensemble_01.png&#34; data-caption=&#34;Three advantages&#34;&gt;


  &lt;img src=&#34;/img/ensemble_01.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Three advantages
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Average&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple averaging
$$H(X) = \frac1T \sum_{i=1}^{T} h_i(x)$$&lt;/li&gt;
&lt;li&gt;Weighted averaging
$$H(x) = \sum_{i=1}^{T} \omega_i h_i(x)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Voting&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;learner $h_i$ predicte a label from the collection of class ${c_1, c_2, \dots, c_N}$. The learner $h_i$ generate a vector $(h_i^1(x); h_i^2(x); \dots; h_i^N(x))$ from the sample $x$.&lt;/li&gt;
&lt;li&gt;Marority voting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$H(\boldsymbol{x})=\left{\begin{array}{ll}{c_{j},} &amp;amp; {\text { if } \sum_{i=1}^{T} h_{i}^{j}(\boldsymbol{x})&amp;gt;0.5 \sum_{k=1}^{N} \sum_{i=1}^{T} h_{i}^{k}(\boldsymbol{x})} \ {\text { reject, }} &amp;amp; {\text { otherwise. }}\end{array}\right.$$&lt;/p&gt;
&lt;p&gt;可以拒绝， 即保证要提供可靠的结果&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plurality voting (预测为票数最多的标记)
$$H(\boldsymbol{x})=c_{\arg \max_j  \sum_{i=1}^{T} h_{i}^{j}(\boldsymbol{x})}$$&lt;/li&gt;
&lt;li&gt;Weighted Voting (加权平均)
$$H(\boldsymbol{x})=c_{\arg \max_j  \sum_{i=1}^{T} \omega_i h_{i}^{j}(\boldsymbol{x})}$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;学习法(stacking)
Stacking 从初始数据集训练出初级学习器， 然后生成一个新数据集用于训练次级学习器。 在新数据集中， 初级学习器的输出被当做样例输入特征， 而初始样本的标记被当做样例标记


















&lt;figure id=&#34;figure-stacking-method&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/ensem_02.png&#34; data-caption=&#34;Stacking Method&#34;&gt;


  &lt;img src=&#34;/img/ensem_02.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Stacking Method
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若直接使用初级学习器的训练集产生次级训练集， 则过拟合的风险较大&lt;/li&gt;
&lt;li&gt;K-Fold： 初始训练集被划为$k$ 个大小相似的集合 $D_1, D_2, \dots D_k$, 令 $\overline{D}_j = D \backslash D_j$
&lt;ul&gt;
&lt;li&gt;给定 T 个 base learner, 初级学习器 $h_t^j$ 通过在 $\overline{D_j}$ 上使用 第$t$ 个学习算法&lt;/li&gt;
&lt;li&gt;对 $D_j$ 中每个样本 $x_i$, 令$z_{it} = h_t^j(x_i)$, 则由 $x_i$ 产生的次级训练样例的示例部分为 $z_i = (z_{i1}; z_{i2}; \dots z_{iT})$, 标记部分为 $y_i$&lt;/li&gt;
&lt;li&gt;交叉验证过程结束后， 从T个初级学习器产生的次级训练集为 $D&#39;=(Z_i, y_i)_{i=1}^m$&lt;/li&gt;
&lt;li&gt;将 $D&#39;$ 用于训练次级学习器&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;bagging&#34;&gt;Bagging&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bagging is a voting method, but base-learners are made different deliberately
&lt;ul&gt;
&lt;li&gt;Train them using slightly different training sets&lt;/li&gt;
&lt;li&gt;Generate $L$ slightly different samples from a given sample is done by &lt;strong&gt;bootstrap&lt;/strong&gt;: given $X$ of size N, we draw N points randomly from $X$ with replacement to get $X^{(j)}$&lt;/li&gt;
&lt;li&gt;Train a base-learner for ecah $X^{(j)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;boosting&#34;&gt;Boosting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In bagging, generate &amp;ldquo;uncorrelated&amp;rdquo; base-learners is left to chance and unstability of the learning method. (让下面的 learner train 前几个learner错误的地方)&lt;/li&gt;
&lt;li&gt;考虑 binary classification: $d^{(j)}(x) \in {1, -1}$&lt;/li&gt;
&lt;li&gt;Cominber three &lt;code&gt;weak learners&lt;/code&gt; to generate a &lt;code&gt;strong learner&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;A week learner has error probability less than 1/2&lt;/li&gt;
&lt;li&gt;A strong learner has arbitrarily small error probability&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Traning Algorithm
&lt;ol&gt;
&lt;li&gt;Given a large traning set, randomly divide in into three&lt;/li&gt;
&lt;li&gt;Use $X^{(1)}$ to train the first learner $d^{(1)}$ and feed $X^{(2)}$ to $d^{(1)}$&lt;/li&gt;
&lt;li&gt;Use all points misclassified by $d^{d(1)}$ and $X^{(2)}$ to train $d^{(2)}$. Then feed $X^{(3)}$ to $d^{(1)}$ and $d^{(2)}$&lt;/li&gt;
&lt;li&gt;Use the points on which $d^{(1)}$ and $d^{(2)}$ disgree to train $d^{(3)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Tesing Algorithm
&lt;ol&gt;
&lt;li&gt;Feed a point it to $d^{(1)}$ and $d^{(2)}$ first. If outpus agree, use them as final predction&lt;/li&gt;
&lt;li&gt;Otherwise the output of $d^{(3)}$ is taken&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Example


















&lt;figure id=&#34;figure-boosting-example&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/ensem_03.jpg&#34; data-caption=&#34;Boosting example&#34;&gt;


  &lt;img src=&#34;/img/ensem_03.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Boosting example
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Disadvantages
Requires a large traning set to afford the three-way split&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;adaboost&#34;&gt;AdaBoost&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the same training set over and over again, but how to make points &amp;ldquo;larger&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Modify the probabilities of drawing the instances as a function of error&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Notation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\text{Pr}^{(i,j)}$: probability that an example $x^{(i)}, y^{(i)}$ is drawn to train the $j$th base-learner $d^{(j)}$&lt;/li&gt;
&lt;li&gt;$\varepsilon^{(j)} = \sum_i \text{Pr}^{(i,j)} 1(y^{(i)} \neq d^{(j)}x^{(i)})$  error rate of $d^{(j)}$ on its training set.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Algorithm&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initalize $\text{Pr}^{(i,j)} = \frac1N$ for all i&lt;/li&gt;
&lt;li&gt;start from $j = 1$:
&lt;ol&gt;
&lt;li&gt;Randomly draw $N$ examples for $X$ with probabilities $\text{Pr}^{(i,j)}$ and use them to train $d^{(j)}$&lt;/li&gt;
&lt;li&gt;Stop adding new base-learners if $\varepsilon^{(j)} \geq \frac12$&lt;/li&gt;
&lt;li&gt;Define $\alpha_j=\frac12 \log \left( \frac{1-\varepsilon^{(j)}}{\varepsilon^{(j)}}\right)&amp;gt;0$ and set $\text{Pr}^{(i,j+1)}  = \text{Pr}^{(i,j)} \exp(-\alpha_j y^{(i)} d^{(j)} (x^{(i)}))$ for all $i$&lt;/li&gt;
&lt;li&gt;Normalize $\Pr^{(i, j+1)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Testing&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given $x$, calcualte $\hat{y}^{(j)}$ for all $j$&lt;/li&gt;
&lt;li&gt;Make final prediction $\hat{y}$ by voting $\hat{y} = \sum_j \alpha_j d^{(j)}(x)$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example
&lt;img src=&#34;/img/ensem_04.jpg&#34; width=&#34;400px&#34;/&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why AdaBoost Works&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Empirical study: AdaBoost reduces overfitting as $L$ grows, even when there is no training error
&lt;img src=&#34;/img/ensem_05.jpg&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;AdaBoost &lt;strong&gt;increases margin&lt;/strong&gt; (Same as SVM)
&lt;ul&gt;
&lt;li&gt;A large margin imporves generalizability&lt;/li&gt;
&lt;li&gt;Define margin of prediction of an example $(x^{(i)}, y^{(i)}) \in X$ as:
$$margin(x^{(i)}, y^{(i)}) = y^{(i)}f(x^{(i)}) = \sum_{j:y^{(i)} = d^{(j)}(x^{(i)})} \alpha_j - \sum_{j:y^{(i)} \neq d^{(j)}(x^{(i)})} \alpha_j$$
&lt;img src=&#34;/img/ensem_06.png&#34; width=&#34;600px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Combining the Biased and Unbiased Sampling Strategy into One Convenient Free Energy Calculation Method</title>
      <link>/publication/mixing-remd/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/publication/mixing-remd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>FSATOOL: A useful tool to do the conformational sampling and trajectory analysis work for biomolecules</title>
      <link>/publication/fsatool/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/publication/fsatool/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>/courses/computer_network/introduction/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/computer_network/introduction/</guid>
      <description>&lt;h2 id=&#34;computing-devices&#34;&gt;Computing devices&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hosts (= end systems)  (PC workstations, servers, Phones) running network apps&lt;/li&gt;
&lt;li&gt;Communication Links (fiber, copper, radio, satellite) physical link
&lt;ul&gt;
&lt;li&gt;Transmission rate = bandwidth&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;routers: forward packets (chunks of data)&lt;/li&gt;
&lt;li&gt;protocols: control sending, receiving of messages (TCP, IP, HTTP, FTP, PPP)&lt;/li&gt;
&lt;li&gt;Internet: network of networks, loosely hierarchical, public Internet(有限的) versus private Intranet&lt;/li&gt;
&lt;li&gt;Internet standards (定标准的组织)
&lt;ul&gt;
&lt;li&gt;RFC request for comments&lt;/li&gt;
&lt;li&gt;IETF: Internet Engineering Task Force&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;communication infrastructure enables distributed applications (Web, email, games)&lt;/li&gt;
&lt;li&gt;communication services provided to applications(connectionless, connection-oriented)&lt;/li&gt;
&lt;li&gt;Protocol
&lt;ul&gt;
&lt;li&gt;all communication activity in Internet governed by protocols&lt;/li&gt;
&lt;li&gt;format and order of messages sent and received among network entities as well as actions taken on the transmission/receipt of a message&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;network edge : applications and hosts&lt;/li&gt;
&lt;li&gt;network core: routers, network of networks&lt;/li&gt;
&lt;li&gt;access networks(连接 edge 和 core, 连接 physical media, communication links)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-network-edge&#34;&gt;The network edge&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;end systems(hosts) run application programs&lt;/li&gt;
&lt;li&gt;client/server model: client host requests, receives service from always-on server&lt;/li&gt;
&lt;li&gt;peer-peer model : minimal(no) use of dedicated server&lt;/li&gt;
&lt;li&gt;TCP services (Transmission Control protocol)
&lt;ul&gt;
&lt;li&gt;reliable, in-order byte-stream data transfer(loss: 掉的做法是 acknowledgements and retransmissions)&lt;/li&gt;
&lt;li&gt;Flow control, sender won&amp;rsquo;t overwhelm receiver 流量控制 (sender 可以送多快是由 receiver 决定的)&lt;/li&gt;
&lt;li&gt;congestion control, senders slow down sending rate when networks congested (connection太多时， 网络发生拥挤， 会放慢速度, 逐渐增大封包量， 如果封包掉了，认为网络拥挤)&lt;/li&gt;
&lt;li&gt;E.g. HTTP(web), FTP(File transfer), SMTP(email)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;UDP (User Datagram Protocol) 不需要建连线
&lt;ul&gt;
&lt;li&gt;unreliable data transfer&lt;/li&gt;
&lt;li&gt;no flow control&lt;/li&gt;
&lt;li&gt;no congestion control&lt;/li&gt;
&lt;li&gt;E.g. streaming media(掉了不会影响), teleconferencing, DNS, Internet telephony&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;network-access-and-physical-media&#34;&gt;Network access and physical media&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How to connect end systems to edge router?
&lt;ul&gt;
&lt;li&gt;residential access nets&lt;/li&gt;
&lt;li&gt;institutional access networks (school, company)&lt;/li&gt;
&lt;li&gt;mobile access networks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;keep in mind:
&lt;ul&gt;
&lt;li&gt;bandwidth (bits per second) of access network&lt;/li&gt;
&lt;li&gt;shared or dedicated&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;residential-access&#34;&gt;Residential access&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;dialup via modem
&lt;ul&gt;
&lt;li&gt;up to 56Kbps direct access to router (often less)&lt;/li&gt;
&lt;li&gt;can&amp;rsquo;t surf and phone at same time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ADSL (asymmetric digital subscriber line)
&lt;ul&gt;
&lt;li&gt;up to 1 Mpbs upstream&lt;/li&gt;
&lt;li&gt;up to 8 Mpbs downstream&lt;/li&gt;
&lt;li&gt;FDM 50KHz - 1MHz for downstream, 4KHz-50kHZ for upstream, 0kHZ-4kHZ for ordinary telephone&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;HFC: hybrid fiber coaxial cable, asymmetric, up to 30 Mbps downstream, 2Mpbs upstream
&lt;ul&gt;
&lt;li&gt;network of cable and fiber attaches homes to ISP router, shared access to router among home, issues(congestion, dimensioning)&lt;/li&gt;
&lt;li&gt;deployment: available via cable components, e.g. MediaOne&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;company-access&#34;&gt;Company access&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;company/university local area network (LAN) connects end system to edge router&lt;/li&gt;
&lt;li&gt;Ethernet: shared or dedicated link connects end system and router(10Mbps, 100Mpbs, Gigabit Ethernet)&lt;/li&gt;
&lt;li&gt;deployment: institutions, home&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;wireless-access-networks&#34;&gt;Wireless access networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;shared wireless access network connects end system to router, via base station&lt;/li&gt;
&lt;li&gt;wireless LANs: 802.11b (WiFI): 11 Mbps, 802.11g: 54Mbps&lt;/li&gt;
&lt;li&gt;wider-area wireless access, provided by telecom operator, 3G/4G, WAP/GPRS&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;physical-media&#34;&gt;Physical Media&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bit: propagates between transmitter/receiver pairs&lt;/li&gt;
&lt;li&gt;Physical link: what lies between transmitter &amp;amp; receiver
&lt;ul&gt;
&lt;li&gt;Guided media, by solid media
&lt;ul&gt;
&lt;li&gt;Twisted Pairs(tow insulated copper wires)&lt;/li&gt;
&lt;li&gt;Coaxial cable, two concentric copper conductors, bidirectional(baseband:single channel on cable, broadband: multiple channels on cable)&lt;/li&gt;
&lt;li&gt;Fiber optic cable: glass fiber carrying light pulses, each pulse a bit, high-speed operation, low error rate: repeaters spaced far apart, immune to electromagnetic noise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;unguided media, no physical line (effected by reflection, obstruction of objects, interference)
&lt;ul&gt;
&lt;li&gt;terrestrial microwave (45 Mpbs channels)&lt;/li&gt;
&lt;li&gt;LAN (e.g. Wifi)&lt;/li&gt;
&lt;li&gt;wide-area (e.g. cellular)&lt;/li&gt;
&lt;li&gt;satellite(50 Mpbs channel, 270 millisecond end-end delay)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;network-core&#34;&gt;Network core&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;连接成网状 mesh of interconnected routers&lt;/li&gt;
&lt;li&gt;how is data transferred through net?
&lt;ul&gt;
&lt;li&gt;Circuit Switching: dedicated circular per call: telephone net&lt;/li&gt;
&lt;li&gt;packet-switching: data sent through net in discrete &amp;ldquo;chunks&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;circuit-switching&#34;&gt;Circuit Switching&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;end to end resources reserved(保留一定的频宽)&lt;/li&gt;
&lt;li&gt;保留 link bandwidth, switch capacity, divided into pieces&lt;/li&gt;
&lt;li&gt;dedicated resources: no sharing (别的core无法使用)&lt;/li&gt;
&lt;li&gt;circuit-like (guaranteed) performance&lt;/li&gt;
&lt;li&gt;call setup required(建连线)
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c94cfc339ab9.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;two splits (TDM)
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c94d0e9d088e.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;packet-switching&#34;&gt;Packet Switching&lt;/h3&gt;
&lt;p&gt;each end-end data stream divided into packets, user A,B packets share network resources, each packet uses &lt;strong&gt;full link bandwidth&lt;/strong&gt;, resources used as needed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;resource contention (竞争)
&lt;ul&gt;
&lt;li&gt;aggregate resource demand can exceed amount available&lt;/li&gt;
&lt;li&gt;congestion: pockets queue, wait for link use&lt;/li&gt;
&lt;li&gt;store and forward, packets move one hop at a time(先排队， 后发送), transmit over link, wait turn at next link
&lt;img src=&#34;https://i.loli.net/2019/03/22/5c94d24556aa0.png&#34; width=&#34;400px&#34;/&gt;
sequence of A &amp;amp; B packets done not have fixed pattern $\rightarrow$ statistical multiplexing, in TDM each host get some slot in revolving TDM frame&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;move packets through routers form source to destination (path selection algorithms)&lt;/li&gt;
&lt;li&gt;datagram network:
&lt;ul&gt;
&lt;li&gt;destination address in packet determines next hop&lt;/li&gt;
&lt;li&gt;routes may change during session&lt;/li&gt;
&lt;li&gt;analogy, driving, asking directions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;virtual circuit network
&lt;ul&gt;
&lt;li&gt;each packet carries tag (virtual circuit ID), tag determines next hop&lt;/li&gt;
&lt;li&gt;fixed path determined at call setup time, remains fixed through call&lt;/li&gt;
&lt;li&gt;routers maintain per-call state (每一个标签需要记住)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;packet-switching-vs-circuit-switching&#34;&gt;Packet Switching VS. Circuit Switching&lt;/h3&gt;
&lt;p&gt;Packet switching allows more user to use network Example: 1Mbit link,  each user 100kbps when &amp;ldquo;active&amp;rdquo;, active 10% of per time&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Circuit-Switching : 10 users&lt;/li&gt;
&lt;li&gt;Packet-Switching: with 35 users, probability &amp;gt; 10 active less than 0.004&lt;/li&gt;
&lt;li&gt;Packet Switching: greater for bursty data, resource sharing, simpler, no call setup&lt;/li&gt;
&lt;li&gt;Excessive congestion: packet delay and loss, protocols needed for reliable data transfer, congestion control&lt;/li&gt;
&lt;li&gt;How to provide circuit-like behavior? bandwidth guarantees needed for audio/video apps, still an unsolved problem&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;internet-structure-network-of-network&#34;&gt;Internet Structure: network of network&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;roughly hierarchical&lt;/li&gt;
&lt;li&gt;Tier 1 ISP(internet service provider) (e.g. UUNet, Sprint, AT&amp;amp;T) national/international coverage (treat each other as equals)&lt;/li&gt;
&lt;li&gt;regional ISP, connect to one or more tire-1 ISPs&lt;/li&gt;
&lt;li&gt;IXP (Internet Exchange point): meeting point where multiple ISPs can peer together
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c96e8bd04c95.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;delay-loss-in-packet-switched-networks&#34;&gt;Delay, Loss in Packet-Switched Networks&lt;/h2&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c96eb7a50ee6.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;Packets queue in router buffers
&lt;ul&gt;
&lt;li&gt;packet arrive rate to link exceeds output link capacity(packets queue, wait for turn)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;types
&lt;ul&gt;
&lt;li&gt;Processing delay, the time required to examine the packet&amp;rsquo;s header and determine where to direct to packet&lt;/li&gt;
&lt;li&gt;queuing delay, the packet waits to be transmitted onto the link, depends on congestion level of router&lt;/li&gt;
&lt;li&gt;transmission delay, time to send bits into link L/R (L: packet length, R: link bandwidth), 多快的速度可以从(router)送出去&lt;/li&gt;
&lt;li&gt;propagation delay, d/s = (s: propagation speed in medium, d = length of physical link)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Transmission and Propagation Delay
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c96edc7b4186.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;nodal delay
$$d_{nodal} = d_{proc} + d_{queue} + d_{trans} + d_{prop}$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;queueing-delay&#34;&gt;Queueing delay&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;L: packet length (bits)&lt;/li&gt;
&lt;li&gt;a: average packet arrival rate&lt;/li&gt;
&lt;li&gt;R: link bandwidth(bps)&lt;/li&gt;
&lt;li&gt;transfer intensity = (L*a) / R&lt;/li&gt;
&lt;li&gt;(L*a) / R ~ 0 : average queueing delay small&lt;/li&gt;
&lt;li&gt;(L*a) / R $\rightarrow$ 1: delays become large&lt;/li&gt;
&lt;li&gt;(L*a) /R &amp;gt; 1: more work arriving than can be serviced, average delay infinite, or packet loss
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c96f0920a293.png&#34; width=&#34;400px&#34;/&gt;&lt;/li&gt;
&lt;li&gt;traceroute program: provides delay measurement from source to router along end-end internet path towards destination&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;packet-loss&#34;&gt;Packet Loss&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Queue(also known as buffer) preceding link in buffers has finite capacity&lt;/li&gt;
&lt;li&gt;when packet arrives to full queue, packet is dropped&lt;/li&gt;
&lt;li&gt;lost packet may be retransmitted by pervious node, by source and system, or not retransmitted at all&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;protocol-layers&#34;&gt;Protocol Layers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Is there any hope of organizing structure of network&lt;/li&gt;
&lt;li&gt;Layers: each layer implements a service
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c96f3fa35287.png&#34; width=&#34;400px&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;application: supporting network applications (FTP, SMTP, HTTP)&lt;/li&gt;
&lt;li&gt;transfer: host-host data transfer (TCP, UDP)&lt;/li&gt;
&lt;li&gt;network: routing of datagrams from source to destination. IP, routing protocols&lt;/li&gt;
&lt;li&gt;Link: data transfer between neighboring network elements (PPP, Ethernet)&lt;/li&gt;
&lt;li&gt;physical: bits &amp;ldquo;on the wire&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;why layering:
&lt;ul&gt;
&lt;li&gt;explicit structure allows identification, relationship of complex system&amp;rsquo;s pieces&lt;/li&gt;
&lt;li&gt;modularization eases maintenance, updating of system, change  of implementation of layer&amp;rsquo;s service transparent to rest of system.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;encapsulation&#34;&gt;Encapsulation&lt;/h3&gt;
&lt;img src=&#34;https://i.loli.net/2019/03/24/5c96f7453b640.png&#34; width=&#34;450px&#34;/&gt;
</description>
    </item>
    
    <item>
      <title>fortran programming</title>
      <link>/post/fortran-programming/</link>
      <pubDate>Mon, 15 Apr 2019 16:59:45 +0000</pubDate>
      <guid>/post/fortran-programming/</guid>
      <description>&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#模块创建显示接口&#34;&gt;模块创建显示接口&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#函数子程序作为参数传递&#34;&gt;函数/子程序作为参数传递&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#子程序传递多维数组&#34;&gt;子程序传递多维数组&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#关键字参数和可选参数&#34;&gt;关键字参数和可选参数&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#过程接口和接口块&#34;&gt;过程接口和接口块&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#generic&#34;&gt;Generic&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#模块中用于过程的通用接口&#34;&gt;模块中用于过程的通用接口&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#内置模块&#34;&gt;内置模块&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#fortran-指针&#34;&gt;Fortran 指针&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#mixing-programming-with-cc&#34;&gt;Mixing programming with C/C++&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#prerequisite&#34;&gt;Prerequisite&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#数据类型兼容&#34;&gt;数据类型兼容&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#按引用传递参数&#34;&gt;按引用传递参数&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#iso_c_binding-模块&#34;&gt;&lt;code&gt;ISO_C_BINDING&lt;/code&gt; 模块&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#working-with-c&#34;&gt;Working with C++&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#reference&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;模块创建显示接口&#34;&gt;模块创建显示接口&lt;/h2&gt;
&lt;p&gt;在模块中编译和调用程序时, 过程接口的所有细节对编译器都是有效的. 当调用程序时, 编译器可以自动检测过程调用中的参数个数, 类型, 是否参数是数组, 已经每个参数的 INTENT 属性. 在模块内, 编译过程和用USE关联访问过程具有一个显示接口 (explicit interface). Fortran 编译器清楚的知道过程每个参数的所有细节. 不在模块内的过程称为隐式接口 (implicit interface). Fortran 编译器调用程序时, 不知道这些过程的信息.&lt;/p&gt;
&lt;h2 id=&#34;函数子程序作为参数传递&#34;&gt;函数/子程序作为参数传递&lt;/h2&gt;
&lt;p&gt;在调用和被调用函数中，函数被声明为外部量时(external), 用户自定义函数才可以当做调用参数传递．　当参数表中的某个名字被声明为外部变量，相当于告诉编译器在参数表中传递的是独立的已编译的函数.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;函数作为参数传递&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;program main
    implicit none
    interface  ! must specify the interface
        subroutine runfunc(fun, array)
            real*8 :: array(:, :)
            real*8, external :: fun
        end subroutine
    end interface
    real*8 :: array(3,3)
    real*8,external :: fun1
    array = 2d0
    call runfunc(fun1, array)
end program
  
subroutine runfunc(fun, array)
    implicit none
    interface  !must specify the interface
        real*8 function fun(array)
            real*8 :: array(:, :)
        end function
    end interface
    real*8 :: array(:, :)
    print*, fun(array)
end subroutine
  
real*8 function fun1(array)
    implicit none
    real*8 :: array(:, :)
    integer :: i, j
    fun1 = 0d0
    do i = 1, size(array, 1)
        do j = 1, size(array, 2)
            fun1 = fun1 + array(i,j)
        enddo
    enddo
end function
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;子程序作为参数传递&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;program main
    implicit none
    integer :: n = 5
    external :: add, prod
    real*8 :: array(5) = (/1,2,3,4,5/)
    call passsub(add, array, n)
    call passsub(prod, array, n)
end program
  
subroutine passsub(sub, array, n)
    integer :: n
    external :: sub
    real*8 :: array(n)
    call sub(array, n)
end subroutine
  
subroutine add(array, n)
    integer :: n
    real*8 :: array(n)
    integer :: i
    real*8 :: sum = 0
  
    do i = 1, n
        sum = sum+ array(i)
    enddo
    print*, sum
end subroutine
  
subroutine prod(array, n)
    integer :: n
    real*8 :: array(n)
    integer :: i
    real*8 :: cuml = 1
    do i = 1, n
        cuml = cuml * array(i)
    enddo
    print*, cuml
end subroutine
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;子程序传递多维数组&#34;&gt;子程序传递多维数组&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;显示结构的形参数组 (explicit-shape dummy arrays)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;  subroutine process1(data1, data2, n, m)
  integer, intent(in) :: n, m
  real,intent(in),dimension(n,m) :: data1
  real,intent(out), dimension(n,m) :: data2
  data2 = 3 * data1
  end subroutine
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;不定结构形参数组 (assumed-shape dummy arrays)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;数组中的每个下标都用冒号代替, 只有子程序或者函数具有显示接口时, 才能使用这种数组. 通常采用将子程序放入模块中, 然后在程序中使用该模块. 编译器可以从接口信息判断每个数组的大小和结构.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;module test_module
contains
    subroutine process2(data1, data2)
    real, intent(in), dimension(:,:) :: data1
    real, intent(out), dimension(:, :) :: data2
    data2 = 3 * data1
    end subroutine
end module
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;关键字参数和可选参数&#34;&gt;关键字参数和可选参数&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;关键字参数
如果过程接口是显示的, 可以改变参数表中调用参数的顺序, 可以用关键字参数(keyword arguments) 提供更强的灵活性&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;module procs
contains
    real, function calc(first, second, third)
    implicit none
    real, intent(in) :: first, second, third
    calc = (fisrt - second) / third
    end function calc
end module
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在调用时, 即可以用&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;program test_keywords
implicit none
write(*, *) calc(3., 1., 2.)
write(*, *) calc(first=3., second=1., third=2.)
write(*, *) calc(3., third=2., second=1.)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;可选参数&lt;/p&gt;
&lt;p&gt;通过在形参申明中加入 &lt;code&gt;optional&lt;/code&gt; 属性, 指定可选参数
&lt;code&gt;integer, intent(in), optioncal :: upper_limit&lt;/code&gt;
可选参数是否出现, 可以用 &lt;strong&gt;fortran&lt;/strong&gt; 中的 &lt;code&gt;present&lt;/code&gt; 来判断&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;  module process
  contains
      subroutine extremes(a,n, maxval, pos_maxval)
          integer, intent(in) :: n
          real, intent(in), dimension(n) :: a
          real, intent(out), optional :: maxval
          integer, intent(out), optional :: pos_maxval
          integer :: i
          real :: real_max
          integer :: pos_max

          real_max = a(1)
          pos_max = 1
          do i = 2, n
              max: if (a(i) &amp;gt; real_max) then
                  real_max = a(i)
                  pos_max = i
              end if max
          enddo
          if (present(maxval)) then
              maxval = real_max
          endif
          if (present(pos_maxval)) then
              pos_maxval = pos_max
          endif
      end subroutine
  end module
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;过程接口和接口块&#34;&gt;过程接口和接口块&lt;/h2&gt;
&lt;p&gt;想要使用不定结构形参数组或者可选参数这类 Fortran 高级特性, 程序必须要有显示接口. 创建显示接口最简单的方法是将过程放在模块中, 但是将过程放在模块中, 在一些场合不可能. 如过程和函数是用早期的 fortran 语言编写, 或者外部函数库是用 C 或者其他语言编写, 这样将过程放在一个模块中并不可能.&lt;/p&gt;
&lt;p&gt;当不能把过程放在模块中, fortran 允许在调用程序中定义一个接口块, 接口快指定了过程所有的接口特征, 编译器根据接口快中的信息执行一致性检查.  接口的一般形式&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;interface
    interface_body_1
    inteface_body_2
    ...
end interface
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用接口时, 将它放在调用调用单元的最前面&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;program interface_example
implicit none
interface
    subroutine sort(a,n)
    implicit none
    real, dimension(:), intent(inout) :: a
    integer, intent(in) :: n
    end sobroutine sort
end interface

real, dimension(6) :: array = (/1., 5., 3., 2., 6., 4.,/)
integer :: nvals = 6
call sort(n = nvals, a = array)
end program
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;当为大型旧版程序或者函数库创建接口时, 可以将它们放在调用一个模块中, 可以使用 Use 访问&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;module interface_def ! no contains statement
interface
    subroutine sort(array, n)
    ...
    end surtoutine
    subroutine sort2(array, n)
    ...
    end subroutine
end interface
end module
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接口是独立的作用域, 接口块中的形参变量必须单独声明, 即使这些变量已经在相关的作用域中已经被声明过了. Fortran 2003 含有 import 语句, 如果 import 语句出现在接口定义中, 那么import 语句中指明的变量会被导入, 不需要在接口中再次申明.  如果 import 语句中没有带变量, 所有变量都会被导入.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Named entities from the host scoping unit are not accessible in an interface body that is not a module procedure interface body. The IMPORT statement makes those entities accessible in such interface body by host association.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;generic&#34;&gt;Generic&lt;/h2&gt;
&lt;p&gt;使用通用接口块定义过程，过程之间通过不同类型的输入参数区分，可以处理不同类型的数据. 给　&lt;code&gt;interface&lt;/code&gt; 加入一个通用名，那么在接口快定义的每个过程接口可以看作一个通过过程．通用过程中所有过程要么都是子程序，要么都是函数.&lt;/p&gt;
&lt;p&gt;例如对于不同类型的数据进行排序，创建一个通用子程序 &lt;code&gt;sort&lt;/code&gt; 实现排序，可以使用通用接口块&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;interface sort
    subroutine sorti(array, nvals)
    ...
    end subroutine sorti
    subroutine sortr(array, nvals)
    ...
    end subroutine sortr
    ...
    subroutine
end interface
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;模块中用于过程的通用接口&#34;&gt;模块中用于过程的通用接口&lt;/h2&gt;
&lt;p&gt;如果子程序都在一个模块中，并且已经有显示接口，如果再加入过程接口，这样是非法的，不能用上述方式添加通用接口．&lt;code&gt;fortran&lt;/code&gt; 包含了一个可用在通用接口模块中的特殊&lt;code&gt;module procedure&lt;/code&gt; 语句．&lt;/p&gt;
&lt;p&gt;如果４个排序子程序定义在一个模块中，　子程序sort 通用接口是&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;interface sort
    module procedure sorti
    module procedure sortr
    module procedure sortd
    ...
end interface sort
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;例如，找出一个数组的最大值和最大值位置，不管数组的类型&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;module generic_maxval
implicit none
interface maxval
    module procedure maxval_i
    module procedure maxval_r
    ...
end interface
contains
    subroutine maxval_i (array, nvals, value_max, pos_maxval)
        implicit none
        integer, intent(in) :: nvals
        integer, intent(in), dimension(nvals) :: array
        integer, intent(out) :: value_max
        integer, intent(out), optional :: pos_maxval
        integer :: i
        integer :: pos_max
        value_max = array(i)
        pos_max = 1
        do i = 2, nvals
            if (array(i) &amp;gt; value_max) then
                value_max = array(i)
                pos_max = i
            end if
        enddo
        if (present(pos_maxval)) then
            pos_maxval = pos_max
        end if
    end sobroutine maxval_i

    subroutine maxval_r(array, nvals, value_max, pos_maxval)
    ...
    end subroutine

    ...
end module generic_maxval
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;内置模块&#34;&gt;内置模块&lt;/h2&gt;
&lt;p&gt;ortran 2003 定义了内置模块(intrinsic module), 这种模块是由 fortran 编译器创造者预定义和编写的．fortran　2003 中，有三种内置模块&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;ISO_FORTRAN_ENV&lt;/code&gt; 定义了描述特定计算机中存储器特征的向量(标准的整型包含多少bit, 标准字符包含多少bit),以及该机器所定义的I/O 单元&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ISO_C_BINDING&lt;/code&gt; 包含了FORTRAN 编译其和特定处理器的c 语言互操作是必要数据&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IEEE&lt;/code&gt; 处理器运行浮点数的特征&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;fortran-指针&#34;&gt;Fortran 指针&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fortran 申明变量为指针时, 需要使用 &lt;code&gt;Pointer&lt;/code&gt; 属性, 并申明指针的类型&lt;/li&gt;
&lt;li&gt;在 Fortran 变量的类型定义语句中, 加入&lt;code&gt;Target&lt;/code&gt;属性, 将其声明为目标变量&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;Program test_ptr
implicit none
real, pointer::p
real, target :: t1 = 10, t2=-17
p =&amp;gt; t1
print*, p, t1, t2
p =&amp;gt; t2
print*, p, t1, t2
end program
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;指针有三种状态, &lt;strong&gt;undefined&lt;/strong&gt;, &lt;strong&gt;associated&lt;/strong&gt;, &lt;strong&gt;disassociated&lt;/strong&gt;, 通过使用 &lt;code&gt;NuLLIFY&lt;/code&gt; 语句将指针和所有目标变量断开&lt;/li&gt;
&lt;li&gt;指针数组. 指向数组的指针必须声明其指向数组的类型和维数, 不需要申明每一维的宽度&lt;/li&gt;
&lt;/ul&gt;
&lt;script src=&#34;https://gist.github.com/zhanghaomiao/2591d8f6fe65f84d4984131cbd553317.js&#34;&gt;&lt;/script&gt;
&lt;h2 id=&#34;mixing-programming-with-cc&#34;&gt;Mixing programming with C/C++&lt;/h2&gt;
&lt;h3 id=&#34;prerequisite&#34;&gt;Prerequisite&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在C中, 所有子程序都是函数, void 类型函数不返回值&lt;/li&gt;
&lt;li&gt;在 fortran 中, 函数会传递一个返回值, 子程序不传递返回值&lt;/li&gt;
&lt;li&gt;Fortran 调用 C 函数时
&lt;ul&gt;
&lt;li&gt;被调用的C函数返回一个值, 会从Fortran 中将其作为函数来调用&lt;/li&gt;
&lt;li&gt;被调用的C函数不返回值, 将其作为子程序来调用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;C 函数调用 Fortran 子程序时
&lt;ul&gt;
&lt;li&gt;如果被调用Fortran 子程序是一个函数, 会从 C 中将其作为一个返回兼容数据类型的函数来调用&lt;/li&gt;
&lt;li&gt;如果被调用的 Fortran 子程序是一个子例程, 会从 C 中将其作为一个返回 int 或 void 值的函数来调用. 如果Fortran 子例程使用交替返回 (返回值为 return 语句中的表达式), 会返回一个值. 如果 return 语句中没有出现表达式, 在Subroutine 语句中申明了交替返回, 则返回 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;C 区分大小写, Fortran 忽略大小写
&lt;ul&gt;
&lt;li&gt;在 C 子程序中, 使C函数名全部小写&lt;/li&gt;
&lt;li&gt;用 -U 选项编译 Fortran 程序, 会告知编译器保留函数/子程序名称现有大小写&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fortran 在编译时,在子程序名和末尾加入下划线, C 编译时函数名称和用户指定名称一致. (Fortran 对于块过程名有两个前导下划线, 减少与用户指定子例程名的冲突)
&lt;ul&gt;
&lt;li&gt;在 C 函数中, 通过在函数名末尾添加下划线更改名称&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;BIND(C)&lt;/code&gt; 属性声明表明外部函数 是C 语言函数&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;-ext_names&lt;/code&gt; 选项编译对于无下划线外部名称引用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fortran 传递字符型参数时, 会传递一个附加参数, 指定字符串的长度, 这个参数在 C 中为 &lt;code&gt;long int&lt;/code&gt; 量, 按值进行传递&lt;/li&gt;
&lt;li&gt;C 数组从 0 开始, Fortran 数组默认以 1 开始&lt;/li&gt;
&lt;li&gt;C 数组按行主顺序存储,  Fortran 按列主顺序存储&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;数据类型兼容&#34;&gt;数据类型兼容&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;C 数据类型 &lt;code&gt;int, long int, long&lt;/code&gt; 在 32 位环境下是等价的 (4 字节). 在 64 位环境中, &lt;code&gt;long&lt;/code&gt; 和指针为8 字节&lt;/li&gt;
&lt;li&gt;数组和结构的元素及字段需要兼容&lt;/li&gt;
&lt;li&gt;不能按值传递数组, 字符串和结构&lt;/li&gt;
&lt;li&gt;按值传递时, 可以使用 &lt;code&gt;%VAL(arg)&lt;/code&gt;, 或者Fortran95 程序具有显含的接口块, 使用 &lt;code&gt;VALUE&lt;/code&gt; 属性声明了伪参数&lt;/li&gt;
&lt;li&gt;数据大小与对齐, 参考 &lt;a href=&#34;https://docs.oracle.com/cd/E19205-01/819-5262/6n7bvdr18/&#34;&gt;https://docs.oracle.com/cd/E19205-01/819-5262/6n7bvdr18/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;按引用传递参数&#34;&gt;按引用传递参数&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;简单数据类型&lt;/li&gt;
&lt;li&gt;string&lt;/li&gt;
&lt;li&gt;structure&lt;/li&gt;
&lt;li&gt;array&lt;/li&gt;
&lt;li&gt;按值传递参数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;代码 example, 参考 &lt;a href=&#34;https://github.com/zhanghaomiao/C_FORTRAN_MIX&#34;&gt;https://github.com/zhanghaomiao/C_FORTRAN_MIX&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;iso_c_binding-模块&#34;&gt;&lt;code&gt;ISO_C_BINDING&lt;/code&gt; 模块&lt;/h3&gt;
&lt;p&gt;Fortran 2003 提供了内置模块,处理C 和  fortran 数据类型的转化&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;c_associated(c_ptr1[, c_ptr_2])&lt;/code&gt;: determines the status of the C pointer c_ptr_1 or if c_ptr_1 is associated with the target c_ptr_2&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-Fortran&#34;&gt;subroutine association_test(a,b)
use iso_c_binding, only: c_associated, c_loc, c_ptr
implicit none
real, pointer :: a
type(c_ptr) :: b
if(c_associated(b, c_loc(a))) &amp;amp;
   stop &#39;b and a do not point to same target&#39;
end subroutine association_test
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;c_f_pointer(cptr, fptr[,shape])&lt;/code&gt; Assign the target, the C pointer, cptr to Frotran pointer.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c_f_procpointer(cptr, fptr)&lt;/code&gt; assigns the target of the C function pointer &lt;code&gt;cptr&lt;/code&gt; to the Fortran procedure pointer &lt;code&gt;fptr&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c_funloc(x)&lt;/code&gt; determine the C address of the argument, return type is &lt;code&gt;c_funptr&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c_loc(x)&lt;/code&gt; determine the C address of the argument, return type is &lt;code&gt;c_ptr&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c_sizeof(x)&lt;/code&gt; calculate the number of bytes of storage the expression x occupies&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;use iso_c_binding
real(c_float) :: r, s(5)
print *, (c_sizeof(s)/c_sizeof(r) == 5)
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;数据类型的对应关系,参考 
&lt;a href=&#34;http://fortranwiki.org/fortran/show/iso_c_binding&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fortran wiki&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;working-with-c&#34;&gt;Working with C++&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在使用C++程序时, 需要加入 extern &amp;lsquo;C&amp;rsquo; 关键字&lt;/li&gt;
&lt;li&gt;在使用到 C++ 标准库的数据结构时, 链接时需要加入标准C++库 (&lt;code&gt;-lstdc++&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Example
  &lt;script src=&#34;https://gist.github.com/zhanghaomiao/5c12cf5c54f405bf0b4df04c69ceb66c.js&#34;&gt;&lt;/script&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://fortranwiki.org/fortran/show/iso_c_binding&#34;&gt;http://fortranwiki.org/fortran/show/iso_c_binding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gcc.gnu.org/onlinedocs/gfortran/Interoperability-with-C.html#Interoperability-with-C&#34;&gt;https://gcc.gnu.org/onlinedocs/gfortran/Interoperability-with-C.html#Interoperability-with-C&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/22813423/c-f-pointer-does-not-work&#34;&gt;https://stackoverflow.com/questions/22813423/c-f-pointer-does-not-work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.yolinux.com/TUTORIALS/LinuxTutorialMixingFortranAndC.html&#34;&gt;http://www.yolinux.com/TUTORIALS/LinuxTutorialMixingFortranAndC.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/cd/E19205-01/819-5262/6n7bvdr18/&#34;&gt;https://docs.oracle.com/cd/E19205-01/819-5262/6n7bvdr18/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>GDB 调试</title>
      <link>/post/gdb%E8%B0%83%E8%AF%95%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Tue, 12 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/gdb%E8%B0%83%E8%AF%95%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;要求&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有的程序编译时需要使用 &lt;code&gt;-g&lt;/code&gt; 参数， 这样gdb程序可以识别&lt;/li&gt;
&lt;li&gt;编译时不要使用优化，使用优化时一些变量的值无法跟踪， 即优化等级设置为 &lt;code&gt;O0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;命令&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt; 后加行数， 设置断点&lt;/li&gt;
&lt;li&gt;&lt;code&gt;li&lt;/code&gt;(start, [end]) 显示在start 和 end 之间的代码， end 为可选参数， 如果没有 end, 则显示以start 为中心前后5句代码&lt;/li&gt;
&lt;li&gt;&lt;code&gt;r&lt;/code&gt; 重新开始调试程序, 在遇到断点处终止&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt; 开始调试程序，在遇到断点处终止, 与 &lt;code&gt;step into my code&lt;/code&gt; 功能类似&lt;/li&gt;
&lt;li&gt;&lt;code&gt;s&lt;/code&gt; 可后接参数 N ， 表示step n 次， 与 &lt;code&gt;step into&lt;/code&gt; 功能类似&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n&lt;/code&gt; 可后接参数 N ， 表示next n 次， 与 &lt;code&gt;step over&lt;/code&gt; 功能类似&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fin&lt;/code&gt; 从子程序跳出， 与 &lt;code&gt;step out&lt;/code&gt; 功能类似&lt;/li&gt;
&lt;li&gt;&lt;code&gt;p&lt;/code&gt; 打印变量值
&lt;ul&gt;
&lt;li&gt;对于一维数组， 使用 p &lt;code&gt;temp(n)&lt;/code&gt; 会显示temp第&lt;code&gt;n&lt;/code&gt;个元素&lt;/li&gt;
&lt;li&gt;对于二维数组， 使用 p &lt;code&gt;temp(i:j)&lt;/code&gt; 会显示第 &lt;code&gt;i&lt;/code&gt; 和 第 &lt;code&gt;j&lt;/code&gt; 行的元素&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;where&lt;/code&gt; 显示当前程序所处位置&lt;/li&gt;
&lt;li&gt;&lt;code&gt;info&lt;/code&gt; b 显示所设置的断点&lt;/li&gt;
&lt;li&gt;&lt;code&gt;del&lt;/code&gt; breakpoints  删除断点&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt; 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three 
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Landing Page</title>
      <link>/tag/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/tag/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
