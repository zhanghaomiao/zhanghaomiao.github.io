<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Overview | Haomiao</title>
    <link>/courses/deep_learning/</link>
      <atom:link href="/courses/deep_learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Overview</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 01 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=true) shape:circle]</url>
      <title>Overview</title>
      <link>/courses/deep_learning/</link>
    </image>
    
    <item>
      <title>Foundations of Convolutional Neural Networks</title>
      <link>/courses/deep_learning/cnn/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0100</pubDate>
      <guid>/courses/deep_learning/cnn/</guid>
      <description>

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;一般计算机视觉问题包含以下几类:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;图像分类(Image classification)&lt;/li&gt;
&lt;li&gt;目标检测(Object Detection)&lt;/li&gt;
&lt;li&gt;风格转换(Neural Style Transfer)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;面临计算机视觉问题时，如何处理图像数据变得很重要，对于一般图片 $1000 \times 1000 \times 3$, 神经网络的输入层将有 $300,000,000$ 数据, 那么网络权重 $W$ 则很大，这会导致两个后果:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;数据量很少，但是神经网络很复杂，容易出现过拟合&lt;/li&gt;
&lt;li&gt;所需要的内存和计算量很大&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;卷积运算&#34;&gt;卷积运算&lt;/h2&gt;

&lt;p&gt;神经网络从浅层到深层，可以检测出图片的边缘特征，局部特征，最后一层则识别整体的面部轮廓， 这些都是通过卷积神经网络实现的&lt;/p&gt;

&lt;h3 id=&#34;边缘检测&#34;&gt;边缘检测&lt;/h3&gt;

&lt;p&gt;垂直检测(Vertical Edges) and 水平检测(Horizontal Edges), 如下图所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/edge_example.png&#34; width=&#34;350px&#34; /&gt;&lt;/p&gt;

&lt;p&gt;边缘检测可以通过相应的 filter 实现，如下图所示&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/Vertical-Edge-Detection.jpg&#34; width=&#34;350px&#34; /&gt;
原始图片尺寸为 6x6, 通过中间的矩阵之后，尺寸为 4x4. 按照如图所示的方式进行计算，卷积运算的求解是在原始矩阵中选择与 filter 相同大小的区域，一一对应相乘后求和，将所得的结果构成一个矩阵&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;下面是一个示例
&lt;img src=&#34;/img/cnn/Convolutional-operation-example.jpg&#34; width=&#34;350px&#34; /&gt;
将矩阵看做一个图像，最右边中间的亮的区域对应于最左边图像中间的黑白分界线&lt;br /&gt;
下面一张图，表示的更加明显
&lt;img src=&#34;/img/cnn/Convolutional-operation.jpg&#34; width=&#34;350px&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vertical filter and horizontal filter
&lt;img src=&#34;/img/cnn/Vertical-and-Horizontal-Filter.png&#34; width=&#34;350px&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sobel filter and Scharr filter, 增加了中间行的权重， 可以提高结果的稳定性
&lt;img src=&#34;/img/cnn/Sobel-Filter-and-Scharr-Filter.png&#34; width=&#34;350px&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filter 中的值可以设置为参数，通过模型训练得到，神经网络通过反向传播学习到一些低级特性，使得可以对图像的边缘进行检测，不仅仅局限于水平和垂直方向&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;padding&#34;&gt;Padding&lt;/h2&gt;

&lt;p&gt;假设输入图像大小为 $n\times n$, filter 大小为 $f\times f$, 则输出图片大小为 $(n-f+1) \times (n-f+1)$&lt;/p&gt;

&lt;p&gt;因此，存在两个问题&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;卷积运算后，图像的尺寸会减小&lt;/li&gt;
&lt;li&gt;原始图片的角落，边缘地区的输出采样很少，因此很多边缘地区的信息被丢失&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一般可以使用 padding 方式，对图像进行填充, 通常填充的值为0
  &lt;img src=&#34;/img/cnn/Padding.jpg&#34; width=&#34;350px&#34; /&gt;
如果对每个方向的padding 数为 p, 则padding后的图像大小 $(n+2p) \times (n+2p)$, 通过 $f\times f$的filter 之后， 大小则为 $(n+2p-f+1)\times (n+2p-f+1)$&lt;/p&gt;

&lt;p&gt;进行卷积操作时，有两种选择&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;valid 卷积： 没有 padding, 直接卷积， $(n-f+1) \times (n-f+1)$&lt;/li&gt;
&lt;li&gt;same 卷积，进行 padding, 使得卷积后结果大小与输入大小一致，此时有 $p = \frac{f-1}{2}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;f 通常为奇数， 原因包括 same 卷积 中可以得到整数，并且 filter 有一个可以表示其所在位置的中心点&lt;/p&gt;

&lt;h2 id=&#34;stride&#34;&gt;Stride&lt;/h2&gt;

&lt;p&gt;通过设置步长来压缩一些信息， 步长表示 filter 在原始图像的水平方向和垂直方向上每次移动的距离。步长设置为2的卷积过程表示为
  &lt;img src=&#34;/img/cnn/Stride.jpg&#34; width=&#34;350px&#34; /&gt;&lt;/p&gt;

&lt;p&gt;设步长为 $s$, 填充长度为 $p$, 输入图片大小为 $n\times n$, filter 大小为 $f\times f$, 则卷积后的图片大小为 $$\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor \times\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor$$
向下取整表示原始矩阵的蓝框完全包含在图像内部时，才进行计算.&lt;/p&gt;

&lt;p&gt;目前为止我们学习的“卷积”实际上被称为互相关（cross-correlation），而非数学意义上的卷积。真正的卷积操作在做元素乘积求和之前，要将滤波器沿水平和垂直轴翻转（相当于旋转 180 度）&lt;/p&gt;

&lt;h2 id=&#34;高维卷积&#34;&gt;高维卷积&lt;/h2&gt;

&lt;p&gt;如果对三通道的RGB进行卷积运算，对应的 filter 也是三通道的，这有点类似我们的眼镜，是 see through 这三个维度，
对每个通道 (R, G, B) 与对应的 filter 进行卷积运算求和，然后将三个通道的值相加，如下图所示，是对 27 个乘积和作为一个输出&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/Convolutions-on-RGB-image.png&#34; width=&#34;400px&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如果想进行更多的边缘检测，可以增加 filter 的数量，如下图所示&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/More-Filters.jpg&#34; width=&#34;400px&#34; /&gt;&lt;/p&gt;

&lt;p&gt;设输入图片的尺寸 $n \times n \times n_c$ , filter 的尺寸为 $f\times f \times n_c$, 则卷积运算后的图片尺寸为 $(n-f+1) \times (n-f+1) \times n_c&amp;rsquo;$,   $n_c&amp;rsquo;$ 为 filter 的数目&lt;/p&gt;

&lt;h2 id=&#34;单层卷积神经网络&#34;&gt;单层卷积神经网络&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/One-Layer-of-a-Convolutional-Network.jpg&#34; width=&#34;400px&#34; /&gt;
  单层卷积神经网络多了激活函数和偏移量，filter 的数值对应 $W^{[l]}$, 卷积运算对应 $W^{[l]}$ 与 $A^{[l-1]}$的乘积
  $$\begin{array}{c}Z^{[l]}=W^{[l]} A^{[l-1]}+b \\ A^{[l]}=g^{[l]}\left(Z^{[l]}\right)\end{array}$$&lt;/p&gt;

&lt;p&gt;对于 $3 \times 3$的 filter, 总共有 28 各参数，不予图片尺寸相关，因此 CNN 的参数相较于标准神经网络要小很多&lt;/p&gt;

&lt;h3 id=&#34;符合表示&#34;&gt;符合表示&lt;/h3&gt;

&lt;p&gt;设 $l$ 层为卷积层：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$f^{[l]}$：&lt;strong&gt;滤波器的高（或宽）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$p^{[l]}$：&lt;strong&gt;填充长度&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$s^{[l]}$：&lt;strong&gt;步长&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$n^{[l]}_c$：&lt;strong&gt;滤波器组的数量&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;输入维度&lt;/strong&gt;：$n^{[l-1]}_H \times n^{[l-1]}_W \times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_H$表示输入图片的高，$n^{[l-1]}_W$表示输入图片的宽。之前的示例中输入图片的高和宽都相同，但是实际中也可能不同，因此加上下标予以区分。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;输出维度&lt;/strong&gt;：$n^{[l]}_H \times n^{[l]}_W \times n^{[l]}_c$ 。其中&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$n^{[l]}_H = \biggl\lfloor \frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \biggr\rfloor$$&lt;/p&gt;

&lt;p&gt;$$n^{[l]}_W = \biggl\lfloor \frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \biggr\rfloor$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;每个滤波器组的维度&lt;/strong&gt;：$f^{[l]} \times f^{[l]} \times n^{[l-1]}_c$ 。其中$n^{[l-1]}_c$ 为输入图片通道数（也称深度）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;权重维度&lt;/strong&gt;：$f^{[l]} \times f^{[l]} \times n^{[l-1]}_c \times n^{[l]}_c$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;偏置维度&lt;/strong&gt;：$1 \times 1 \times 1 \times n^{[l]}_c$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于深度学习的相关文献并未对卷积标示法达成一致，因此不同的资料关于高度、宽度和通道数的顺序可能不同。有些作者会将通道数放在首位，需要根据标示自行分辨。&lt;/p&gt;

&lt;h2 id=&#34;简单神经网络&#34;&gt;简单神经网络&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/Simple-Convolutional-Network-Example.jpg&#34; width=&#34;400px&#34; /&gt;
  典型的神经网络包含三层， Convolution layer, Pooling layer and Fully Connected Layer.&lt;/p&gt;

&lt;h2 id=&#34;pooling-layer&#34;&gt;Pooling Layer&lt;/h2&gt;

&lt;p&gt;作用是减小模型大小，提高计算速度， 以及减小噪声，提高提取特征的稳健性。  采用最多的一种 Pooling 方式是 Max Pooling, 如下图所示
  &lt;img src=&#34;/img/cnn/Max-Pooling.png&#34; width=&#34;400px&#34; /&gt;
  另外一种是 Average Pooling, 求区域的平均值
  &lt;img src=&#34;/img/cnn/Average-Pooling.png&#34; width=&#34;400px&#34; /&gt;
  Pooling 设有一组超参数，即 filter 大小，步长 s, 选用 MaxPooling 或者 AveragePooling 但并没有参数需要学习，&lt;/p&gt;

&lt;p&gt;设 Pooling 之前输入的维度为 $n_H \times n_W \times n_c$
  则输出为&lt;/p&gt;


  $$\left\lfloor\frac{n_{H}-f}{s}+1\right\rfloor \times\left\lfloor\frac{n_{W}-f}{s}+1\right\rfloor \times n_{c}$$
   

&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/CNN-Example.jpg&#34; width=&#34;700px&#34; /&gt;
  各个层的参数表示为&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Activation shape&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Activation Size&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;#parameters&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Input:&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(32, 32, 3)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3072&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CONV1(f=5, s=1)&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(28, 28, 6)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4704&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;158&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;POOL1&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(14, 14, 6)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1176&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CONV2(f=5, s=1)&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(10, 10, 16)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1600&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;416&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;POOL2&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(5, 5, 16)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;FC3&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(120, 1)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;120&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48120&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;FC4&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(84, 1)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10164&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Softmax&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(10, 1)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;850&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;为什么使用卷积&#34;&gt;为什么使用卷积&lt;/h2&gt;

&lt;p&gt;卷积过程有效的减小了CNN 参数数量，主要通过两种方式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;参数共享（Parameter sharing）&lt;/strong&gt;：特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;稀疏连接（Sparsity of connections）&lt;/strong&gt;：在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值
池化过程则在卷积后很好地聚合了特征，通过降维来减少运算量。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Examples of CNN</title>
      <link>/courses/deep_learning/cnn_example/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0100</pubDate>
      <guid>/courses/deep_learning/cnn_example/</guid>
      <description>

&lt;p&gt;主要包括下面几个&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;LeNet-5&lt;/li&gt;
&lt;li&gt;AlexNet&lt;/li&gt;
&lt;li&gt;VGG&lt;/li&gt;
&lt;li&gt;ResNet&lt;/li&gt;
&lt;li&gt;Inception Neural Network&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;classical-cnn&#34;&gt;Classical CNN&lt;/h2&gt;

&lt;h3 id=&#34;lenet-5&#34;&gt;LeNet-5&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/LeNet-5.png&#34; width=&#34;700px&#34;/&gt;&lt;/p&gt;

&lt;p&gt;特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;LeNet-5 针对灰度图像而训练，因此输入图片的通道数为 1。&lt;/li&gt;
&lt;li&gt;该模型总共包含了约 6 万个参数，远少于标准神经网络所需。&lt;/li&gt;
&lt;li&gt;典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 CONV layer-&amp;gt;POOL layer-&amp;gt;CONV layer-&amp;gt;POOL layer-&amp;gt;FC layer-&amp;gt;FC layer-&amp;gt;OUTPUT layer。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用。&lt;/li&gt;
&lt;li&gt;当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相关论文：&lt;a href=&#34;http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726791&amp;amp;tag=1&#34;&gt;LeCun et.al., 1998. Gradient-based learning applied to document recognition&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;alexnet&#34;&gt;AlexNet&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/AlexNet.png&#34; width=&#34;700px&#34;/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AlexNet 模型与 LeNet-5 模型类似，但是更复杂，包含约 6000 万个参数。另外，AlexNet 模型使用了 ReLU 函数。&lt;/li&gt;
&lt;li&gt;当用于训练图像和数据集时，AlexNet 能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相关论文：&lt;a href=&#34;http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34;&gt;Krizhevsky et al.,2012. ImageNet classification with deep convolutional neural networks&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;vgg&#34;&gt;VGG&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/VGG.png&#34; width=&#34;700px&#34;/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VGG 又称 VGG-16 网络，“16”指网络中包含 16 个卷积层和全连接层。&lt;/li&gt;
&lt;li&gt;超参数较少，只需要专注于构建卷积层。&lt;/li&gt;
&lt;li&gt;结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。&lt;/li&gt;
&lt;li&gt;VGG 需要训练的特征数量巨大，包含多达约 1.38 亿个参数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相关论文：&lt;a href=&#34;https://arxiv.org/pdf/1409.1556.pdf&#34;&gt;Simonvan &amp;amp; Zisserman 2015. Very deep convolutional networks for large-scale image recognition&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;resnet&#34;&gt;ResNet&lt;/h2&gt;

&lt;p&gt;网络越深，容易存在梯度消失问题，使得网络不好训练&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/Residual-block.jpg&#34; width=&#34;800px&#34;/&gt;&lt;/p&gt;

&lt;p&gt;上图的结构被称为&lt;strong&gt;残差块（Residual block）&lt;/strong&gt;。通过&lt;strong&gt;捷径（Short cut，或者称跳远连接，Skip connections）&lt;/strong&gt;可以将 $a^{[l]}$添加到第二个 ReLU 过程中，直接建立 $a^{[l]}$与 $a^{[l+2]}$之间的隔层联系。表达式如下：&lt;/p&gt;

&lt;p&gt;$$z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$$&lt;/p&gt;

&lt;p&gt;$$a^{[l+1]} = g(z^{[l+1]})$$&lt;/p&gt;

&lt;p&gt;$$z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}$$&lt;/p&gt;

&lt;p&gt;$$a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$$&lt;/p&gt;

&lt;p&gt;构建一个残差网络就是将许多残差块堆积在一起，形成一个深度网络。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/Residual-Network.jpg&#34; width=&#34;500px&#34;/&gt;&lt;/p&gt;

&lt;p&gt;为了便于区分，在 ResNets 的论文&lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34;&gt;He et al., 2015. Deep residual networks for image recognition&lt;/a&gt;中，非残差网络被称为&lt;strong&gt;普通网络（Plain Network）&lt;/strong&gt;。将它变为残差网络的方法是加上所有的跳远连接。&lt;/p&gt;

&lt;p&gt;在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，训练错误会先减少，然后开始增多。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/ResNet-Training-Error.jpg&#34; width=&#34;500px&#34;/&gt;&lt;/p&gt;

&lt;h3 id=&#34;残差网络有效的原因&#34;&gt;残差网络有效的原因&lt;/h3&gt;

&lt;p&gt;假设有一个大型神经网络，其输入为 $X$，输出为 $a^{[l]}$。给这个神经网络额外增加两层，输出为 $a^{[l+2]}$。将这两层看作一个具有跳远连接的残差块。为了方便说明，假设整个网络中都选用 ReLU 作为激活函数，因此输出的所有激活值都大于等于 0。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/Why-do-residual-networks-work.jpg&#34; alt=&#34;Why-do-residual-networks-work&#34; /&gt;&lt;/p&gt;

&lt;p&gt;则有：&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{split}
 a^{[l+2]} &amp;amp;= g(z^{[l+2]}+a^{[l]})&lt;br /&gt;
     \\ &amp;amp;= g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})
\end{split}
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;当发生梯度消失时，$W^{[l+2]}\approx0$，$b^{[l+2]}\approx0$，则有：&lt;/p&gt;

&lt;p&gt;$$a^{[l+2]} = g(a^{[l]}) = ReLU(a^{[l]}) = a^{[l]}$$&lt;/p&gt;

&lt;p&gt;因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。&lt;/p&gt;

&lt;p&gt;注意，如果 $a^{[l]}$与 $a^{[l+2]}$的维度不同，需要引入矩阵 $W_s$与 $a^{[l]}$相乘，使得二者的维度相匹配。参数矩阵 $W_s$既可以通过模型训练得到，也可以作为固定值，仅使 $a^{[l]}$截断或者补零。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/ResNet-Paper.png&#34; alt=&#34;ResNet-Paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图是论文提供的 CNN 中 ResNet 的一个典型结构。卷积层通常使用 Same 卷积以保持维度相同，而不同类型层之间的连接（例如卷积层和池化层），如果维度不同，则需要引入矩阵 $W_s$。&lt;/p&gt;

&lt;h2 id=&#34;1x1-卷积&#34;&gt;1x1 卷积&lt;/h2&gt;

&lt;p&gt;1x1 卷积（1x1 convolution，或称为 Network in Network）指滤波器的尺寸为 1。当通道数为 1 时，1x1 卷积意味着卷积操作等同于乘积操作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/1x1-Conv-1.png&#34; alt=&#34;1x1-Conv-1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;而当通道数更多时，1x1 卷积的作用实际上类似全连接层的神经网络结构，从而降低（或升高，取决于滤波器组数）数据的维度。&lt;/p&gt;

&lt;p&gt;池化能压缩数据的高度（$n_H$）及宽度（$n_W$），而 1×1 卷积能压缩数据的通道数（$n_C$）。在如下图所示的例子中，用 32 个大小为 1×1×192 的滤波器进行卷积，就能使原先数据包含的 192 个通道压缩为 32 个。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/1x1-Conv-2.png&#34; alt=&#34;1x1-Conv-2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;虽然论文&lt;a href=&#34;https://arxiv.org/pdf/1312.4400.pdf&#34;&gt;Lin et al., 2013. Network in network&lt;/a&gt;中关于架构的详细内容并没有得到广泛应用，但是 1x1 卷积的理念十分有影响力，许多神经网络架构（包括 Inception 网络）都受到它的影响。&lt;/p&gt;

&lt;h2 id=&#34;inception-网络&#34;&gt;Inception 网络&lt;/h2&gt;

&lt;p&gt;在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 &lt;strong&gt;Inception 网络的作用&lt;/strong&gt;即是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/Motivation-for-inception-network.jpg&#34; alt=&#34;Motivation-for-inception-network&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如图，Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。&lt;/p&gt;

&lt;p&gt;相关论文：&lt;a href=&#34;https://arxiv.org/pdf/1409.4842.pdf&#34;&gt;Szegedy et al., 2014, Going Deeper with Convolutions&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;计算成本问题&#34;&gt;计算成本问题&lt;/h3&gt;

&lt;p&gt;在提升性能的同时，Inception 网络有着较大的计算成本。下图是一个例子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/The-problem-of-computational-cost.png&#34; alt=&#34;The-problem-of-computational-cost&#34; /&gt;&lt;/p&gt;

&lt;p&gt;图中有 32 个滤波器，每个滤波器的大小为 5x5x192。输出大小为 28x28x32，所以需要计算 28x28x32 个数字，对于每个数，都要执行 5x5x192 次乘法运算。加法运算次数与乘法运算次数近似相等。因此，可以看作这一层的计算量为 28x28x32x5x5x192 = 1.2亿。&lt;/p&gt;

&lt;p&gt;为了解决计算量大的问题，可以引入 1x1 卷积来减少其计算量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/Using-1x1-convolution.png&#34; alt=&#34;Using-1x1-convolution&#34; /&gt;&lt;/p&gt;

&lt;p&gt;对于同一个例子，我们使用 1x1 卷积把输入数据从 192 个通道减少到 16 个通道，然后对这个较小层运行 5x5 卷积，得到最终输出。这个 1x1 的卷积层通常被称作&lt;strong&gt;瓶颈层（Bottleneck layer）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;改进后的计算量为 28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。&lt;/p&gt;

&lt;p&gt;只要合理构建瓶颈层，就可以既显著缩小计算规模，又不会降低网络性能。&lt;/p&gt;

&lt;h3 id=&#34;完整的-inception-网络&#34;&gt;完整的 Inception 网络&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/Inception-module.jpg&#34; alt=&#34;Inception-module&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图是引入 1x1 卷积后的 Inception 模块。值得注意的是，为了将所有的输出组合起来，红色的池化层使用 Same 类型的填充（padding）来池化使得输出的宽高不变，通道数也不变。&lt;/p&gt;

&lt;p&gt;多个 Inception 模块组成一个完整的 Inception 网络（被称为 GoogLeNet，以向 LeNet 致敬），如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cnn/Inception-network.jpg&#34; alt=&#34;Inception-network&#34; /&gt;&lt;/p&gt;

&lt;p&gt;注意黑色椭圆圈出的隐藏层，这些分支都是 Softmax 的输出层，可以用来参与特征的计算及结果预测，起到调整并防止发生过拟合的效果。&lt;/p&gt;

&lt;p&gt;经过研究者们的不断发展，Inception 模型的 V2、V3、V4 以及引入残差网络的版本被提出，这些变体都基于 Inception V1 版本的基础思想上。顺便一提，Inception 模型的名字来自电影《盗梦空间》。&lt;/p&gt;

&lt;h2 id=&#34;使用开源的实现方案&#34;&gt;使用开源的实现方案&lt;/h2&gt;

&lt;p&gt;很多神经网络复杂细致，并充斥着参数调节的细节问题，因而很难仅通过阅读论文来重现他人的成果。想要搭建一个同样的神经网络，查看开源的实现方案会快很多。&lt;/p&gt;

&lt;h2 id=&#34;迁移学习&#34;&gt;迁移学习&lt;/h2&gt;

&lt;p&gt;在“搭建机器学习项目”课程中，&lt;a href=&#34;http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Structuring_Machine_Learning_Projects/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5%EF%BC%882%EF%BC%89?id=%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0&#34;&gt;迁移学习&lt;/a&gt;已经被提到过。计算机视觉是一个经常用到迁移学习的领域。在搭建计算机视觉的应用时，相比于从头训练权重，下载别人已经训练好的网络结构的权重，用其做&lt;strong&gt;预训练&lt;/strong&gt;，然后转换到自己感兴趣的任务上，有助于加速开发。&lt;/p&gt;

&lt;p&gt;对于已训练好的卷积神经网络，可以将所有层都看作是&lt;strong&gt;冻结的&lt;/strong&gt;，只需要训练与你的 Softmax 层有关的参数即可。大多数深度学习框架都允许用户指定是否训练特定层的权重。&lt;/p&gt;

&lt;p&gt;而冻结的层由于不需要改变和训练，可以看作一个固定函数。可以将这个固定函数存入硬盘，以便后续使用，而不必每次再使用训练集进行训练了。&lt;/p&gt;

&lt;p&gt;上述的做法适用于你只有一个较小的数据集。如果你有一个更大的数据集，应该冻结更少的层，然后训练后面的层。越多的数据意味着冻结越少的层，训练更多的层。如果有一个极大的数据集，你可以将开源的网络和它的权重整个当作初始化（代替随机初始化），然后训练整个网络。&lt;/p&gt;

&lt;h2 id=&#34;数据扩增&#34;&gt;数据扩增&lt;/h2&gt;

&lt;p&gt;计算机视觉领域的应用都需要大量的数据。当数据不够时，&lt;strong&gt;数据扩增（Data Augmentation）&lt;/strong&gt;就有帮助。常用的数据扩增包括镜像翻转、随机裁剪、色彩转换。&lt;/p&gt;

&lt;p&gt;其中，色彩转换是对图片的 RGB 通道数值进行随意增加或者减少，改变图片色调。另外，&lt;strong&gt;PCA 颜色增强&lt;/strong&gt;指更有针对性地对图片的 RGB 通道进行主成分分析（Principles Components Analysis，PCA），对主要的通道颜色进行增加或减少，可以采用高斯扰动做法来增加有效的样本数量。具体的 PCA 颜色增强做法可以查阅 AlexNet 的相关论文或者开源代码。&lt;/p&gt;

&lt;p&gt;在构建大型神经网络的时候，数据扩增和模型训练可以由两个或多个不同的线程并行来实现。&lt;/p&gt;

&lt;h2 id=&#34;计算机视觉现状&#34;&gt;计算机视觉现状&lt;/h2&gt;

&lt;p&gt;通常，学习算法有两种知识来源：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;被标记的数据&lt;/li&gt;
&lt;li&gt;手工工程&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;手工工程（Hand-engineering，又称 hacks）&lt;/strong&gt;指精心设计的特性、网络体系结构或是系统的其他组件。手工工程是一项非常重要也比较困难的工作。在数据量不多的情况下，手工工程是获得良好表现的最佳方式。正因为数据量不能满足需要，历史上计算机视觉领域更多地依赖于手工工程。近几年数据量急剧增加，因此手工工程量大幅减少。&lt;/p&gt;

&lt;p&gt;另外，在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;集成（Ensembling）：独立地训练几个神经网络，并平均输出它们的输出&lt;/li&gt;
&lt;li&gt;Multi-crop at test time：将数据扩增应用到测试集，对结果进行平均&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是由于这些方法计算和内存成本较大，一般不适用于构建实际的生产项目。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>目标检测</title>
      <link>/courses/deep_learning/object_detect/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0100</pubDate>
      <guid>/courses/deep_learning/object_detect/</guid>
      <description>

&lt;p&gt;目标检测是计算机视觉领域中一个新兴的应用方向，其任务是对输入图像进行分类的同时，检测图像中是否包含某些目标，并对他们准确定位并标识。&lt;/p&gt;

&lt;h2 id=&#34;目标定位&#34;&gt;目标定位&lt;/h2&gt;

&lt;p&gt;定位分类问题不仅要求判断出图片中物体的种类，还要在图片中标记出它的具体位置，用&lt;strong&gt;边框（Bounding Box，或者称包围盒）&lt;/strong&gt;把物体圈起来。一般来说，定位分类问题通常只有一个较大的对象位于图片中间位置；而在目标检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。&lt;/p&gt;

&lt;p&gt;为了定位图片中汽车的位置，可以让神经网络多输出 4 个数字，标记为 $b_x$、$b_y$、$b_h$、$b_w$。将图片左上角标记为 (0, 0)，右下角标记为 (1, 1)，则有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;红色方框的中心点：($b_x$，$b_y$)&lt;/li&gt;
&lt;li&gt;边界框的高度：$b_h$&lt;/li&gt;
&lt;li&gt;边界框的宽度：$b_w$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此，训练集不仅包含对象分类标签，还包含表示边界框的四个数字。定义目标标签 Y 如下：&lt;/p&gt;

&lt;p&gt;$$\left[\begin{matrix}P_c\\ b_x\\ b_y\\ b_h\\ b_w\\ c_1\\ c_2\\ c_3\end{matrix}\right]$$&lt;/p&gt;

&lt;p&gt;则有：&lt;/p&gt;

&lt;p&gt;$$P_c=1, Y = \left[\begin{matrix}1\\ b_x\\ b_y\\ b_h\\ b_w\\ c_1\\ c_2\\ c_3\end{matrix}\right]
$$&lt;/p&gt;

&lt;p&gt;其中，$c_n$表示存在第 $n$个种类的概率；如果 $P_c=0$，表示没有检测到目标，则输出标签后面的 7 个参数都是无效的，可以忽略（用 ? 来表示）。&lt;/p&gt;

&lt;p&gt;$$P_c=0, Y = \left[\begin{matrix}0\\ ?\\ ?\\ ?\\ ?\\ ?\\ ?\\ ?\end{matrix}\right]$$&lt;/p&gt;

&lt;p&gt;损失函数可以表示为 $L(\hat y, y)$，如果使用平方误差形式，对于不同的 $P_c$有不同的损失函数（注意下标 $i$指标签的第 $i$个值）：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;$P_c=1$，即$y_1=1$：&lt;/p&gt;

&lt;p&gt;$L(\hat y,y)=(\hat y_1-y_1)^2+(\hat y_2-y_2)^2+\cdots+(\hat y_8-y_8)^2$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$P_c=0$，即$y_1=0$：&lt;/p&gt;

&lt;p&gt;$L(\hat y,y)=(\hat y_1-y_1)^2$&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;除了使用平方误差，也可以使用逻辑回归损失函数，类标签 $c_1,c_2,c_3$ 也可以通过 softmax 输出。相比较而言，平方误差已经能够取得比较好的效果。&lt;/p&gt;

&lt;h2 id=&#34;特征点检测&#34;&gt;特征点检测&lt;/h2&gt;

&lt;p&gt;神经网络可以像标识目标的中心点位置那样，通过输出图片上的特征点，来实现对目标特征的识别。在标签中，这些特征点以多个二维坐标的形式表示。&lt;/p&gt;

&lt;p&gt;通过检测人脸特征点可以进行情绪分类与判断，或者应用于 AR 领域等等。也可以透过检测姿态特征点来进行人体姿态检测。&lt;/p&gt;

&lt;h2 id=&#34;目标检测&#34;&gt;目标检测&lt;/h2&gt;

&lt;p&gt;想要实现目标检测，可以采用&lt;strong&gt;基于滑动窗口的目标检测（Sliding Windows Detection）&lt;/strong&gt;算法。该算法的步骤如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;训练集上搜集相应的各种目标图片和非目标图片，样本图片要求尺寸较小，相应目标居于图片中心位置并基本占据整张图片。&lt;/li&gt;
&lt;li&gt;使用训练集构建 CNN 模型，使得模型有较高的识别率。&lt;/li&gt;
&lt;li&gt;选择大小适宜的窗口与合适的固定步幅，对测试图片进行从左到右、从上倒下的滑动遍历。每个窗口区域使用已经训练好的 CNN 模型进行识别判断。&lt;/li&gt;
&lt;li&gt;可以选择更大的窗口，然后重复第三步的操作。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Sliding-windows-detection.png&#34; alt=&#34;Sliding-windows-detection&#34; /&gt;&lt;/p&gt;

&lt;p&gt;滑动窗口目标检测的&lt;strong&gt;优点&lt;/strong&gt;是原理简单，且不需要人为选定目标区域；&lt;strong&gt;缺点&lt;/strong&gt;是需要人为直观设定滑动窗口的大小和步幅。滑动窗口过小或过大，步幅过大均会降低目标检测的正确率。另外，每次滑动都要进行一次 CNN 网络计算，如果滑动窗口和步幅较小，计算成本往往很大。&lt;/p&gt;

&lt;p&gt;所以，滑动窗口目标检测算法虽然简单，但是性能不佳，效率较低。&lt;/p&gt;

&lt;h2 id=&#34;基于卷积的滑动窗口实现&#34;&gt;基于卷积的滑动窗口实现&lt;/h2&gt;

&lt;p&gt;相比从较大图片多次截取，在卷积层上应用滑动窗口目标检测算法可以提高运行速度。所要做的仅是将全连接层换成卷积层，即使用与上一层尺寸一致的滤波器进行卷积运算。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Convolution-implementation-of-sliding-windows.png&#34; alt=&#34;Convolution-implementation-of-sliding-windows&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如图，对于 16x16x3 的图片，步长为 2，CNN 网络得到的输出层为 2x2x4。其中，2x2 表示共有 4 个窗口结果。对于更复杂的 28x28x3 的图片，得到的输出层为 8x8x4，共 64 个窗口结果。最大池化层的宽高和步长相等。&lt;/p&gt;

&lt;p&gt;运行速度提高的原理：在滑动窗口的过程中，需要重复进行 CNN 正向计算。因此，不需要将输入图片分割成多个子集，分别执行向前传播，而是将它们作为一张图片输入给卷积网络进行一次 CNN 正向计算。这样，公共区域的计算可以共享，以降低运算成本。&lt;/p&gt;

&lt;p&gt;相关论文：&lt;a href=&#34;https://arxiv.org/pdf/1312.6229.pdf&#34;&gt;Sermanet et al., 2014. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;边框预测&#34;&gt;边框预测&lt;/h2&gt;

&lt;p&gt;在上述算法中，边框的位置可能无法完美覆盖目标，或者大小不合适，或者最准确的边框并非正方形，而是长方形。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;YOLO（You Only Look Once）算法&lt;/strong&gt;可以用于得到更精确的边框。YOLO 算法将原始图片划分为 n×n 网格，并将&lt;a href=&#34;http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Convolutional_Neural_Networks/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B?id=%e7%9b%ae%e6%a0%87%e5%ae%9a%e4%bd%8d&#34;&gt;目标定位&lt;/a&gt;一节中提到的图像分类和目标定位算法，逐一应用在每个网格中，每个网格都有标签如：&lt;/p&gt;

&lt;p&gt;$$\left[\begin{matrix}P_c\\ b_x\\ b_y\\ b_h\\ b_w\\ c_1\\ c_2\\ c_3\end{matrix}\right]$$&lt;/p&gt;

&lt;p&gt;若某个目标的中点落在某个网格，则该网格负责检测该对象。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Bounding-Box-Predictions.png&#34; alt=&#34;Bounding-Box-Predictions&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如上面的示例中，如果将输入的图片划分为 3×3 的网格、需要检测的目标有 3 类，则每一网格部分图片的标签会是一个 8 维的列矩阵，最终输出的就是大小为 3×3×8 的结果。要得到这个结果，就要训练一个输入大小为 100×100×3，输出大小为 3×3×8 的 CNN。在实践中，可能使用更为精细的 19×19 网格，则两个目标的中点在同一个网格的概率更小。&lt;/p&gt;

&lt;p&gt;YOLO 算法的优点：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;和图像分类和目标定位算法类似，显式输出边框坐标和大小，不会受到滑窗分类器的步长大小限制。&lt;/li&gt;
&lt;li&gt;仍然只进行一次 CNN 正向计算，效率很高，甚至可以达到实时识别。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如何编码边框 $b_x$、$b_y$、$b_h$、$b_w$？YOLO 算法设 $b_x$、$b_y$、$b_h$、$b_w$ 的值是相对于网格长的比例。则 $b_x$、$b_y$ 在 0 到 1 之间，而 $b_h$、$b_w$ 可以大于 1。当然，也有其他参数化的形式，且效果可能更好。这里只是给出一个通用的表示方法。&lt;/p&gt;

&lt;p&gt;相关论文：&lt;a href=&#34;https://arxiv.org/pdf/1506.02640.pdf&#34;&gt;Redmon et al., 2015. You Only Look Once: Unified, Real-Time Object Detection&lt;/a&gt;。Ng 认为该论文较难理解。&lt;/p&gt;

&lt;h2 id=&#34;交互比&#34;&gt;交互比&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;交互比（IoU, Intersection Over Union）&lt;/strong&gt;函数用于评价对象检测算法，它计算预测边框和实际边框交集（I）与并集（U）之比：&lt;/p&gt;

&lt;p&gt;$$IoU = \frac{I}{U}$$&lt;/p&gt;

&lt;p&gt;IoU 的值在 0～1 之间，且越接近 1 表示目标的定位越准确。IoU 大于等于 0.5 时，一般可以认为预测边框是正确的，当然也可以更加严格地要求一个更高的阈值。&lt;/p&gt;

&lt;h2 id=&#34;非极大值抑制&#34;&gt;非极大值抑制&lt;/h2&gt;

&lt;p&gt;YOLO 算法中，可能有很多网格检测到同一目标。&lt;strong&gt;非极大值抑制（Non-max Suppression）&lt;/strong&gt;会通过清理检测结果，找到每个目标中点所位于的网格，确保算法对每个目标只检测一次。&lt;/p&gt;

&lt;p&gt;进行非极大值抑制的步骤如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;将包含目标中心坐标的可信度 $P_c$ 小于阈值（例如 0.6）的网格丢弃；&lt;/li&gt;
&lt;li&gt;选取拥有最大 $P_c$ 的网格；&lt;/li&gt;
&lt;li&gt;分别计算该网格和其他所有网格的 IoU，将 IoU 超过预设阈值的网格丢弃；&lt;/li&gt;
&lt;li&gt;重复第 2~3 步，直到不存在未处理的网格。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上述步骤适用于单类别目标检测。进行多个类别目标检测时，对于每个类别，应该单独做一次非极大值抑制。&lt;/p&gt;

&lt;h2 id=&#34;anchor-boxes&#34;&gt;Anchor Boxes&lt;/h2&gt;

&lt;p&gt;到目前为止，我们讨论的情况都是一个网格只检测一个对象。如果要将算法运用在多目标检测上，需要用到 Anchor Boxes。一个网格的标签中将包含多个 Anchor Box，相当于存在多个用以标识不同目标的边框。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Overlapping-objects.png&#34; alt=&#34;Overlapping-objects&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在上图示例中，我们希望同时检测人和汽车。因此，每个网格的的标签中含有两个 Anchor Box。输出的标签结果大小从 3×3×8 变为 3×3×16。若两个 $P_c$ 都大于预设阈值，则说明检测到了两个目标。&lt;/p&gt;

&lt;p&gt;在单目标检测中，图像中的目标被分配给了包含该目标中点的那个网格；引入 Anchor Box 进行多目标检测时，图像中的目标则被分配到了包含该目标中点的那个网格以及具有最高 IoU 值的该网格的 Anchor Box。&lt;/p&gt;

&lt;p&gt;Anchor Boxes 也有局限性，对于同一网格有三个及以上目标，或者两个目标的 Anchor Box 高度重合的情况处理不好。&lt;/p&gt;

&lt;p&gt;Anchor Box 的形状一般通过人工选取。高级一点的方法是用 k-means 将两类对象形状聚类，选择最具代表性的 Anchor Box。&lt;/p&gt;

&lt;p&gt;如果对以上内容不是很理解，在“3.9 YOLO 算法”一节中视频的第 5 分钟，有一个更为直观的示例。&lt;/p&gt;

&lt;h2 id=&#34;r-cnn&#34;&gt;R-CNN&lt;/h2&gt;

&lt;p&gt;前面介绍的滑动窗口目标检测算法对一些明显没有目标的区域也进行了扫描，这降低了算法的运行效率。为了解决这个问题，&lt;strong&gt;R-CNN（Region CNN，带区域的 CNN）&lt;/strong&gt;被提出。通过对输入图片运行&lt;strong&gt;图像分割算法&lt;/strong&gt;，在不同的色块上找出&lt;strong&gt;候选区域（Region Proposal）&lt;/strong&gt;，就只需要在这些区域上运行分类器。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/R-CNN.png&#34; alt=&#34;R-CNN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;R-CNN 的缺点是运行速度很慢，所以有一系列后续研究工作改进。例如 Fast R-CNN（与基于卷积的滑动窗口实现相似，但得到候选区域的聚类步骤依然很慢）、Faster R-CNN（使用卷积对图片进行分割）。不过大多数时候还是比 YOLO 算法慢。&lt;/p&gt;

&lt;p&gt;相关论文：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;R-CNN：&lt;a href=&#34;https://arxiv.org/pdf/1311.2524.pdf&#34;&gt;Girshik et al., 2013. Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fast R-CNN：&lt;a href=&#34;https://arxiv.org/pdf/1504.08083.pdf&#34;&gt;Girshik, 2015. Fast R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Faster R-CNN：&lt;a href=&#34;https://arxiv.org/pdf/1506.01497v3.pdf&#34;&gt;Ren et al., 2016. Faster R-CNN: Towards real-time object detection with region proposal networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>人脸识别和风格迁移</title>
      <link>/courses/deep_learning/transfer/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0100</pubDate>
      <guid>/courses/deep_learning/transfer/</guid>
      <description>

&lt;h2 id=&#34;人脸识别&#34;&gt;人脸识别&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;人脸验证（Face Verification）&lt;/strong&gt;和&lt;strong&gt;人脸识别（Face Recognition）&lt;/strong&gt;的区别：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;人脸验证：一般指一个一对一问题，只需要验证输入的人脸图像是否与某个已知的身份信息对应；&lt;/li&gt;
&lt;li&gt;人脸识别：一个更为复杂的一对多问题，需要验证输入的人脸图像是否与多个已知身份信息中的某一个匹配。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一般来说，由于需要匹配的身份信息更多导致错误率增加，人脸识别比人脸验证更难一些。&lt;/p&gt;

&lt;h3 id=&#34;one-shot-学习&#34;&gt;One-Shot 学习&lt;/h3&gt;

&lt;p&gt;人脸识别所面临的一个挑战是要求系统只采集某人的一个面部样本，就能快速准确地识别出这个人，即只用一个训练样本来获得准确的预测结果。这被称为&lt;strong&gt;One-Shot 学习&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;有一种方法是假设数据库中存有 N 个人的身份信息，对于每张输入图像，用 Softmax 输出 N+1 种标签，分别对应每个人以及都不是。然而这种方法的实际效果很差，因为过小的训练集不足以训练出一个稳健的神经网络；并且如果有新的身份信息入库，需要重新训练神经网络，不够灵活。&lt;/p&gt;

&lt;p&gt;因此，我们通过学习一个 Similarity 函数来实现 One-Shot 学习过程。Similarity 函数定义了输入的两幅图像的差异度，其公式如下：&lt;/p&gt;

&lt;p&gt;$$Similarity  = d(img1, img2)$$&lt;/p&gt;

&lt;p&gt;可以设置一个超参数 $τ$ 作为阈值，作为判断两幅图片是否为同一个人的依据。&lt;/p&gt;

&lt;h3 id=&#34;siamese-网络&#34;&gt;Siamese 网络&lt;/h3&gt;

&lt;p&gt;实现 Similarity 函数的一种方式是使用&lt;strong&gt;Siamese 网络&lt;/strong&gt;，它是一种对两个不同输入运行相同的卷积网络，然后对它们的结果进行比较的神经网络。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Siamese.png&#34; alt=&#34;Siamese&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如上图示例，将图片 $x^{(1)}$、$x^{(2)}$ 分别输入两个相同的卷积网络中，经过全连接层后不再进行 Softmax，而是得到特征向量 $f(x^{(1)})$、$f(x^{(2)})$。这时，Similarity 函数就被定义为两个特征向量之差的 L2 范数：&lt;/p&gt;

&lt;p&gt;$$d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||^2_2$$&lt;/p&gt;

&lt;p&gt;相关论文：&lt;a href=&#34;http://www.cs.wayne.edu/~mdong/taigman_cvpr14.pdf&#34;&gt;Taigman et al., 2014, DeepFace closing the gap to human level performance&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;triplet-损失&#34;&gt;Triplet 损失&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Triplet 损失函数&lt;/strong&gt;用于训练出合适的参数，以获得高质量的人脸图像编码。“Triplet”一词来源于训练这个神经网络需要大量包含 Anchor（靶目标）、Positive（正例）、Negative（反例）的图片组，其中 Anchor 和 Positive 需要是同一个人的人脸图像。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Training-set-using-triplet-loss.png&#34; alt=&#34;Training-set-using-triplet-loss&#34; /&gt;&lt;/p&gt;

&lt;p&gt;对于这三张图片，应该有：&lt;/p&gt;

&lt;p&gt;$$||f(A) - f(P)||^2_2 + \alpha \le ||f(A) - f(N)||^2_2$$&lt;/p&gt;

&lt;p&gt;其中，$\alpha$ 被称为&lt;strong&gt;间隔（margin）&lt;/strong&gt;，用于确保 $f()$ 不会总是输出零向量（或者一个恒定的值）。&lt;/p&gt;

&lt;p&gt;Triplet 损失函数的定义：&lt;/p&gt;

&lt;p&gt;$$L(A, P, N) = max(||f(A) - f(P)||^2_2 - ||f(A) - f(N)||^2_2 + \alpha, 0)$$&lt;/p&gt;

&lt;p&gt;其中，因为 $||f(A) - f(P)||^2_2 - ||f(A) - f(N)||^2_2 + \alpha$ 的值需要小于等于 0，因此取它和 0 的更大值。&lt;/p&gt;

&lt;p&gt;对于大小为 $m$ 的训练集，代价函数为：&lt;/p&gt;

&lt;p&gt;$$J = \sum^m_{i=1}L(A^{(i)}, P^{(i)}, N^{(i)})$$&lt;/p&gt;

&lt;p&gt;通过梯度下降最小化代价函数。&lt;/p&gt;

&lt;p&gt;在选择训练样本时，随机选择容易使 Anchor 和 Positive 极为接近，而 Anchor 和 Negative 相差较大，以致训练出来的模型容易抓不到关键的区别。因此，最好的做法是人为增加 Anchor 和 Positive 的区别，缩小 Anchor 和 Negative 的区别，促使模型去学习不同人脸之间的关键差异。&lt;/p&gt;

&lt;p&gt;相关论文：&lt;a href=&#34;https://arxiv.org/pdf/1503.03832.pdf&#34;&gt;Schroff et al., 2015,  FaceNet: A unified embedding for face recognition and clustering&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;二分类结构&#34;&gt;二分类结构&lt;/h3&gt;

&lt;p&gt;除了 Triplet 损失函数，二分类结构也可用于学习参数以解决人脸识别问题。其做法是输入一对图片，将两个 Siamese 网络产生的特征向量输入至同一个 Sigmoid 单元，输出 1 则表示是识别为同一人，输出 0 则表示识别为不同的人。&lt;/p&gt;

&lt;p&gt;Sigmoid 单元对应的表达式为：&lt;/p&gt;

&lt;p&gt;$$\hat y = \sigma (\sum^K_{k=1}w_k|f(x^{(i)})_{k} - x^{(j)})_{k}| + b)$$&lt;/p&gt;

&lt;p&gt;其中，$w_k$ 和 $b$ 都是通过梯度下降算法迭代训练得到的参数。上述计算表达式也可以用另一种表达式代替：&lt;/p&gt;

&lt;p&gt;$$\hat y = \sigma (\sum^K_{k=1}w_k
\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k} + b)$$&lt;/p&gt;

&lt;p&gt;其中，$\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k}$ 被称为 $\chi$ 方相似度。&lt;/p&gt;

&lt;p&gt;无论是对于使用 Triplet 损失函数的网络，还是二分类结构，为了减少计算量，可以提前计算好编码输出 $f(x)$ 并保存。这样就不必存储原始图片，并且每次进行人脸识别时只需要计算测试图片的编码输出。&lt;/p&gt;

&lt;h2 id=&#34;神经风格迁移&#34;&gt;神经风格迁移&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;神经风格迁移（Neural style transfer）&lt;/strong&gt;将参考风格图像的风格“迁移”到另外一张内容图像中，生成具有其特色的图像。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Neural-style-transfer.png&#34; alt=&#34;Neural-style-transfer&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;深度卷积网络在学什么&#34;&gt;深度卷积网络在学什么？&lt;/h3&gt;

&lt;p&gt;想要理解如何实现神经风格转换，首先要理解在输入图像数据后，一个深度卷积网络从中都学到了些什么。我们借助可视化来做到这一点。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Visualizing-deep-layers.png&#34; alt=&#34;Visualizing-deep-layers&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们通过遍历所有的训练样本，找出使该层激活函数输出最大的 9 块图像区域。可以看出，浅层的隐藏层通常检测出的是原始图像的边缘、颜色、阴影等简单信息。随着层数的增加，隐藏单元能捕捉的区域更大，学习到的特征也由从边缘到纹理再到具体物体，变得更加复杂。&lt;/p&gt;

&lt;p&gt;相关论文：&lt;a href=&#34;https://arxiv.org/pdf/1311.2901.pdf&#34;&gt;Zeiler and Fergus., 2013, Visualizing and understanding convolutional networks&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;代价函数&#34;&gt;代价函数&lt;/h3&gt;

&lt;p&gt;神经风格迁移生成图片 G 的代价函数如下：&lt;/p&gt;

&lt;p&gt;$$J(G) = \alpha \cdot J_{content}(C, G) + \beta \cdot J_{style}(S, G)$$&lt;/p&gt;

&lt;p&gt;其中，$\alpha$、$\beta$ 是用于控制相似度比重的超参数。&lt;/p&gt;

&lt;p&gt;神经风格迁移的算法步骤如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;随机生成图片 G 的所有像素点；&lt;/li&gt;
&lt;li&gt;使用梯度下降算法使代价函数最小化，以不断修正 G 的所有像素点。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;相关论文：&lt;a href=&#34;https://arxiv.org/pdf/1508.06576v2.pdf&#34;&gt;Gatys al., 2015. A neural algorithm of artistic style&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;内容代价函数&#34;&gt;内容代价函数&lt;/h4&gt;

&lt;p&gt;上述代价函数包含一个内容代价部分和风格代价部分。我们先来讨论内容代价函数 $J_{content}(C, G)$，它表示内容图片 C 和生成图片 G 之间的相似度。&lt;/p&gt;

&lt;p&gt;$J_{content}(C, G)$ 的计算过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用一个预训练好的 CNN（例如 VGG）；&lt;/li&gt;
&lt;li&gt;选择一个隐藏层 $l$ 来计算内容代价。$l$ 太小则内容图片和生成图片像素级别相似，$l$ 太大则可能只有具体物体级别的相似。因此，$l$ 一般选一个中间层；&lt;/li&gt;
&lt;li&gt;设 $a^{&amp;copy;[l]}$、$a^{(G)[l]}$ 为 C 和 G 在 $l$ 层的激活，则有：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$J_{content}(C, G) = \frac{1}{2}||(a^{&amp;copy;[l]} - a^{(G)[l]})||^2$$&lt;/p&gt;

&lt;p&gt;$a^{&amp;copy;[l]}$ 和 $a^{(G)[l]}$ 越相似，则 $J_{content}(C, G)$ 越小。&lt;/p&gt;

&lt;h4 id=&#34;风格代价函数&#34;&gt;风格代价函数&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Intuition-about-style-of-an-image.png&#34; alt=&#34;Intuition-about-style-of-an-image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每个通道提取图片的特征不同，比如标为红色的通道提取的是图片的垂直纹理特征，标为黄色的通道提取的是图片的橙色背景特征。那么计算这两个通道的相关性，相关性的大小，即表示原始图片既包含了垂直纹理也包含了该橙色背景的可能性大小。通过 CNN，“风格”被定义为同一个隐藏层不同通道之间激活值的相关系数，因其反映了原始图片特征间的相互关系。&lt;/p&gt;

&lt;p&gt;对于风格图像 S，选定网络中的第 $l$ 层，则相关系数以一个 gram 矩阵的形式表示：&lt;/p&gt;

&lt;p&gt;$$G^{[l](S)}_{kk&amp;rsquo;} = \sum^{n^{[l]}_H}_{i=1} \sum^{n^{[l]}_W}_{j=1} a^{[l](S)}_{ijk} a^{[l](S)}_{ijk&amp;rsquo;}$$&lt;/p&gt;

&lt;p&gt;其中，$i$ 和 $j$ 为第 $l$ 层的高度和宽度；$k$ 和 $k&amp;rsquo;$ 为选定的通道，其范围为 $1$ 到 $n_C^{[l]}$；$a^{[l](S)}_{ijk}$ 为激活。&lt;/p&gt;

&lt;p&gt;同理，对于生成图像 G，有：&lt;/p&gt;

&lt;p&gt;$$G^{[l](G)}_{kk&amp;rsquo;} = \sum^{n^{[l]}_H}_{i=1} \sum^{n^{[l]}_W}_{j=1} a^{[l](G)}_{ijk} a^{[l](G)}_{ijk&amp;rsquo;}$$&lt;/p&gt;

&lt;p&gt;因此，第 $l$ 层的风格代价函数为：&lt;/p&gt;

&lt;p&gt;$$J^{[l]}_{style}(S, G) = \frac{1}{(2n^{[l]}_Hn^{[l]}_Wn^{[l]}_C)^2} \sum_k \sum_{k&amp;rsquo;}(G^{[l](S)}_{kk&amp;rsquo;} - G^{[l](G)}_{kk&amp;rsquo;})^2$$&lt;/p&gt;

&lt;p&gt;如果对各层都使用风格代价函数，效果会更好。因此有：&lt;/p&gt;

&lt;p&gt;$$J_{style}(S, G) = \sum_l \lambda^{[l]} J^{[l]}_{style}(S, G)$$&lt;/p&gt;

&lt;p&gt;其中，$lambda$ 是用于设置不同层所占权重的超参数。&lt;/p&gt;

&lt;h3 id=&#34;推广至一维和三维&#34;&gt;推广至一维和三维&lt;/h3&gt;

&lt;p&gt;之前我们处理的都是二维图片，实际上卷积也可以延伸到一维和三维数据。我们举两个示例来说明。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/1D-3D-Convolution.png&#34; alt=&#34;1D-3D-Convolution&#34; /&gt;&lt;/p&gt;

&lt;p&gt;EKG 数据（心电图）是由时间序列对应的每个瞬间的电压组成，是一维数据。一般来说我们会用 RNN（循环神经网络）来处理，不过如果用卷积处理，则有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;输入时间序列维度：14 x 1&lt;/li&gt;
&lt;li&gt;滤波器尺寸：5 x 1，滤波器个数：16&lt;/li&gt;
&lt;li&gt;输出时间序列维度：10 x 16&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而对于三维图片的示例，有&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;输入 3D 图片维度：14 x 14 x 14 x 1&lt;/li&gt;
&lt;li&gt;滤波器尺寸：5 x 5 x 5 x 1，滤波器个数：16&lt;/li&gt;
&lt;li&gt;输出 3D 图片维度：10 x 10 x 10 x 16&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
